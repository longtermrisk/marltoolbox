import random
from typing import Dict

import torch
from ray.rllib.agents import ppo, pg, dqn, sac
from ray.rllib.agents.a3c.a3c_torch_policy import A3CTorchPolicy
from ray.rllib.agents.a3c.a3c_tf_policy import A3CTFPolicy
from ray.rllib.agents.callbacks import DefaultCallbacks
from ray.rllib.evaluation.sample_batch_builder import \
    MultiAgentSampleBatchBuilder
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.utils import merge_dicts
from ray.rllib.policy import Policy

from marltoolbox.utils.postprocessing import WELFARES
from marltoolbox.algos.exploiters.evader import EvaderTorchPolicy, \
    create_evader_policy_config
from marltoolbox.envs.matrix_sequential_social_dilemma import \
    IteratedPrisonersDilemma


def given_an_evader_policy():
    config = merge_dicts(
        ppo.DEFAULT_CONFIG,
        {
            'nested_policies': [
                # You need to provide the policy class for every nested Policies
                {"Policy_class": ppo.PPOTorchPolicy,
                 "config_update": {}},
                {"Policy_class": ppo.PPOTorchPolicy,
                 "config_update": {}}
            ],
            "start_exploit_at_step_n": random.randint(1, 1000),
            "copy_weights_every_n_steps": random.randint(1, 1000),
        }
    )
    evader = EvaderTorchPolicy(
        observation_space=IteratedPrisonersDilemma.OBSERVATION_SPACE,
        action_space=IteratedPrisonersDilemma.ACTION_SPACE,
        config=config)
    return evader, \
           config["start_exploit_at_step_n"], \
           config["copy_weights_every_n_steps"]


def when_reaching_the_start_of_exploitation(evader,
                                            start_exploit_at_step_n):
    evader = set_current_step_and_perform_one_optimization_step(
        evader, start_exploit_at_step_n)
    return evader


def get_fake_training_batch_for_ppo_in_ipd(policy):
    policy_id = "fake_player"
    players = {policy_id: policy}

    multi_agent_batch_builder = MultiAgentSampleBatchBuilder(
        policy_map={player_id: player for player_id, player in
                    players.items()},
        clip_rewards=False,
        callbacks=DefaultCallbacks()
    )

    n_steps_in_epi = 20
    for step_n in range(n_steps_in_epi):
        step_player_values = {
            SampleBatch.EPS_ID: 0,
            SampleBatch.OBS: 0,
            SampleBatch.NEXT_OBS: 0,
            SampleBatch.ACTIONS: 0,
            SampleBatch.REWARDS: 0,
            SampleBatch.PREV_REWARDS: 0,
            SampleBatch.VF_PREDS: 0,
            SampleBatch.DONES: step_n == n_steps_in_epi,
            SampleBatch.ACTION_DIST_INPUTS: [1.0, 0.0],
            SampleBatch.ACTION_LOGP: 1.0,
        }
        multi_agent_batch_builder.add_values(
            agent_id=policy_id,
            policy_id=policy_id,
            **step_player_values
        )

    multiagent_batch = multi_agent_batch_builder.build_and_reset()
    return multiagent_batch.policy_batches[policy_id]


def assert_weights_of_both_policies_are_equal(evader):
    evading_weights, in_use_weights = get_models_weights(evader)
    assert torch.all(evading_weights == in_use_weights)


def get_models_weights(evader):
    evading_weights = flatten_list_tensor(evader.get_evading_weights())
    in_use_weights = flatten_list_tensor(evader.get_in_use_weights())
    return evading_weights, in_use_weights


def flatten_list_tensor(list_tensor):
    # TODO use flatten from rllib.model.modelv2?
    list_tensor = [torch.reshape(w_tensor, shape=(-1,))
                   for w_tensor in list_tensor]
    return torch.cat(list_tensor, dim=0)


def when_before_the_start_of_exploitation(
        evader, start_exploit_at_step_n):
    n_steps_played = random.randint(0, start_exploit_at_step_n - 1)

    evader = set_current_step_and_perform_one_optimization_step(evader,
                                                                n_steps_played)
    return evader


def set_current_step_and_perform_one_optimization_step(evader, n_steps_played,
                                                       samples=None):
    evader.on_global_var_update({"timestep": n_steps_played})
    print("n_steps_played", n_steps_played)

    samples = get_fake_training_batch_for_ppo_in_ipd(
        evader) if samples is None else samples
    evader.learn_on_batch(samples)
    return evader


def assert_weights_of_both_policies_are_not_equal(evader):
    evading_weights, in_use_weights = get_models_weights(evader)
    assert torch.any(evading_weights != in_use_weights)


def when_every_n_step_after_start_of_exploitation(
        evader,
        start_exploit_at_step_n,
        copy_weights_every_n_steps,
        nth_repetition):
    n_steps_played = start_exploit_at_step_n + \
                     copy_weights_every_n_steps * nth_repetition
    evader = set_current_step_and_perform_one_optimization_step(
        evader, start_exploit_at_step_n)
    evader = set_current_step_and_perform_one_optimization_step(
        evader, n_steps_played)
    return evader


def when_in_between_n_step_after_start_of_exploitation(
        evader,
        start_exploit_at_step_n,
        copy_weights_every_n_steps,
        nth_repetition):
    play_intermediary_copies(evader, start_exploit_at_step_n, nth_repetition,
                             copy_weights_every_n_steps)

    additional_steps_be_be_in_between_two_copies = random.randint(
        1, copy_weights_every_n_steps - 1)
    n_steps_played = start_exploit_at_step_n + \
                     copy_weights_every_n_steps * (nth_repetition - 1) + \
                     additional_steps_be_be_in_between_two_copies
    evader = set_current_step_and_perform_one_optimization_step(
        evader, n_steps_played)
    return evader


def play_intermediary_copies(evader, start_exploit_at_step_n, nth_repetition,
                             copy_weights_every_n_steps):
    evader = set_current_step_and_perform_one_optimization_step(
        evader, start_exploit_at_step_n)
    if (nth_repetition - 1) > 0:
        for i in range(1, nth_repetition):
            n_steps_played = start_exploit_at_step_n + \
                             copy_weights_every_n_steps * i
            evader = set_current_step_and_perform_one_optimization_step(
                evader, n_steps_played)


def given_the_config_of_an_rllib_policy():
    policy_configs = (ppo.PPOTorchPolicy,
                      ppo.PPOTFPolicy,
                      pg.PGTorchPolicy,
                      pg.PGTFPolicy,
                      dqn.DQNTorchPolicy,
                      dqn.DQNTFPolicy,
                      A3CTorchPolicy,
                      A3CTFPolicy,
                      sac.SACTorchPolicy,
                      sac.SACTFPolicy)
    return random.sample(policy_configs, 1)[0]


def when_using_the_helper_to_create_an_evader_policy_config(
        welfare_function=None,
        cooperative_policy_config_update: Dict = None,
        start_exploit_at_step_n: int = None,
        copy_weights_every_n_steps: int = None,
        exploiter_policy: Policy = None):
    return create_evader_policy_config(
        welfare_function,
        cooperative_policy_config_update,
        start_exploit_at_step_n,
        copy_weights_every_n_steps,
        exploiter_policy)


def assert_that_the_policy_config_is_conform(evader_policy_config,
                                             welfare_function=None):
    all_necessary_keys_are_in_evader_config(evader_policy_config)
    hierarchical_policy_contains_two_nested_policies(evader_policy_config)
    # TODO add that after refactoring adding welfare functions
    if welfare_function is not None:
        first_nested_policy_is_cooperative(evader_policy_config,
                                           welfare_function)
        second_nested_policy_is_selfish(evader_policy_config)


def all_necessary_keys_are_in_evader_config(evader_policy_config):
    minimum_keys_in_config = ("start_exploit_at_step_n",
                              "copy_weights_every_n_steps",
                              "nested_policies")
    for key in minimum_keys_in_config:
        assert key in evader_policy_config.keys()


def hierarchical_policy_contains_two_nested_policies(
        hierarchical_policy_config):
    assert len(hierarchical_policy_config["nested_policies"]) == 2


def first_nested_policy_is_cooperative(evader_policy_config,
                                       welfare_function):
    first_nested_policy_config_update = \
        evader_policy_config["nested_policies"][0]["config_update"]
    assert "welfare" in first_nested_policy_config_update.keys()
    assert welfare_function == first_nested_policy_config_update["welfare"]


def second_nested_policy_is_selfish(evader_policy_config):
    assert "welfare" not in \
           evader_policy_config["nested_policies"][1]["config_update"].keys()

def given_a_welfare_function():
    for welfare_function in WELFARES:
        yield welfare_function

def given_a_config_to_set_cooperation():
    pass