import random
from typing import Dict

import torch
from ray.rllib.agents import ppo, pg, dqn, sac
from ray.rllib.agents.a3c.a3c_torch_policy import A3CTorchPolicy
from ray.rllib.agents.a3c.a3c_tf_policy import A3CTFPolicy
from ray.rllib.agents.callbacks import DefaultCallbacks
from ray.rllib.evaluation.sample_batch_builder import \
    MultiAgentSampleBatchBuilder
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.utils import merge_dicts
from ray.rllib.policy import Policy

from marltoolbox.algos.exploiters.influence_evader import \
    InfluenceEvaderTorchPolicy
from marltoolbox.envs.matrix_sequential_social_dilemma import \
    IteratedPrisonersDilemma


def given_an_evader_policy():
    config = merge_dicts(
        ppo.DEFAULT_CONFIG,
        {
            'nested_policies': [
                # You need to provide the policy class for every nested Policies
                {"Policy_class": ppo.PPOTorchPolicy,
                 "config_update": {}},
                {"Policy_class": ppo.PPOTorchPolicy,
                 "config_update": {}}
            ],
            "start_exploit_at_step_n": random.randint(1, 1000),
            "copy_weights_every_n_steps": random.randint(1, 1000),
        }
    )
    evader = InfluenceEvaderTorchPolicy(
        observation_space=IteratedPrisonersDilemma.OBSERVATION_SPACE,
        action_space=IteratedPrisonersDilemma.ACTION_SPACE,
        config=config)
    return evader, \
           config["start_exploit_at_step_n"], \
           config["copy_weights_every_n_steps"]


def when_reaching_the_start_of_exploitation(evader,
                                            start_exploit_at_step_n):
    evader = set_current_step_and_perform_one_optimization_step(
        evader, start_exploit_at_step_n)
    return evader


def get_fake_training_batch_for_ppo_in_ipd(policy):
    policy_id = "fake_player"
    players = {policy_id: policy}

    multi_agent_batch_builder = MultiAgentSampleBatchBuilder(
        policy_map={player_id: player for player_id, player in
                    players.items()},
        clip_rewards=False,
        callbacks=DefaultCallbacks()
    )

    n_steps_in_epi = 20
    for step_n in range(n_steps_in_epi):
        step_player_values = {
            SampleBatch.EPS_ID: 0,
            SampleBatch.OBS: 0,
            SampleBatch.NEXT_OBS: 0,
            SampleBatch.ACTIONS: 0,
            SampleBatch.REWARDS: 0,
            SampleBatch.PREV_REWARDS: 0,
            SampleBatch.VF_PREDS: 0,
            SampleBatch.DONES: step_n == n_steps_in_epi,
            SampleBatch.ACTION_DIST_INPUTS: [1.0, 0.0],
            SampleBatch.ACTION_LOGP: 1.0,
        }
        multi_agent_batch_builder.add_values(
            agent_id=policy_id,
            policy_id=policy_id,
            **step_player_values
        )

    multiagent_batch = multi_agent_batch_builder.build_and_reset()
    return multiagent_batch.policy_batches[policy_id]


def assert_weights_of_both_policies_are_equal(evader):
    evading_weights, in_use_weights = get_models_weights(evader)
    assert torch.all(evading_weights == in_use_weights)


def get_models_weights(evader):
    evading_weights = flatten_list_tensor(evader.get_evading_weights())
    in_use_weights = flatten_list_tensor(evader.get_in_use_weights())
    return evading_weights, in_use_weights


def flatten_list_tensor(list_tensor):
    # TODO use flatten from rllib.model.modelv2?
    list_tensor = [torch.reshape(w_tensor, shape=(-1,))
                   for w_tensor in list_tensor]
    return torch.cat(list_tensor, dim=0)


def when_before_the_start_of_exploitation(
        evader, start_exploit_at_step_n):
    n_steps_played = random.randint(0, start_exploit_at_step_n - 1)

    evader = set_current_step_and_perform_one_optimization_step(evader,
                                                                n_steps_played)
    return evader


def set_current_step_and_perform_one_optimization_step(evader, n_steps_played,
                                                       samples=None):
    evader.on_global_var_update({"timestep": n_steps_played})
    print("n_steps_played", n_steps_played)

    samples = get_fake_training_batch_for_ppo_in_ipd(
        evader) if samples is None else samples
    evader.learn_on_batch(samples)
    return evader


def assert_weights_of_both_policies_are_not_equal(evader):
    evading_weights, in_use_weights = get_models_weights(evader)
    assert torch.any(evading_weights != in_use_weights)


def when_every_n_step_after_start_of_exploitation(
        evader,
        start_exploit_at_step_n,
        copy_weights_every_n_steps,
        nth_repetition):
    n_steps_played = start_exploit_at_step_n + \
                     copy_weights_every_n_steps * nth_repetition
    evader = set_current_step_and_perform_one_optimization_step(
        evader, start_exploit_at_step_n)
    evader = set_current_step_and_perform_one_optimization_step(
        evader, n_steps_played)
    return evader


def when_in_between_n_step_after_start_of_exploitation(
        evader,
        start_exploit_at_step_n,
        copy_weights_every_n_steps,
        nth_repetition):
    play_intermediary_copies(evader, start_exploit_at_step_n, nth_repetition,
                             copy_weights_every_n_steps)

    additional_steps_be_be_in_between_two_copies = random.randint(
        1, copy_weights_every_n_steps - 1)
    n_steps_played = start_exploit_at_step_n + \
                     copy_weights_every_n_steps * (nth_repetition - 1) + \
                     additional_steps_be_be_in_between_two_copies
    evader = set_current_step_and_perform_one_optimization_step(
        evader, n_steps_played)
    return evader


def play_intermediary_copies(evader, start_exploit_at_step_n, nth_repetition,
                             copy_weights_every_n_steps):
    evader = set_current_step_and_perform_one_optimization_step(
        evader, start_exploit_at_step_n)
    if (nth_repetition - 1) > 0:
        for i in range(1, nth_repetition):
            n_steps_played = start_exploit_at_step_n + \
                             copy_weights_every_n_steps * i
            evader = set_current_step_and_perform_one_optimization_step(
                evader, n_steps_played)
