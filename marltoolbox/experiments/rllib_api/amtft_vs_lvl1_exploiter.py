import copy
import logging
import os

import pandas as pd
import ray
from ray.rllib.agents import dqn
from ray.rllib.utils import merge_dicts
from ray.tune.integration.wandb import WandbLogger
from ray.tune.logger import DEFAULT_LOGGERS

from marltoolbox.algos import amTFT
from marltoolbox.algos.amTFT import inversed_policy
from marltoolbox.experiments.rllib_api import amtft_various_env
from marltoolbox.utils import postprocessing, plot

logger = logging.getLogger(__name__)


def main(debug, train_n_replicates=None, filter_utilitarian=None, env=None):
    env = "IteratedPrisonersDilemma"
    # env = "IteratedAsymBoS"
    # env = "CoinGame"
    # env = "AsymCoinGame"

    train_n_replicates = 1 if debug else 4

    hparams = amtft_various_env.get_hyperparameters(
        debug, train_n_replicates, filter_utilitarian, env)
    hparams = modify_hp_for_lvl_exploiter(hparams)

    if hparams["load_plot_data"] is None:
        ray.init(
            num_cpus=os.cpu_count(),
            num_gpus=0,
            local_mode=hparams["debug"])

        # Train
        if hparams["load_policy_data"] is None:
            tune_analysis_per_welfare = \
                train_for_each_welfare_function(hparams)
        else:
            tune_analysis_per_welfare = \
                amtft_various_env.load_tune_analysis(hparams[
                                                         "load_policy_data"])

        _evaluate_perf_wt_and_without_exploiter(hparams,
                                                tune_analysis_per_welfare)
        ray.shutdown()
    else:
        tune_analysis_per_welfare = None
        _evaluate_perf_wt_and_without_exploiter(hparams,
                                                tune_analysis_per_welfare)

    return tune_analysis_per_welfare


def train_for_each_welfare_function(hp):
    tune_analysis_per_welfare = {}
    for welfare_fn, welfare_group_name in hp['welfare_functions']:
        print("==============================================")
        print("Going to start two_steps_training with welfare function",
              welfare_fn)
        if welfare_fn == postprocessing.WELFARE_UTILITARIAN:
            hp = amtft_various_env.preprocess_utilitarian_config(hp)
        stop, env_config, rllib_config = \
            amtft_various_env.get_rllib_config(hp, welfare_fn)
        rllib_config = modify_rllib_config_for_lvl_1_exploiter(
            hp, rllib_config, welfare_fn)

        exp_name = os.path.join(hp["exp_name"], welfare_fn)
        results = amTFT.train_amtft(
            stop_config=stop,
            rllib_config=rllib_config,
            name=exp_name,
            TrainerClass=dqn.DQNTrainer,
            plot_keys=hp["plot_keys"],
            plot_assemblage_tags=hp["plot_assemblage_tags"],
            debug=hp["debug"],
            log_to_file=not hp["debug"],
            loggers=None if hp["debug"] else DEFAULT_LOGGERS + (WandbLogger,),
        )
        if welfare_fn == postprocessing.WELFARE_UTILITARIAN:
            results, hp = amtft_various_env.postprocess_utilitarian_results(
                results, env_config, hp)
        tune_analysis_per_welfare[welfare_group_name] = results
    return tune_analysis_per_welfare


def modify_hp_for_lvl_exploiter(hp):
    hp["exploiter_policy_n"] = 0
    hp["exploiter_activated"] = True
    if hp["debug"]:
        hp["lookahead_n_times"] = 2
        hp["debit_threshold_range"] = range(3, 5, 1)
        hp["punishment_multiplier_range"] = range(2, 4, 1)
    else:
        hp["lookahead_n_times"] = 5
        # hp["punishment_multiplier_range"] = [1, 2, 4, 8]
        hp["punishment_multiplier_range"] = [4]
        if "CoinGame" in hp["env_name"]:
            hp["debit_threshold_range"] = [2, 4, 8, 16, 32, 64]
        else:
            hp["debit_threshold_range"] = [0.125, 0.25, 0.5,
                                           1, 2, 4, 8, 16, 32, 64]
            hp["debit_threshold_range"] = [0.125, 0.18,
                                           0.25, 0.35,
                                           0.5, 0.75,
                                           1, 1.5,
                                           2, 3,
                                           4, 6,
                                           8, 12,
                                           16, 28,
                                           32, 48,
                                           64]
            # hp["debit_threshold_range"] = [2, 8, 32]
    if "CoinGame" in hp["env_name"]:
        hp["player_1_metric"] = "policy_reward_mean/player_red"
        hp["player_2_metric"] = "policy_reward_mean/player_blue"
    else:
        hp["player_1_metric"] = "policy_reward_mean/player_row"
        hp["player_2_metric"] = "policy_reward_mean/player_col"

    # if hp["debug"]:
    #     hp["load_policy_data"] = {
    #         "Util": [
    #             "~/dev-maxime/CLR/vm-data/instance-60-cpu-1-preemtible/amTFT/2021_04_08/20_51_42/utilitarian_welfare/coop"
    #             "/DQN_IteratedPrisonersDilemma_97f95_00000_0_seed"
    #             "=1617915106_2021-04-08_21-08-40/checkpoint_200/checkpoint-200",
    #             "~/dev-maxime/CLR/vm-data/instance-60-cpu-1-preemtible/amTFT/2021_04_08/20_51_42/utilitarian_welfare/coop"
    #             "/DQN_IteratedPrisonersDilemma_97f95_00001_1_seed=1617915107_2021-04-08_21-08-42/checkpoint_200/checkpoint-200",
    #         ],
    #         'IA': [
    #             "~/dev-maxime/CLR/vm-data/instance-60-cpu-1-preemtible/amTFT/2021_04_08/20_51_42/inequity_aversion_welfare/coop"
    #             "/DQN_IteratedPrisonersDilemma_c9ddf_00000_0_seed"
    #             "=1617915102_2021-04-08_20-55-45/checkpoint_200/checkpoint-200",
    #             "~/dev-maxime/CLR/vm-data/instance-60-cpu-1-preemtible/amTFT/2021_04_08/20_51_42/inequity_aversion_welfare/coop"
    #             "/DQN_IteratedPrisonersDilemma_c9ddf_00001_1_seed=1617915103_2021-04-08_20-55-45/checkpoint_200/checkpoint-200",
    #         ],
    #     }

    return hp


def modify_rllib_config_for_lvl_1_exploiter(hp, rllib_config, welfare_fn):
    exploiter_config = _get_exploiter_policy_config(hp, rllib_config)
    exploiter_config = _update_exploiter_policy_config(
        hp, exploiter_config, welfare_fn)
    rllib_config = _set_exploiter_policy_config(
        hp, rllib_config, exploiter_config)
    return rllib_config


def _get_exploiter_policy_config(hp, rllib_config):
    policy_ids = rllib_config["env_config"]["players_ids"]
    exploiter_policy_id = policy_ids[hp["exploiter_policy_n"]]
    policies = rllib_config["multiagent"]["policies"]
    exploiter_config = list(policies[exploiter_policy_id])
    return exploiter_config


def _update_exploiter_policy_config(hp, exploiter_config, welfare_fn):
    amTFT_config = exploiter_config[3]
    amTFT_config.update({
        "auto_load_checkpoint": False,
    })

    NestedPolicyClass, CoopNestedPolicyClass = \
        amtft_various_env.get_nested_policy_class(hp, welfare_fn)

    exploiter_config[0] = amTFT.Level1amTFTExploiterTorchPolicy
    exploiter_config[3] = merge_dicts(
        amTFT.level_1_exploiter.DEFAULT_CONFIG,
        {
            "verbose": 0,
            "lookahead_n_times": hp["lookahead_n_times"],
            "exploiter_activated": hp["exploiter_activated"],
            "welfare_key": welfare_fn,
            'nested_policies': [
                {"Policy_class": CoopNestedPolicyClass, "config_update": {
                    "optimizer": {"sgd_momentum": hp["sgd_momentum"], },
                }},
                {"Policy_class": NestedPolicyClass, "config_update": {
                    "optimizer": {"sgd_momentum": hp["sgd_momentum"], },
                }},
                {"Policy_class":
                     inversed_policy.InversedAmTFTRolloutsTorchPolicy,
                 "config_update": amTFT_config}
            ],
        }
    )
    return exploiter_config


def _set_exploiter_policy_config(hp, rllib_config, exploiter_config):
    policy_ids = rllib_config["env_config"]["players_ids"]
    exploiter_policy_id = policy_ids[hp["exploiter_policy_n"]]
    policies = rllib_config["multiagent"]["policies"]
    policies[exploiter_policy_id] = tuple(exploiter_config)
    return rllib_config


def _evaluate_perf_wt_and_without_exploiter(hp, tune_analysis_per_welfare):
    for exploiter_activated in (True, False):
        _evaluate_perf_inself_or_cross_play(
            hp, tune_analysis_per_welfare, exploiter_activated)


def _evaluate_perf_inself_or_cross_play(
        hp, tune_analysis_per_welfare, exploiter_activated):
    for in_cross_play in (False, True):
        hp_eval = copy.deepcopy(hp)
        if in_cross_play:
            # hp_eval["n_self_play_per_checkpoint"] = 1
            hp_eval["n_cross_play_per_checkpoint"] = 0
        else:
            hp_eval["n_self_play_per_checkpoint"] = 0
            # hp_eval["n_cross_play_per_checkpoint"] = \
            #     min(5,
            #         ((hp_eval["train_n_replicates"] *
            #           len(hp_eval["welfare_functions"]))
            #          - 1)
            #         )
        _evaluate_perf_over_all_possible_hp(
            hp_eval, tune_analysis_per_welfare, exploiter_activated,
            in_cross_play)


def _evaluate_perf_over_all_possible_hp(
        hp, tune_analysis_per_welfare, exploiter_activated, in_cross_play):
    for punishment_multiplier in hp["punishment_multiplier_range"]:
        hp_eval = copy.deepcopy(hp)
        hp_eval["punishment_multiplier"] = punishment_multiplier
        hp_eval["exploiter_activated"] = exploiter_activated
        data_groups = {}

        (rows_data_p1_util_vs_util, rows_data_p2_util_vs_util,
         rows_data_p1_ia_vs_ia, rows_data_p2_ia_vs_ia) = \
            _gather_data_for_one_group_of_hp_sets(
                hp_eval, tune_analysis_per_welfare, punishment_multiplier)

        data_groups["p1_util_vs_util"] = \
            pd.DataFrame(rows_data_p1_util_vs_util) \
                .set_index(keys="debit_threshold")
        data_groups["p2_util_vs_util"] = \
            pd.DataFrame(rows_data_p2_util_vs_util) \
                .set_index(keys="debit_threshold")
        data_groups["p1_ia_vs_ia"] = \
            pd.DataFrame(rows_data_p1_ia_vs_ia) \
                .set_index(keys="debit_threshold")
        data_groups["p2_ia_vs_ia"] = \
            pd.DataFrame(rows_data_p2_ia_vs_ia) \
                .set_index(keys="debit_threshold")

        suffix = f"use_expl_{exploiter_activated}" \
                 f"_in_cross_play_{in_cross_play}" \
                 f"_punish_mul_{punishment_multiplier}"
        _plot_final_results(hp, data_groups,
                            suffix=suffix)


def _gather_data_for_one_group_of_hp_sets(
        hp_eval, tune_analysis_per_welfare, punishment_multiplier):
    rows_data_p1_util_vs_util = []
    rows_data_p2_util_vs_util = []
    rows_data_p1_ia_vs_ia = []
    rows_data_p2_ia_vs_ia = []

    for debit_threshold in hp_eval["debit_threshold_range"]:
        hp_eval_updated = copy.deepcopy(hp_eval)
        hp_eval_updated["debit_threshold"] = debit_threshold

        analysis_metrics_per_mode, hp_eval_updated = \
            _evaluate_perf_for_one_hp_set(
                tune_analysis_per_welfare, hp_eval_updated)

        (mean_reward_util_vs_util_p1, mean_reward_util_vs_util_p2,
         mean_reward_ia_vs_ia_p1, mean_reward_ia_vs_ia_p2) = \
            _print_result_summary(analysis_metrics_per_mode,
                                  debit_threshold,
                                  punishment_multiplier, hp_eval_updated)

        rows_data_p1_util_vs_util.append({
            "debit_threshold": debit_threshold,
            "reward_util_vs_util_p1": mean_reward_util_vs_util_p1,
        })
        rows_data_p2_util_vs_util.append({
            "debit_threshold": debit_threshold,
            "reward_util_vs_util_p2": mean_reward_util_vs_util_p2,
        })
        rows_data_p1_ia_vs_ia.append({
            "debit_threshold": debit_threshold,
            "reward_ia_vs_ia_p1": mean_reward_ia_vs_ia_p1,
        })
        rows_data_p2_ia_vs_ia.append({
            "debit_threshold": debit_threshold,
            "reward_ia_vs_ia_p2": mean_reward_ia_vs_ia_p2,
        })

    return rows_data_p1_util_vs_util, rows_data_p2_util_vs_util, \
           rows_data_p1_ia_vs_ia, rows_data_p2_ia_vs_ia


def _evaluate_perf_for_one_hp_set(tune_analysis_per_welfare, hp_eval):
    config_eval, env_config, stop, hp_eval_updated = \
        _generate_eval_config(hp_eval)

    # Eval & Plot
    analysis_metrics_per_mode = \
        amtft_various_env.evaluate_self_play_cross_play(
            tune_analysis_per_welfare, config_eval, env_config, stop,
            hp_eval_updated)

    return analysis_metrics_per_mode, hp_eval_updated


def _generate_eval_config(hp):
    hp_eval = amtft_various_env.modify_hp_for_evaluation(hp)
    # hp_eval["n_cross_play_per_checkpoint"] = 0
    fake_welfare_function = postprocessing.WELFARE_INEQUITY_AVERSION
    stop, env_config, rllib_config = amtft_various_env.get_rllib_config(
        hp_eval,
        fake_welfare_function,
        eval=True)
    rllib_config = modify_rllib_config_for_lvl_1_exploiter(
        hp, rllib_config, fake_welfare_function)
    hp_eval["debit_threshold_debug_override"] = False
    config_eval = amtft_various_env.modify_config_for_evaluation(
        rllib_config, hp_eval, env_config)
    return config_eval, env_config, stop, hp_eval


def _print_result_summary(analysis_metrics_per_mode,
                          debit_threshold, punishment_multiplier, hp_eval):
    mean_reward_util_vs_util_p1 = None
    mean_reward_util_vs_util_p2 = None
    mean_reward_ia_vs_ia_p1 = None
    mean_reward_ia_vs_ia_p2 = None
    print(f"====== debit_threshold {debit_threshold} ==== "
          f"punishment_multiplier {punishment_multiplier} ======")
    for play_mode, metrics, _, welfares in analysis_metrics_per_mode:
        reward_player1 = [replicate[hp_eval["player_1_metric"]]["avg"]
                          for replicate in metrics]
        reward_player2 = [replicate[hp_eval["player_2_metric"]]["avg"]
                          for replicate in metrics]
        mean_r_player1 = sum(reward_player1) / len(reward_player1)
        mean_r_player2 = sum(reward_player2) / len(reward_player2)

        print(f"In play_mode {play_mode} with welfares {welfares}")
        print("reward_player1", mean_r_player1, reward_player1)
        print("reward_player2", mean_r_player2, reward_player2)

        if all([w == "utilitarian" for w in welfares]):
            mean_reward_util_vs_util_p1 = mean_r_player1
            mean_reward_util_vs_util_p2 = mean_r_player2
        elif all([w == "inequity_aversion" for w in welfares]):
            mean_reward_ia_vs_ia_p1 = mean_r_player1
            mean_reward_ia_vs_ia_p2 = mean_r_player2

    return mean_reward_util_vs_util_p1, mean_reward_util_vs_util_p2, \
           mean_reward_ia_vs_ia_p1, mean_reward_ia_vs_ia_p2


def _plot_final_results(hp: dict, data_groups: dict, suffix: str = ""):
    save_dir_path = os.path.join("~/ray_results",
                                 hp["exp_name"])
    save_dir_path = os.path.expanduser(save_dir_path)
    plot_config = plot.PlotConfig(
        xlabel="debit_threshold",
        ylabel="total_payoffs",
        save_dir_path=save_dir_path,
        filename_prefix=f"plot_{suffix}",
        x_use_log_scale=True,
    )
    plotter = plot.PlotHelper(plot_config)
    plotter.plot_lines(data_groups=data_groups)


if __name__ == "__main__":
    debug_mode = True
    main(debug_mode)
