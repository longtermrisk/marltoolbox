import copy
import random
import torch
import numpy as np
from marltoolbox.experiments.tune_class_api import (
    lola_exact_official,
    various_algo_meta_game,
)
from marltoolbox.utils import log, miscellaneous, exp_analysis
from marltoolbox.experiments.tune_class_api.various_algo_meta_game import (
    META_UNIFORM,
    META_SOS,
    META_RANDOM,
    META_LOLA_EXACT,
    META_PG,
    META_APLHA_RANK,
    META_APLHA_PURE,
    META_REPLICATOR_DYNAMIC,
    META_REPLICATOR_DYNAMIC_ZERO_INIT,
    BASE_NEGOTIATION,
    BASE_LOLA_EXACT,
    BASE_AMTFT,
    POLICY_ID_PL0,
    POLICY_ID_PL1,
    META_MINIMUM,
)

from marltoolbox.experiments.tune_class_api import meta_solver_cross_play
from marltoolbox.experiments.rllib_api import amtft_meta_game

POLICY_IDS = [POLICY_ID_PL0, POLICY_ID_PL1]


def main(
    debug,
    base_game_algo=None,
    meta_game_algo=None,
    exploiter_idx=None,
    being_exploited_idx=None,
):
    """Evaluate meta game performances"""

    print(f"==== base_game_algo {base_game_algo} =====")
    print(f"==== meta_game_algo {meta_game_algo} =====")
    train_n_replicates = 1
    tau = 0.0
    n_in_bootstrap = 10
    n_replicates = 40
    seeds = miscellaneous.get_random_seeds(train_n_replicates)
    exp_name, _ = log.log_in_current_day_dir("meta_game_compare")

    hp = _get_meta_payoff_matrices_and_policies(
        meta_game_algo,
        debug,
        seeds,
        exp_name,
        base_game_algo,
    )
    pl_1_r = []
    pl_2_r = []
    br_idx = []
    bootstrapped_idx = []
    sets_announced = []
    for repl_i in range(n_replicates):
        bootstrapped_hp = _bootstrap_payoff_mat(hp, n_in_bootstrap)
        meta_payoff_mat_avg = _average_payoff_matrices(bootstrapped_hp)
        player_0_meta_policy = _average_meta_opponent_policies(
            bootstrapped_hp, being_exploited_idx
        )
        (
            best_reponse_action,
            player_0_payoff_in_br,
            player_1_payoff_in_br,
        ) = _get_best_response(
            meta_payoff_mat_avg,
            player_0_meta_policy,
            tau,
            exploiter_idx,
            being_exploited_idx,
        )
        pl_1_r.append(player_0_payoff_in_br)
        pl_2_r.append(player_1_payoff_in_br)
        br_idx.append(int(best_reponse_action))
        bootstrapped_idx.append(bootstrapped_hp["bootstrapped_idx"])
        best_reponse_set = bootstrapped_hp["actions_possible"][
            best_reponse_action
        ]
        print(
            "best_reponse_action",
            best_reponse_action,
            best_reponse_set,
        )
        sets_announced.append(list(best_reponse_set))

    pl_1_r = np.array(pl_1_r)
    print("pl_1_r.shape", pl_1_r.shape)
    pl_1_r_mean = pl_1_r.mean()
    pl_1_r_std_err = pl_1_r.std() / np.sqrt(pl_1_r.shape[0])

    pl_2_r = np.array(pl_2_r)
    print("pl_2_r.shape", pl_2_r.shape)
    pl_2_r_mean = pl_2_r.mean()
    pl_2_r_std_err = pl_2_r.std() / np.sqrt(pl_2_r.shape[0])

    print("players payoffs", player_0_payoff_in_br, player_1_payoff_in_br)
    result_to_json = {
        "br_idx": br_idx,
        "sets_announced": sets_announced,
        "pl_1_r_mean": float(pl_1_r_mean),
        "pl_2_r_mean": float(pl_2_r_mean),
        "pl_1_r_std_err": float(pl_1_r_std_err),
        "pl_2_r_std_err": float(pl_2_r_std_err),
        "bootstrapped_idx": bootstrapped_idx,
        "player_0_meta_policy": player_0_meta_policy.tolist(),
        "payoff_matrices": [el.tolist() for el in hp["payoff_matrices"]],
        "meta_game_policy_distributions": [
            {k: v.tolist() for k, v in el.items()}
            for el in hp["meta_game_policy_distributions"]
        ],
    }
    amtft_meta_game.save_to_json(
        exp_name=hp["exp_name"], object=result_to_json
    )
    return (
        float(pl_1_r_mean),
        float(pl_2_r_mean),
        float(pl_1_r_std_err),
        float(pl_2_r_std_err),
    )


def _bootstrap_payoff_mat(hp, n_in_bootstrap):
    bootstrapped_hp = copy.deepcopy(hp)
    meta_payoff_matrices = bootstrapped_hp["payoff_matrices"]
    print("len before bootstrapped", len(meta_payoff_matrices))
    bootstrapped_hp["bootstrapped_idx"] = random.choices(
        range(len(meta_payoff_matrices)), k=n_in_bootstrap
    )
    bootstrapped_hp["payoff_matrices"] = [
        meta_payoff_matrices[idx]
        for idx in bootstrapped_hp["bootstrapped_idx"]
    ]
    print("len after bootstrapped", len(bootstrapped_hp["payoff_matrices"]))
    return bootstrapped_hp


def _average_payoff_matrices(hp):
    print("len(payoff_matrices)", len(hp["payoff_matrices"]))
    print("payoff_matrices[0].shape", hp["payoff_matrices"][0].shape)

    meta_payoff_mat_avg = copy.deepcopy(hp["payoff_matrices"])
    meta_payoff_mat_avg = np.array(meta_payoff_mat_avg).mean(axis=0)
    print("meta_payoff_mat_avg", meta_payoff_mat_avg.shape)
    return meta_payoff_mat_avg


def _average_meta_opponent_policies(hp, being_exploited_idx):
    pl_being_exploited_id = POLICY_IDS[being_exploited_idx]
    player_0_policies = [
        el[pl_being_exploited_id]
        for el in hp["meta_game_policy_distributions"]
    ]
    print("player_0_policies", player_0_policies)
    player_0_meta_policy = torch.stack(player_0_policies, dim=0).mean(dim=0)
    print("mean player_0_policies", player_0_meta_policy)

    return player_0_meta_policy


def _get_best_response(
    meta_payoff_mat_avg,
    player_0_meta_policy,
    tau=0.0,
    exploiter_idx=None,
    being_exploited_idx=None,
):
    meta_payoff_mat_avg = torch.tensor(meta_payoff_mat_avg)
    exploiter_payoffs = meta_payoff_mat_avg[..., exploiter_idx]
    print("player_0_meta_policy", player_0_meta_policy)
    matrix_opp_policy_prob = torch.stack(
        [player_0_meta_policy] * exploiter_payoffs.shape[0], dim=1
    )
    print("matrix_opp_policy_prob", matrix_opp_policy_prob)
    exploiter_expected_payoffs = matrix_opp_policy_prob * exploiter_payoffs
    print(
        "exploiter_expected_payoffs",
        exploiter_expected_payoffs,
        exploiter_expected_payoffs.shape,
    )
    mean_exploiter_payoff = exploiter_expected_payoffs.mean(dim=0)
    print("mean_exploiter_payoff", mean_exploiter_payoff)
    best_reponse_action = np.argmax(mean_exploiter_payoff)
    print("best_reponse_action", best_reponse_action)
    exploiter_payoff_in_br = (
        exploiter_payoffs[:, best_reponse_action] * player_0_meta_policy
    )
    being_exploited_payoff_in_br = (
        meta_payoff_mat_avg[..., being_exploited_idx][:, best_reponse_action]
        * player_0_meta_policy
    )
    being_exploited_payoff_in_br = being_exploited_payoff_in_br.sum()
    exploiter_payoff_in_br = exploiter_payoff_in_br.sum()
    if exploiter_idx == 1:
        return (
            best_reponse_action,
            being_exploited_payoff_in_br,
            exploiter_payoff_in_br,
        )
    elif exploiter_idx == 0:
        return (
            best_reponse_action,
            exploiter_payoff_in_br,
            being_exploited_payoff_in_br,
        )
    else:
        raise ValueError()


def _get_meta_payoff_matrices_and_policies(
    meta_game_algo,
    debug,
    seeds,
    exp_name,
    base_game_algo,
):
    hp = various_algo_meta_game._get_hyperparameters(
        debug, seeds, exp_name, base_game_algo, meta_game_algo
    )
    global payoffs_per_groups
    (
        hp["payoff_matrices"],
        hp["actions_possible"],
        hp["base_ckpt_per_replicat"],
        payoffs_per_groups,
    ) = various_algo_meta_game._form_n_matrices_from_base_game_payoffs(hp)
    hp = meta_solver_cross_play._load_meta_policies(hp, meta_game_algo)
    return hp


if __name__ == "__main__":
    debug_mode = False
    loop_over_main = True

    if loop_over_main:
        base_game_algo_to_eval = (
            BASE_LOLA_EXACT,
            # BASE_NEGOTIATION,
        )
        meta_game_algo_to_eval = (
            META_APLHA_RANK,
            META_APLHA_PURE,
            META_REPLICATOR_DYNAMIC,
            # META_REPLICATOR_DYNAMIC_ZERO_INIT,
            META_RANDOM,
            META_PG,
            # META_LOLA_EXACT,
            META_SOS,
            META_UNIFORM,
            META_MINIMUM,
        )
        pairs_seen = []
        results = []
        for base_game_algo_ in base_game_algo_to_eval:
            for meta_game_algo_ in meta_game_algo_to_eval:
                pair_result = []
                for exploiter_idx in range(2):
                    being_exploited_idx = (exploiter_idx + 1) % 2
                    pl_1_mean, pl_2_mean, pl_1_std_err, pl_2_std_err = main(
                        debug_mode,
                        base_game_algo_,
                        meta_game_algo_,
                        exploiter_idx,
                        being_exploited_idx,
                    )
                    pair_result.append(
                        {
                            "exploiter_player": exploiter_idx + 1,
                            "player_being_exploited": being_exploited_idx + 1,
                            "base_game": base_game_algo_,
                            "meta_game": meta_game_algo_,
                            "pl_1_mean": pl_1_mean,
                            "pl_2_mean": pl_2_mean,
                            "pl_1_std_err": pl_1_std_err,
                            "pl_2_std_err": pl_2_std_err,
                        }
                    )
                results.append(pair_result)

        print("final results")
        print(results)
    else:
        main(debug_mode)
