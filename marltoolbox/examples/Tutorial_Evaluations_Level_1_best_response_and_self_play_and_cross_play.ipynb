{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_Evaluations_Level_1_best_response_and_self_play_and_cross_play.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKYet7sZ4Itg"
      },
      "source": [
        "# Tutorial - Evaluations - Level 1 best-response and self-play and cross-play"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJnqi3Tt5PTN"
      },
      "source": [
        "## Install the toolbox (and Ray, Tune, RLLib, PyTorch, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bihmk2BFD1cv"
      },
      "source": [
        "We need python 3.6 to use the LOLA algorithm from the toolbox."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:18:33.641375Z",
          "start_time": "2021-03-03T16:18:33.634883Z"
        },
        "id": "KMXC7aH5CD8p"
      },
      "source": [
        "import sys\n",
        "print(\"python --version\", sys.version_info)\n",
        "if sys.version_info[0] != 3 or (sys.version_info[1] != 6 and sys.version_info[1] != 7) :\n",
        "    raise Exception(\"Must be using Python 3.6 or 3.7\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrTzb0VsFQYO"
      },
      "source": [
        "If you are running on Google Colab (which you should), uncomment the cell below to install the necessary dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:18:33.711939Z",
          "start_time": "2021-03-03T16:18:33.642757Z"
        },
        "id": "DsrzADRv4Itm"
      },
      "source": [
        "# print(\"Setting up colab environment\")\n",
        "\n",
        "# !pip uninstall -y pyarrow\n",
        "# !pip install bs4\n",
        "# !git clone https://github.com/longtermrisk/marltoolbox.git\n",
        "# !pip install -e marltoolbox/.[lola]\n",
        "# !pip uninstall -y dataclasses\n",
        "\n",
        "# # # A hack to force the runtime to restart, needed to include the above dependencies.\n",
        "# print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n",
        "# import os\n",
        "# os._exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jlW2Qb3BNqs"
      },
      "source": [
        "**After you run the cell above, comment all its lines.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX-suOml4Itn"
      },
      "source": [
        "## Plan  \n",
        "\n",
        "3. Using the experimentation tools in the toolbox  \n",
        "  a. Evaluate the self-play and cross-play performances  \n",
        "  b. Evaluate the exploitability of an algorithm using Level 1 Best-Response (L1BR)  \n",
        "  \n",
        "(Section 1 and 2 are in the tutorial: Basics - How to use the toolbox)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVHvW6PB_EBN"
      },
      "source": [
        "## Requirements\n",
        "\n",
        "You have done the first tutorial (Basics - How to use the toolbox)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGZWm3Op4Ito"
      },
      "source": [
        " # 3. Using the experimentation tools in the toolbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfu_VKVU4Ito"
      },
      "source": [
        " ## a. Evaluate the self-play and cross-play performances "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:18:36.714225Z",
          "start_time": "2021-03-03T16:18:33.714211Z"
        },
        "id": "BdtPEwKt-b47"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.evaluation.sample_batch_builder import MultiAgentSampleBatchBuilder\n",
        "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
        "from ray.rllib.agents.ppo import PPOTrainer, PPOTorchPolicy\n",
        "from ray.rllib.agents.ppo.ppo_torch_policy import setup_mixins\n",
        "\n",
        "from marltoolbox.utils import log, miscellaneous\n",
        "from marltoolbox.envs.matrix_sequential_social_dilemma import IteratedBoS\n",
        "from marltoolbox.utils import self_and_cross_perf, restore\n",
        "from marltoolbox.utils.plot import PlotConfig\n",
        "from marltoolbox.envs.utils.wrappers import add_RewardUncertaintyEnvClassWrapper\n",
        "from marltoolbox.utils.miscellaneous import check_learning_achieved"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3f0f36U-nwE"
      },
      "source": [
        "We need to train some agents with different random seeds to then compute their self-play and cross-play performances after deployment.\n",
        "\n",
        "We are going to train PPO agents on the BachOrStravinsky(BoS) game using the `RLLib` API.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:18:36.720479Z",
          "start_time": "2021-03-03T16:18:36.715682Z"
        },
        "id": "9fZtrKHovNvV"
      },
      "source": [
        "bos_env_payoffs = IteratedBoS({}).PAYOUT_MATRIX\n",
        "for a_1, action_player_1 in enumerate([\"Bach\",\"Stravinsky\"]):\n",
        "    for a_2, action_player_2 in enumerate([\"Bach\",\"Stravinsky\"]):\n",
        "        print(f\"Payoffs for action pair ({action_player_1},{action_player_2}): \" \n",
        "              f\"({bos_env_payoffs[a_1][a_2][0]},{bos_env_payoffs[a_1][a_2][1]})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akV87_3evRdb"
      },
      "source": [
        " Here is the `RLLib` configuration for such training, we will not detail it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:18:36.810178Z",
          "start_time": "2021-03-03T16:18:36.721978Z"
        },
        "id": "MHNIE5wp-nV8"
      },
      "source": [
        "def get_trainer_config(hp):\n",
        "    train_n_replicates = hp[\"train_n_replicates\"]\n",
        "    seeds = miscellaneous.get_random_seeds(train_n_replicates)\n",
        "    exp_name, _ = log.log_in_current_day_dir(\"PPO_BoS\")\n",
        "\n",
        "    # This modification to the policy will allow us to load each policies from different checkpoints \n",
        "    # This will be used during evaluation.\n",
        "    def merged_after_init(*args, **kwargs):\n",
        "      setup_mixins(*args, **kwargs)\n",
        "      restore.after_init_load_policy_checkpoint(*args, **kwargs)\n",
        "    MyPPOPolicy = PPOTorchPolicy.with_updates(after_init=merged_after_init)\n",
        "\n",
        "    stop_config = {\n",
        "        \"episodes_total\": hp[\"episodes_total\"],\n",
        "    }\n",
        "\n",
        "    env_config = {\n",
        "        \"players_ids\": [\"player_row\", \"player_col\"],\n",
        "        \"max_steps\": hp[\"steps_per_epi\"],  # Length of an episode\n",
        "    }\n",
        "\n",
        "    trainer_config = {\n",
        "        # We add some variance on the reward returned by the environment\n",
        "        \"env\": add_RewardUncertaintyEnvClassWrapper(\n",
        "                  IteratedBoS,\n",
        "                  reward_uncertainty_std=0.1),\n",
        "        \"env_config\": env_config,\n",
        "\n",
        "        \"multiagent\": {\n",
        "            \"policies\": {\n",
        "                env_config[\"players_ids\"][0]: (MyPPOPolicy,\n",
        "                                               IteratedBoS.OBSERVATION_SPACE,\n",
        "                                               IteratedBoS.ACTION_SPACE,\n",
        "                                               {}),\n",
        "                env_config[\"players_ids\"][1]: (MyPPOPolicy,\n",
        "                                               IteratedBoS.OBSERVATION_SPACE,\n",
        "                                               IteratedBoS.ACTION_SPACE,\n",
        "                                               {}),\n",
        "            },\n",
        "            \"policy_mapping_fn\": lambda agent_id: agent_id,\n",
        "        },\n",
        "\n",
        "        #### PPO config ####\n",
        "        # Size of batches collected from each worker.\n",
        "        \"rollout_fragment_length\": hp[\"steps_per_epi\"], \n",
        "        # Number of timesteps collected for each SGD round. This defines the size\n",
        "        # of each SGD epoch.\n",
        "        \"train_batch_size\": hp[\"steps_per_epi\"]*3, \n",
        "        # Total SGD batch size across all devices for SGD. This defines the\n",
        "        # minibatch size within each epoch.\n",
        "        \"sgd_minibatch_size\": hp[\"steps_per_epi\"],\n",
        "        # Number of SGD iterations in each outer loop (i.e., number of epochs to\n",
        "        # execute per train batch).\n",
        "        \"num_sgd_iter\": 3,\n",
        "        \"model\": {\n",
        "            # Number of hidden layers for fully connected net\n",
        "            \"fcnet_hiddens\": [4, 2],\n",
        "            # Nonlinearity for fully connected net (tanh, relu)\n",
        "            \"fcnet_activation\": \"relu\",\n",
        "        },\n",
        "\n",
        "\n",
        "        \"lr\": hp[\"base_lr\"],\n",
        "        \"lr_schedule\": [(0, hp[\"base_lr\"]),\n",
        "                (int(hp[\"steps_per_epi\"] * hp[\"episodes_total\"]), hp[\"base_lr\"] / 1e9)],\n",
        "    \n",
        "        \"seed\": tune.grid_search(seeds),\n",
        "        \"callbacks\": log.get_logging_callbacks_class(),\n",
        "        \"framework\": \"torch\",\n",
        "        \"num_workers\":0,\n",
        "    }\n",
        "\n",
        "    return trainer_config, env_config, stop_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmeYqMa00Gns"
      },
      "source": [
        "Let's train 8 pairs of PPO agents: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:19:29.188364Z",
          "start_time": "2021-03-03T16:18:36.811427Z"
        },
        "id": "XijF3Q7O0FxF"
      },
      "source": [
        "hyperparameters = {\n",
        "    \"steps_per_epi\": 20,\n",
        "    \"train_n_replicates\": 8,\n",
        "    \"episodes_total\": 200,\n",
        "    \"exp_name\": \"PPO_BoS\",\n",
        "    \"base_lr\": 5e-1,\n",
        "}\n",
        "\n",
        "trainer_config, _, stop_config = get_trainer_config(hyperparameters)\n",
        "ray.shutdown()\n",
        "ray.init(num_cpus=os.cpu_count(), num_gpus=0, local_mode=False)\n",
        "tune_analysis = tune.run(PPOTrainer, config=trainer_config, stop=stop_config,\n",
        "                    checkpoint_freq=0, checkpoint_at_end=True, name=hyperparameters[\"exp_name\"],\n",
        "                    metric=\"episode_reward_mean\", mode=\"max\")\n",
        "ray.shutdown()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6reFekyz6En"
      },
      "source": [
        "We now have 16 PPO agents trained with 8 differents random seeds, which perform well on BoS (check that values the \"reward\" columns are close to 100).  \n",
        "We will be able totat load these agents using the checkpoints we saved at the end of the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:19:29.205134Z",
          "start_time": "2021-03-03T16:19:29.190897Z"
        },
        "id": "e4qziRvL0o_E"
      },
      "source": [
        "print(\"location of the best checkpoint\",tune_analysis.best_checkpoint)\n",
        "tune_analysis_per_exp = {\"\": tune_analysis}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y9EBOZm9qz7"
      },
      "source": [
        "We will use the `SelfAndCrossPlayEvaluator` from the toolbox, to evaluate the self-play and cross-play performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:20:01.330274Z",
          "start_time": "2021-03-03T16:19:29.207786Z"
        },
        "id": "6ZyWFTfy-KSd"
      },
      "source": [
        "def evaluate_self_and_cross_perf(tune_analysis_per_exp, hp):\n",
        "    eval_config, env_config, stop_config, hp_eval = generate_eval_config(hp)\n",
        "\n",
        "    evaluator = self_and_cross_perf.SelfAndCrossPlayEvaluator(exp_name=hp_eval[\"exp_name\"])\n",
        "    analysis_metrics_per_mode = evaluator.perform_evaluation_or_load_data(\n",
        "        evaluation_config=eval_config, \n",
        "        stop_config=stop_config,\n",
        "        policies_to_load_from_checkpoint=copy.deepcopy(env_config[\"players_ids\"]),\n",
        "        tune_analysis_per_exp=tune_analysis_per_exp,\n",
        "        TrainerClass=PPOTrainer,\n",
        "        n_cross_play_per_checkpoint=2)\n",
        "\n",
        "    # Specify how to plot\n",
        "    plot_config = PlotConfig(xlim=hp_eval[\"x_limits\"], ylim=hp_eval[\"y_limits\"],\n",
        "                             markersize=5, alpha=1.0, jitter=hp_eval[\"jitter\"],\n",
        "                             xlabel=\"player 1 payoffs\", ylabel=\"player 2 payoffs\",\n",
        "                             title=\"self-play and cross-play performances: BoS\",\n",
        "                             x_scale_multiplier=hp_eval[\"scale_multipliers\"][0],\n",
        "                             y_scale_multiplier=hp_eval[\"scale_multipliers\"][1])\n",
        "    \n",
        "    evaluator.plot_results(analysis_metrics_per_mode, plot_config=plot_config,\n",
        "                           x_axis_metric=f\"policy_reward_mean/{env_config['players_ids'][0]}\",\n",
        "                           y_axis_metric=f\"policy_reward_mean/{env_config['players_ids'][1]}\")\n",
        "\n",
        "def generate_eval_config(hp):\n",
        "    \n",
        "    hp_eval = copy.deepcopy(hp)\n",
        "    hp_eval[\"steps_per_epi\"]= 20\n",
        "    hp_eval[\"episodes_total\"]= 1\n",
        "    hp_eval[\"scale_multipliers\"] = (1/hp_eval[\"steps_per_epi\"], 1/hp_eval[\"steps_per_epi\"])\n",
        "    hp_eval[\"base_lr\"]= 0.0\n",
        "    hp_eval[\"jitter\"]= 0.05\n",
        "    hp_eval[\"x_limits\"]= (-0.5,3.5)\n",
        "    hp_eval[\"y_limits\"]= (-0.5,3.5)\n",
        "\n",
        "    eval_config, env_config, stop_config = get_trainer_config(hp_eval)\n",
        "\n",
        "    eval_config[\"explore\"] = False\n",
        "    eval_config[\"seed\"] = miscellaneous.get_random_seeds(1)[0]\n",
        "    eval_config[\"train_batch_size\"] = hp_eval[\"steps_per_epi\"]\n",
        "\n",
        "    return eval_config, env_config, stop_config, hp_eval\n",
        "\n",
        "ray.shutdown()\n",
        "evaluate_self_and_cross_perf(tune_analysis_per_exp, hyperparameters)\n",
        "ray.shutdown()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsnOVlVwAPFS"
      },
      "source": [
        "We can see the self-play and cross-play performances in the plot. You should see some failures in cross-play (close to (0,0)).  \n",
        "Theses failures are explained by the fact that the PPO agents only learned to coordinate on playing either Bach or Stravinsky. They have not learned to adapt to a change of behavior in the other player."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgH0UUQ1RAcU"
      },
      "source": [
        "##  b. Evaluate the exploitability of an algorithm using Level 1 Best-Response (L1BR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:20:01.802924Z",
          "start_time": "2021-03-03T16:20:01.332817Z"
        },
        "id": "wmvRIbPqUAwy"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.agents.pg import PGTrainer, PGTorchPolicy\n",
        "from ray.rllib.utils.typing import TrainerConfigDict\n",
        "\n",
        "from marltoolbox.envs.matrix_sequential_social_dilemma import IteratedPrisonersDilemma\n",
        "from marltoolbox.utils import log, miscellaneous, exploration, lvl1_best_response, policy\n",
        "from marltoolbox.algos import population\n",
        "from marltoolbox.algos.lola.train_exact_tune_class_API import LOLAExact\n",
        "from marltoolbox.utils.miscellaneous import check_learning_achieved"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbAZZ_S_RZeF"
      },
      "source": [
        "We are going to see if `LOLAExact` is exploitable after deployement in the `IteratedPrisonersDilemma` environment. We will train two populations of agents. First, the level 0 agents will use the `LOLA-Exact` policy and will be trained in self-play. Then we will freeze their weights like if they were deployed in production. And we will train level 1 PolicyGradient (PG) agents against this population of level 0 agents.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzaVT6SYwTNu"
      },
      "source": [
        "Here are the payoffs in the `IteratedPrisonersDilemma`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:20:01.809130Z",
          "start_time": "2021-03-03T16:20:01.804476Z"
        },
        "id": "bVqSBL72wHrd"
      },
      "source": [
        "ipd_env_payoffs = IteratedPrisonersDilemma({}).PAYOUT_MATRIX\n",
        "for a_1, action_player_1 in enumerate([\"Coop\",\"Defect\"]):\n",
        "    for a_2, action_player_2 in enumerate([\"Coop\",\"Defect\"]):\n",
        "        print(f\"payoffs for action pair ({action_player_1},{action_player_2}): \" \n",
        "              f\"({ipd_env_payoffs[a_1][a_2][0]},{ipd_env_payoffs[a_1][a_2][1]})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xyAY7QxTvwY"
      },
      "source": [
        "To train the level 0 `LOLAExact` agents. We use the `Tune` class API because the current implementation of `LOLAExact` doesn't follow the `RLLib` API. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:20:01.880502Z",
          "start_time": "2021-03-03T16:20:01.810732Z"
        },
        "id": "vMbfhunq_ben"
      },
      "source": [
        "def train_lvl0_agents(lvl0_hparameters):\n",
        "\n",
        "    tune_config = get_tune_config(lvl0_hparameters)\n",
        "    stop_config = get_stop_config(lvl0_hparameters)\n",
        "    ray.shutdown()\n",
        "    ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
        "    tune_analysis_lvl0 = tune.run(LOLAExact, name=\"Lvl0_LOLAExact\", config=tune_config,\n",
        "                                  checkpoint_at_end=True, stop=stop_config, \n",
        "                                  metric=lvl0_hparameters[\"metric\"], mode=\"max\")\n",
        "    ray.shutdown()\n",
        "    return tune_analysis_lvl0\n",
        "\n",
        "def get_tune_config(hp: dict) -> dict:\n",
        "    tune_config = copy.deepcopy(hp)\n",
        "    return tune_config\n",
        "\n",
        "def get_env_config(hp):\n",
        "    env_config = {\n",
        "        \"players_ids\": [\"player_row\", \"player_col\"],\n",
        "        \"max_steps\": hp[\"trace_length\"],\n",
        "        \"get_additional_info\": True,\n",
        "    }\n",
        "    return env_config\n",
        "\n",
        "def get_stop_config(hp):\n",
        "    stop_config = {\n",
        "        \"episodes_total\": hp[\"num_episodes\"]\n",
        "    }\n",
        "    return stop_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVcItfIxUIKO"
      },
      "source": [
        "We train 8 level 0 `LOLAExact` agents. This is going to take around 15 minutes. You can read in advance the next steps during this time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:37.068582Z",
          "start_time": "2021-03-03T16:20:01.882931Z"
        },
        "id": "Im5nFVs7YUtU"
      },
      "source": [
        "train_n_replicates = 8\n",
        "\n",
        "lvl0_hparameters = {\n",
        "    \"train_n_replicates\": train_n_replicates,\n",
        "    \"env\": \"IPD\",\n",
        "    \"num_episodes\": 50,\n",
        "    \"trace_length\": 200,\n",
        "    \"simple_net\": True,\n",
        "    \"corrections\": True,\n",
        "    \"pseudo\": False,\n",
        "    \"num_hidden\": 32,\n",
        "    \"reg\": 0.0,\n",
        "    \"lr\": 1.,\n",
        "    \"lr_correction\": 1.0,\n",
        "    \"gamma\": 0.96,\n",
        "    \"metric\": \"ret1\",\n",
        "\n",
        "    # We use tune hyperparameter search API to train several agents in parralel\n",
        "    \"seed\": tune.grid_search(miscellaneous.get_random_seeds(train_n_replicates)),\n",
        "}\n",
        "\n",
        "tune_analysis_lvl0 = train_lvl0_agents(lvl0_hparameters)\n",
        "\n",
        "check_learning_achieved(tune_results=tune_analysis_lvl0, metric=\"ret1\", \n",
        "                        min_=-1.5, trial_idx=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goz-zWzID08p"
      },
      "source": [
        "`LOLAExact` should learn to cooperate in `IteratedPrisonersDilemma`, the rewards of player 1 and 2 should be close to -1 (\"ret1\" and \"ret2\").  \n",
        "Yet `LOLAExact` regularly fails to cooperate. We are thus going to filter the failures:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:37.076982Z",
          "start_time": "2021-03-03T16:21:37.070563Z"
        },
        "id": "I5wI5Cay7aY6"
      },
      "source": [
        "filtered_tune_analysis_lvl0 = miscellaneous.filter_tune_results(\n",
        "    tune_analysis_lvl0,\n",
        "    metric=f\"ret1\",\n",
        "    metric_threshold=-1.4,\n",
        "    metric_mode=\"last\", \n",
        "    threshold_mode=\"above\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZRENNdiSLV4"
      },
      "source": [
        "We now have several pairs of `LOLAExact` agents trained in self-play. We are playing in `IteratedPrisonersDilemma` and thus we may fear that an opponent could exploit our agents after they have been deployed.   \n",
        "\n",
        "We are going to look at that precisely. We will train level 1 PolicyGradient agents that will learn while the `LOLAExact` agents are frozen (not learning any more). The PolicyGradient agents will learn by playing against a population of `LOLAExact` agents. This is used to simulate the fact that when training the exploiter, we may not know which `LOLAExact` agent will be in practice deployed and thus we want to produce an agent that would exploit any `LOLAExact` agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:37.527028Z",
          "start_time": "2021-03-03T16:21:37.078562Z"
        },
        "id": "P2loJhuzVho6"
      },
      "source": [
        "def train_lvl1_agents(hp_lvl1, tune_analysis_lvl0):\n",
        "\n",
        "    rllib_config_lvl1, trainable_class, env_config = get_rllib_config(hp_lvl1)\n",
        "    stop_config = get_stop_config(hp_lvl1)\n",
        "    \n",
        "    # We use an helper to extract all the checkpoints saved in the tune_analysis_lvl0\n",
        "    checkpoints_lvl0 = miscellaneous.extract_checkpoints(tune_analysis_lvl0)\n",
        "    \n",
        "    # We modify the rllib_config to use population of level 0 agents\n",
        "    rllib_config_lvl1 = modify_config_for_lvl1_training(hp_lvl1, env_config, rllib_config_lvl1, checkpoints_lvl0)\n",
        "\n",
        "    ray.shutdown()\n",
        "    ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
        "    tune_analysis_lvl1 = ray.tune.run(PGTrainer, config=rllib_config_lvl1,\n",
        "                                      stop=stop_config,\n",
        "                                      checkpoint_at_end=True,\n",
        "                                      metric=\"episode_reward_mean\", mode=\"max\",\n",
        "                                      name=\"Lvl1_PG\")\n",
        "    ray.shutdown()\n",
        "    return tune_analysis_lvl1\n",
        "\n",
        "\n",
        "def get_rllib_config(hp_eval):\n",
        "\n",
        "    env_config = get_env_config(hp_eval)\n",
        "    \n",
        "    tune_config = get_tune_config(hp_eval)\n",
        "    tune_config[\"TuneTrainerClass\"] = LOLAExact\n",
        "\n",
        "    rllib_config_lvl1 = {\n",
        "        \"env\": IteratedPrisonersDilemma,\n",
        "        \"env_config\": env_config,\n",
        "        \"multiagent\": {\n",
        "            \"policies\": {\n",
        "                env_config[\"players_ids\"][0]: (\n",
        "                    PGTorchPolicy,\n",
        "                    IteratedPrisonersDilemma.OBSERVATION_SPACE,\n",
        "                    IteratedPrisonersDilemma.ACTION_SPACE,\n",
        "                    {}),\n",
        "\n",
        "                env_config[\"players_ids\"][1]: (\n",
        "                    # We use a class to convert a Tune class into a frozen RLLib Policy\n",
        "                    policy.get_tune_policy_class(PGTorchPolicy),\n",
        "                    IteratedPrisonersDilemma.OBSERVATION_SPACE,\n",
        "                    IteratedPrisonersDilemma.ACTION_SPACE,\n",
        "                    # The tune_config contains the informations needed by the Tune class\n",
        "                    {\"tune_config\": copy.deepcopy(tune_config)}),\n",
        "            },\n",
        "            \"policy_mapping_fn\": lambda agent_id: agent_id,\n",
        "        },\n",
        "        \"seed\": hp_eval[\"seed\"],\n",
        "        \"min_iter_time_s\": hp_eval[\"min_iter_time_s\"],\n",
        "    }\n",
        "\n",
        "    policies_to_load = copy.deepcopy(env_config[\"players_ids\"])\n",
        "    trainable_class = LOLAExact\n",
        "    \n",
        "\n",
        "    return rllib_config_lvl1, trainable_class, env_config\n",
        "\n",
        "\n",
        "def modify_config_for_lvl1_training(hp_lvl1, env_config, rllib_config_lvl1, lvl0_checkpoints):\n",
        "    \n",
        "    # The level 0 agents will be player 2 and the level 1 agents will be player 1\n",
        "    lvl0_policy_idx = 1\n",
        "    lvl1_policy_idx = 0\n",
        "    lvl0_policy_id = env_config[\"players_ids\"][lvl0_policy_idx]\n",
        "    lvl1_policy_id = env_config[\"players_ids\"][lvl1_policy_idx]\n",
        "\n",
        "\n",
        "    # We add a callack needed by the PopulationOfIdenticalAlgo policy\n",
        "    rllib_config_lvl1[\"callbacks\"] = miscellaneous.merge_callbacks(\n",
        "        population.PopulationOfIdenticalAlgoCallBacks,\n",
        "        log.get_logging_callbacks_class(log_env_step=True, log_from_policy=True))\n",
        "    \n",
        "\n",
        "    # Finally, we use an helper to replace player_2's policy (LOLA-Exact) by a PopulationOfIdenticalAlgo policy \n",
        "    #   that use nested LOLA-Exact policies\n",
        "    # Before each episode, this PopulationOfIdenticalAlgo will switch between the LOLAExact agents available\n",
        "    l1br_configuration_helper = lvl1_best_response.L1BRConfigurationHelper(rllib_config_lvl1, lvl0_policy_id, lvl1_policy_id)\n",
        "    l1br_configuration_helper.define_exp(\n",
        "        use_n_lvl0_agents_in_each_population=hp_lvl1[\"n_seeds_lvl0\"] // hp_lvl1[\"n_seeds_lvl1\"],\n",
        "        train_n_lvl1_agents=hp_lvl1[\"n_seeds_lvl1\"],\n",
        "        lvl0_checkpoints=lvl0_checkpoints)\n",
        "    rllib_config_lvl1 = l1br_configuration_helper.prepare_config_for_lvl1_training()\n",
        "    \n",
        "\n",
        "    return rllib_config_lvl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvp_BOm1IuAZ"
      },
      "source": [
        "We train 2 level 1 PolicyGradient agents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:32.199951Z",
          "start_time": "2021-03-03T16:21:37.530347Z"
        },
        "id": "b2kmotZMYZBX",
        "scrolled": true
      },
      "source": [
        "lvl1_hparameters = copy.deepcopy(lvl0_hparameters)\n",
        "lvl1_hparameters.update({\n",
        "    \"n_seeds_lvl0\": len(filtered_tune_analysis_lvl0.trials),\n",
        "    \"n_seeds_lvl1\": 2,\n",
        "    \"min_iter_time_s\": 0.0,\n",
        "    \"batch_size\": 1, # To work with RLLib\n",
        "    \"num_episodes\": 1000,\n",
        "    \"trace_length\": 10,\n",
        "    \"seed\": None, # The seeds will be added by the L1BRConfigurationHelper\n",
        "    })\n",
        "\n",
        "tune_analysis_lvl1 = train_lvl1_agents(lvl1_hparameters, filtered_tune_analysis_lvl0)\n",
        "\n",
        "check_learning_achieved(tune_results=tune_analysis_lvl1, \n",
        "                        max_=-25, trial_idx=0)\n",
        "\n",
        "print(\"All metrics:\", list(tune_analysis_lvl1.results_df.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:32.340865Z",
          "start_time": "2021-03-03T16:22:32.202239Z"
        },
        "id": "M1lSyEf1D08q"
      },
      "source": [
        "print(\"Averaged state during the last episode for each seeds:\")\n",
        "print(\"Playing CC:\", tune_analysis_lvl1.results_df[\"custom_metrics.CC/player_col_mean\"].tolist())\n",
        "print(\"Playing CD:\", tune_analysis_lvl1.results_df[\"custom_metrics.CD/player_col_mean\"].tolist())\n",
        "print(\"Playing DC:\", tune_analysis_lvl1.results_df[\"custom_metrics.DC/player_col_mean\"].tolist())\n",
        "print(\"Playing DD:\", tune_analysis_lvl1.results_df[\"custom_metrics.DD/player_col_mean\"].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxqMUxpqD08q"
      },
      "source": [
        "Each indexes in these lists refers to one seed used during the level 1 training.  \n",
        "You should observe that the DC action pair is the most played. This means that most of the times, the level 1 agents defects (player 1 plays D) while the level 0 agents continues to cooperate (player 2 plays C).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:32.407054Z",
          "start_time": "2021-03-03T16:22:32.343988Z"
        },
        "id": "v67qWvcDD08q"
      },
      "source": [
        "print(\"Level 1 agents, player 1, mean rewards:\", tune_analysis_lvl1.results_df[\"policy_reward_mean.player_row\"].tolist())\n",
        "print(\"Level 0 agents, player 2, mean rewards:\", tune_analysis_lvl1.results_df[\"policy_reward_mean.player_col\"].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HqhzrhTD08r"
      },
      "source": [
        "`LOLAExact` is here exploited by level 1 PolicyGradient agents. This is confirmed by the rewards accumulated during the last episode (10 steps)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Cg-m_n4Itx"
      },
      "source": [
        "##  c. Use TensorBoard to visualize the trainings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BisqDo64Itx"
      },
      "source": [
        "You can uncomment and use TensorBoard to view trial performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:32.440171Z",
          "start_time": "2021-03-03T16:22:32.408487Z"
        },
        "id": "u00VE8oB4Itx"
      },
      "source": [
        "# %load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:32.515545Z",
          "start_time": "2021-03-03T16:22:32.443266Z"
        },
        "id": "NpeJySSk4Itx",
        "scrolled": true
      },
      "source": [
        "# %tensorboard --logdir /root/ray_results/ # On Google Colab\n",
        "# %tensorboard --logdir ~/ray_results/ # On your machine\n",
        "\n",
        "# You can filter the graphs with \".*mean.*|episode_reward_mean|ret1|ret2\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}