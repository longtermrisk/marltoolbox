{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial - Basics - How to use the toolbox.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKYet7sZ4Itg"
      },
      "source": [
        "# Tutorial: How to use the `marltoolbox`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wGpnJei4Itm"
      },
      "source": [
        "**Overview of the toolbox**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Goal**: Facilitate and speed up the research on bargaining in MARL. \n",
        "\n",
        "**Components:** We rely on two main components: \n",
        "- The [`Ray/Tune/RLLib` framework](https://docs.ray.io/en/master/rllib.html):\n",
        "  which we use as a research framework (and which is agnostic to the deep learning framework used). \n",
        "- A toolbox: this repository with specific contents related to bargaining in \n",
        "  MARL.\n",
        "\n",
        "**Concrete value of using `Tune`/`RLLib` + `marltoolbox`**:  \n",
        "- **with 1h of practice:**   \n",
        "    track your experiments, \n",
        "    log easily in TensorBoard, run hyperparameter search, \n",
        "    use the provided environments and run the provided algorithms, \n",
        "    mostly agnostic to the deep learning framework, \n",
        "    create custom algorithms using the `Tune` API \n",
        "- **with 10h of practice:**  \n",
        "    use some of the components of `RLLib` \n",
        "    (like using a PPO agent in your custom algorithms), use checkpoints, \n",
        "    use the experimentation tools provided here, create new environments, \n",
        "    create simple custom algorithm with the `RLLib` API\n",
        "- **with more than 10h of practice:**  \n",
        "    build custom distributed algorithms,\n",
        "    use all of the components of `RLLib`, \n",
        "    use the fully customizable training pipeline of `RLLib`,\n",
        "    create complex custom algorithm with the `RLLib` API  \n",
        "  \n",
        "**Code**: https://github.com/longtermrisk/marltoolbox/tree/master/marltoolbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJnqi3Tt5PTN"
      },
      "source": [
        "## Install the toolbox (and Ray, Tune, RLLib, PyTorch, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppl4AnD_FLvj"
      },
      "source": [
        "If you are running on Google Colab (which you should), uncomment the cell below to install the necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:20:59.364048Z",
          "start_time": "2021-03-03T16:20:59.360039Z"
        },
        "id": "DsrzADRv4Itm"
      },
      "source": [
        "# print(\"Setting up colab environment\")\n",
        "\n",
        "# !pip uninstall -y pyarrow\n",
        "# !pip install bs4\n",
        "# !git clone https://github.com/longtermrisk/marltoolbox.git\n",
        "# !pip install -e marltoolbox/.\n",
        "# !pip uninstall -y dataclasses\n",
        "\n",
        "# # Needed for TensorBoard\n",
        "# !pip install tensorflow\n",
        "\n",
        "# # # A hack to force the runtime to restart, needed to include the above dependencies.\n",
        "# print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n",
        "# import os\n",
        "# os._exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jlW2Qb3BNqs"
      },
      "source": [
        "**After you run the cell above, comment all its lines.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX-suOml4Itn"
      },
      "source": [
        "## Plan\n",
        "\n",
        "1. Running experiments using the `Tune` class API  \n",
        "  a. Using the `IteratedPrisonersDilemma` environment from `marltoolbox` and components from `RLLib`   \n",
        "  b. Using `Tune`'s hyperparameter search functionnality \n",
        "\n",
        "2. Running experiments using the `RLLib` API  \n",
        "  a. Using the `IteratedPrisonersDilemma` environment and the `LTFT` algorithm from `marltoolbox`    \n",
        "  b. Using some `RLLib` functionnalities    \n",
        "  c. Use TensorBoard to visualize the trainings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVHvW6PB_EBN"
      },
      "source": [
        "##Â Requirements\n",
        "\n",
        "Be sure to have read, at least, the following quick introductions to `Tune` and `RLLib`:  \n",
        "- Quick introduction to \n",
        "[`Tune`'s key concepts](https://docs.ray.io/en/master/tune/key-concepts.html) (< 5 min).  \n",
        "- Quick introduction \n",
        "[`RLlib` in 60 seconds](https://docs.ray.io/en/master/rllib.html#rllib-in-60-seconds) (< 5 min).  \n",
        "- The README of the `Ray`/`Tune`/`RLLib` project:\n",
        "[`Ray` github](https://github.com/ray-project/ray) (<5 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGZWm3Op4Ito"
      },
      "source": [
        "# 1. Running experiments using the `Tune` class API  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:04.982839Z",
          "start_time": "2021-03-03T16:20:59.373480Z"
        },
        "id": "BdtPEwKt-b47"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.agents.pg import PGTorchPolicy, DEFAULT_CONFIG\n",
        "from ray.rllib.evaluation.sample_batch_builder import MultiAgentSampleBatchBuilder\n",
        "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
        "\n",
        "from marltoolbox.envs.matrix_sequential_social_dilemma import IteratedPrisonersDilemma\n",
        "from marltoolbox.utils.miscellaneous import check_learning_achieved"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfu_VKVU4Ito"
      },
      "source": [
        " ## a. Using the `IteratedPrisonersDilemma` environment from `marltoolbox` and components from `RLLib`  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3f0f36U-nwE"
      },
      "source": [
        "We use the `Tune` class API, which requires a Trainable class with at minimum a setup and a step method, like that:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:04.997217Z",
          "start_time": "2021-03-03T16:21:04.985657Z"
        },
        "id": "MHNIE5wp-nV8"
      },
      "source": [
        "class TrainableWIP(tune.Trainable):\n",
        "    def setup(self, config):\n",
        "        # config (dict): A dict of hyperparameters\n",
        "        pass \n",
        "\n",
        "    def step(self):  # This is called iteratively to train the agents.\n",
        "        pass\n",
        "        return {\"fake_score\": self.fake_score}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y9EBOZm9qz7"
      },
      "source": [
        "We use the `IteratedPrisonersDilemma` environment (IPD) from the toolbox. This is a two player game.  \n",
        "Let's look at its payoffs (rewards) given the joint actions of the players:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:05.082297Z",
          "start_time": "2021-03-03T16:21:05.001068Z"
        },
        "id": "nEC3Fbp9trEZ"
      },
      "source": [
        "ipd_env_payoffs = IteratedPrisonersDilemma({}).PAYOUT_MATRIX\n",
        "for a_1, action_player_1 in enumerate([\"Coop\",\"Defect\"]):\n",
        "    for a_2, action_player_2 in enumerate([\"Coop\",\"Defect\"]):\n",
        "        print(f\"Payoffs for action pair ({action_player_1},{action_player_2}): \" \n",
        "              f\"({ipd_env_payoffs[a_1][a_2][0]},{ipd_env_payoffs[a_1][a_2][1]})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjvxM5aYu-14"
      },
      "source": [
        "Create the environment in the Trainable class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:05.187612Z",
          "start_time": "2021-03-03T16:21:05.085520Z"
        },
        "id": "6ZyWFTfy-KSd"
      },
      "source": [
        "class TrainableWIP(tune.Trainable):\n",
        "    def setup(self, config):\n",
        "        self.env = self._init_env(config)\n",
        "\n",
        "##### NEW ######\n",
        "    def _init_env(self, config):\n",
        "        return IteratedPrisonersDilemma(config[\"env_config\"])\n",
        "#####\n",
        "\n",
        "    def step(self): \n",
        "        pass\n",
        "        return {\"fake_score\": self.fake_score}\n",
        "\n",
        "##### NEW ######\n",
        "# This dict will be sent to the setup method by Tune when we will run the training.\n",
        "tune_config = {\n",
        "    \"env_config\": {\n",
        "        \"max_steps\": 10,  # Length of an episode\n",
        "    }\n",
        "}\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsnOVlVwAPFS"
      },
      "source": [
        "We create two simple PolicyGradient(PG) players using the `PGTorchPolicy` policy class from `RLLib`.  \n",
        "And we create a `MultiAgentSampleBatchBuilder` (from `RLLib`) to aggregate our data into batches. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:05.274954Z",
          "start_time": "2021-03-03T16:21:05.190978Z"
        },
        "id": "DrBiGbnSBFns"
      },
      "source": [
        "class TrainableWIP(tune.Trainable):\n",
        "    def setup(self, config):\n",
        "        self.env = self._init_env(config)\n",
        "        self.players = self._init_players(config)\n",
        "        self.multi_agent_batch_builder = self._init_batch_builder()\n",
        "        \n",
        "    def _init_env(self, config):\n",
        "        return IteratedPrisonersDilemma(config[\"env_config\"])\n",
        "\n",
        "##### NEW ######\n",
        "    def _init_players(self, config):\n",
        "        # We will use the default config provided for the PG policy by RLLib, \n",
        "        #   with a few modfications.\n",
        "        my_pg_config = DEFAULT_CONFIG\n",
        "        my_pg_config[\"gamma\"] = 0.5\n",
        "        my_pg_config[\"train_batch_size\"] = config[\"env_config\"][\"max_steps\"]\n",
        "\n",
        "        players = {}\n",
        "        for player_id in self.env.players_ids:\n",
        "            players[player_id] = PGTorchPolicy(self.env.OBSERVATION_SPACE, \n",
        "                                              self.env.ACTION_SPACE,\n",
        "                                              my_pg_config)\n",
        "        return players\n",
        "          \n",
        "    def _init_batch_builder(self):\n",
        "        return MultiAgentSampleBatchBuilder(\n",
        "            policy_map={player_id: player for player_id, player in self.players.items()},\n",
        "            clip_rewards=False,\n",
        "            callbacks=DefaultCallbacks()\n",
        "        )\n",
        "#####\n",
        "\n",
        "    def step(self):  # This is called iteratively.\n",
        "        pass\n",
        "        return {\"fake_score\": self.fake_score}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bec0882xClRv"
      },
      "source": [
        "We play one episode per call to `TrainableWIP.step`.  \n",
        "And then we report the total reward (of both players) averaged per environment step.  \n",
        "This information will be saved as a metric and displayed in TensorBoard as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:05.394885Z",
          "start_time": "2021-03-03T16:21:05.277702Z"
        },
        "id": "tB0ig1cuCkl7"
      },
      "source": [
        "class TrainableWIP(tune.Trainable):\n",
        "    def setup(self, config):\n",
        "        self.env = self._init_env(config)\n",
        "        self.players = self._init_players(config)\n",
        "        self.multi_agent_batch_builder = self._init_batch_builder()\n",
        "        self.total_welfare = None\n",
        "\n",
        "    def _init_env(self, config):\n",
        "        return IteratedPrisonersDilemma(config[\"env_config\"])\n",
        "\n",
        "    def _init_players(self, config):\n",
        "        my_pg_config = DEFAULT_CONFIG\n",
        "        my_pg_config[\"gamma\"] = 0.5\n",
        "        my_pg_config[\"train_batch_size\"] = config[\"env_config\"][\"max_steps\"]\n",
        "\n",
        "        players = {}\n",
        "        for player_id in self.env.players_ids:\n",
        "            players[player_id] = PGTorchPolicy(self.env.OBSERVATION_SPACE, \n",
        "                                              self.env.ACTION_SPACE,\n",
        "                                              my_pg_config)\n",
        "        return players\n",
        "          \n",
        "    def _init_batch_builder(self):\n",
        "        return MultiAgentSampleBatchBuilder(\n",
        "            policy_map={player_id: player for player_id, player in self.players.items()},\n",
        "            clip_rewards=False,\n",
        "            callbacks=DefaultCallbacks()\n",
        "        )\n",
        "\n",
        "##### NEW ######\n",
        "    def step(self):\n",
        "        self.to_report = {}\n",
        "        self._play_one_episode() \n",
        "        self.to_report[\"mean_welfare\"] = self.total_welfare / self.config[\"env_config\"][\"max_steps\"]\n",
        "        return self.to_report\n",
        "\n",
        "    def _play_one_episode(self):\n",
        "        obs_before_act = self.env.reset()\n",
        "        done = {\"__all__\": False}\n",
        "        self.total_welfare = 0.0\n",
        "        while not done[\"__all__\"]:\n",
        "            obs_after_act, actions, rewards, done = self._play_one_step(obs_before_act)\n",
        "            self._add_step_in_batch_builder_buffer(obs_before_act, actions, rewards, done)\n",
        "            obs_before_act = obs_after_act\n",
        "          \n",
        "    def _play_one_step(self, obs_before_act):\n",
        "        actions = {player_id: player_policy.compute_actions([obs_before_act[player_id]])[0][0] \n",
        "                                  for player_id, player_policy in self.players.items()}\n",
        "\n",
        "        obs_after_act, rewards, done, info = self.env.step(actions)\n",
        "        self.to_report.update(info)\n",
        "        \n",
        "        return obs_after_act, actions, rewards, done\n",
        "\n",
        "    def _add_step_in_batch_builder_buffer(self, obs_before_act, actions, rewards, done):\n",
        "        for player_id in self.players.keys():\n",
        "            self.total_welfare += rewards[player_id]\n",
        "\n",
        "            step_player_values = {\n",
        "                \"eps_id\": self.training_iteration,\n",
        "                \"obs\": obs_before_act[player_id],\n",
        "                \"actions\": actions[player_id],\n",
        "                \"rewards\": rewards[player_id],\n",
        "                \"dones\": done[player_id],\n",
        "            }\n",
        "            # The policy_id and agent_id used in the RLLib API are the same in our case (equal to player_id)\n",
        "            self.multi_agent_batch_builder.add_values(agent_id=player_id, policy_id=player_id, **step_player_values) \n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uEOXrUJCfre"
      },
      "source": [
        "Finally, after each epsisode we train the policies of both our agents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:05.504566Z",
          "start_time": "2021-03-03T16:21:05.397565Z"
        },
        "id": "s4G0pHVJCpGU"
      },
      "source": [
        "class Trainable(tune.Trainable):\n",
        "\n",
        "    def setup(self, config):\n",
        "        self.env = self._init_env(config)\n",
        "        self.players = self._init_players(config)\n",
        "        self.multi_agent_batch_builder = self._init_batch_builder()\n",
        "        \n",
        "    def _init_env(self, config):\n",
        "        return IteratedPrisonersDilemma(config[\"env_config\"])\n",
        "\n",
        "    def _init_players(self, config):\n",
        "        my_pg_config = DEFAULT_CONFIG\n",
        "        my_pg_config[\"gamma\"] = 0.5\n",
        "        my_pg_config[\"train_batch_size\"] = config[\"env_config\"][\"max_steps\"]\n",
        "\n",
        "        players = {}\n",
        "        for player_id in self.env.players_ids:\n",
        "            players[player_id] = PGTorchPolicy(self.env.OBSERVATION_SPACE, \n",
        "                                              self.env.ACTION_SPACE,\n",
        "                                              my_pg_config)\n",
        "        return players\n",
        "          \n",
        "    def _init_batch_builder(self):\n",
        "        return MultiAgentSampleBatchBuilder(\n",
        "            policy_map={player_id: player for player_id, player in self.players.items()},\n",
        "            clip_rewards=False,\n",
        "            callbacks=DefaultCallbacks()\n",
        "        )\n",
        "\n",
        "    def step(self):\n",
        "        self.to_report = {}\n",
        "\n",
        "        self._play_one_episode()\n",
        "        self._optimize_weights()\n",
        "\n",
        "        self.to_report[\"mean_welfare\"] = self.total_welfare / self.config[\"env_config\"][\"max_steps\"]\n",
        "        self.to_report[\"training_iteration\"] = self.training_iteration # This is an attribute from tune.Trainable \n",
        "        return self.to_report \n",
        "\n",
        "    def _play_one_episode(self):\n",
        "        obs_before_act = self.env.reset()\n",
        "        done = {\"__all__\": False}\n",
        "        self.total_welfare = 0.0\n",
        "        while not done[\"__all__\"]:\n",
        "            obs_after_act, actions, rewards, done = self._play_one_step(obs_before_act)\n",
        "            self._add_step_in_batch_builder_buffer(obs_before_act, actions, rewards, done)\n",
        "            obs_before_act = obs_after_act\n",
        "      \n",
        "    def _play_one_step(self, obs_before_act):\n",
        "        actions = {player_id: player_policy.compute_actions([obs_before_act[player_id]])[0][0] \n",
        "            for player_id, player_policy in self.players.items()}\n",
        "\n",
        "        obs_after_act, rewards, done, info = self.env.step(actions)\n",
        "        self.to_report.update(info)\n",
        "\n",
        "        return obs_after_act, actions, rewards, done\n",
        "        \n",
        "    def _add_step_in_batch_builder_buffer(self, obs_before_act, actions, rewards, done):\n",
        "        for player_id in self.players.keys():\n",
        "            self.total_welfare += rewards[player_id]\n",
        "\n",
        "            step_player_values = {\n",
        "                \"eps_id\": self.training_iteration,\n",
        "                \"obs\": obs_before_act[player_id],\n",
        "                \"actions\": actions[player_id],\n",
        "                \"rewards\": rewards[player_id],\n",
        "                \"dones\": done[player_id],\n",
        "            }\n",
        "            self.multi_agent_batch_builder.add_values(agent_id=player_id, policy_id=player_id, **step_player_values) \n",
        "\n",
        "##### NEW ######\n",
        "    def _optimize_weights(self):\n",
        "        \n",
        "        multiagent_batch = self.multi_agent_batch_builder.build_and_reset()\n",
        "        for player_id, player in self.players.items():\n",
        "            multiagent_batch = self._center_reward(multiagent_batch, player_id)\n",
        "            stats = player.learn_on_batch(multiagent_batch.policy_batches[player_id])\n",
        "\n",
        "    def _center_reward(self, multiagent_batch, player_id):\n",
        "        multiagent_batch.policy_batches[player_id][\"rewards\"] = (multiagent_batch.policy_batches[player_id][\"rewards\"] - \n",
        "                                                        multiagent_batch.policy_batches[player_id][\"rewards\"].mean())\n",
        "        return multiagent_batch\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvb080G9CM0x"
      },
      "source": [
        "We can now run this experiment with `Tune`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:21:36.891482Z",
          "start_time": "2021-03-03T16:21:05.508765Z"
        },
        "id": "ST5qb68DCMI7",
        "scrolled": true
      },
      "source": [
        "tune_config = {\n",
        "    \"env_config\": {\n",
        "        \"max_steps\": 10, # Length of an episode\n",
        "    }\n",
        "}\n",
        "\n",
        "# stop the training after N Trainable.step (here this is equivalent to N episodes and N updates)\n",
        "stop_config = {\"training_iteration\": 200} \n",
        "\n",
        "# Restart Ray defensively in case the ray connection is lost.\n",
        "ray.shutdown() \n",
        "ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
        "# Run the experiment\n",
        "tune_analysis = tune.run(\n",
        "    Trainable,\n",
        "    stop=stop_config,\n",
        "    config=tune_config,\n",
        "    name=\"PG_IPD\",\n",
        "    )\n",
        "\n",
        "ray.shutdown()\n",
        "\n",
        "check_learning_achieved(tune_results=tune_analysis, metric=\"mean_welfare\", max_=-3.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcsUOT-HD2Kf"
      },
      "source": [
        "You should get a mean_welfare close to -4, which means that both players are defecting and they both get a reward of -2 per step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgH0UUQ1RAcU"
      },
      "source": [
        "##  b. Using `Tune`'s hyperparameter search functionnality\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbAZZ_S_RZeF"
      },
      "source": [
        "We are going to do a simple hyperparameter grid search using `Tune`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:00.841656Z",
          "start_time": "2021-03-03T16:21:36.895560Z"
        },
        "id": "vMbfhunq_ben"
      },
      "source": [
        "tune_config = {\n",
        "    \"env_config\": {\n",
        "        \"max_steps\": 10,\n",
        "    }\n",
        "}\n",
        "\n",
        "##### NEW ######\n",
        "# Usually hyperparameter searches are done in the tune_config dictionary \n",
        "# but here varying \"training_iteration\" is interesting.\n",
        "stop_config = {\"training_iteration\": tune.grid_search([2, 4, 8, 16, 32, 64, 128])} \n",
        "#####\n",
        "\n",
        "ray.shutdown() \n",
        "ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
        "tune_analysis = tune.run(\n",
        "    Trainable,\n",
        "    stop=stop_config,\n",
        "    config=tune_config,\n",
        "    name=\"PG_IPD\",\n",
        "    )\n",
        "ray.shutdown()\n",
        "\n",
        "check_learning_achieved(tune_results=tune_analysis, metric=\"mean_welfare\", max_=-3.5, trial_idx=6)\n",
        "check_learning_achieved(tune_results=tune_analysis, metric=\"mean_welfare\", max_=-2.5, trial_idx=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZRENNdiSLV4"
      },
      "source": [
        "As expected, the more we train, the worst we are!   \n",
        "This is expected since we are playing on the `IteratedPrisonersDilemma` environment with selfish agents.  \n",
        "Both agents are slowly learning to defect.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTPki_b7t8iC"
      },
      "source": [
        "All hyperparameter search spaces available in `Tune` are listed at [`Tune`'s search-spaces](https://docs.ray.io/en/master/tune/key-concepts.html#search-spaces)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVLRgZrAZLvf"
      },
      "source": [
        "# 2. Running experiments using the `RLLib` API  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:00.865775Z",
          "start_time": "2021-03-03T16:22:00.843884Z"
        },
        "id": "9uHr3nuI-Lap"
      },
      "source": [
        "import os \n",
        "import torch\n",
        "import copy\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.policy.policy import Policy\n",
        "from ray.rllib.utils import merge_dicts\n",
        "from ray.rllib.utils.schedules import PiecewiseSchedule\n",
        "from ray.rllib.utils.typing import TrainerConfigDict\n",
        "\n",
        "from marltoolbox.envs.matrix_sequential_social_dilemma import IteratedPrisonersDilemma\n",
        "from marltoolbox.algos.ltft import LTFTCallbacks, LTFTTrainer, prepare_default_config\n",
        "from marltoolbox.utils import log, miscellaneous, exploration\n",
        "from marltoolbox.envs.utils.wrappers import add_RewardUncertaintyEnvClassWrapper\n",
        "from marltoolbox.utils.miscellaneous import check_learning_achieved"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_eyBAAGaOB8"
      },
      "source": [
        "##  a. Using the `IteratedPrisonersDilemma` environment and the `LTFT` algorithm from `marltoolbox`  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jefGxUbqb0TC"
      },
      "source": [
        "We are going to train two `LTFT` agents in the `IteratedPrisonersDilemma` environment (both from the toolbox). \n",
        "\n",
        "Instead of creating our own Trainable class like we did with the `Tune` API, when using the `RLLib` API we provide an `RLLib` Trainable class and customize it extensively. \n",
        "Inside a configuration dictionnary, we provide everything like the environment, the exploration policy, etc..., and we can customize the agent's policies.  \n",
        "\n",
        "Let's configure that!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-g2yQI2AT7u"
      },
      "source": [
        "Configure the policies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:00.965263Z",
          "start_time": "2021-03-03T16:22:00.868567Z"
        },
        "id": "mbxWsThD-vEt"
      },
      "source": [
        "def get_rllib_config_WIP(hyperparameters: dict)-> dict:\n",
        "\n",
        "    rllib_config = {}\n",
        "    rllib_config.update(get_policies_config(hyperparameters))   \n",
        "    # ...\n",
        "    # ...\n",
        "    # ...\n",
        "    # ...\n",
        "\n",
        "\n",
        "    return rllib_config\n",
        "\n",
        "##### NEW ######\n",
        "def get_policies_config(hp):\n",
        "\n",
        "    # We will need to use the LTFTTrainer to manage the dataflow\n",
        "    # and we use the DEFAULT_CONFIG from this Trainer\n",
        "    policies_config = prepare_default_config(\n",
        "        lr=hp[\"base_lr\"],\n",
        "        lr_spl=hp[\"base_lr\"] * hp[\"spl_lr_mul\"],\n",
        "        n_epi=hp[\"n_epi\"],\n",
        "        n_steps_per_epi=hp[\"n_steps_per_epi\"])\n",
        "    \n",
        "    policies_config.update({\n",
        "        # Inside the \"multiagent\" key of the RLLib config dict, we define all the policies that are going to be used\n",
        "        \"multiagent\": {\n",
        "            \"policies\": {\n",
        "                \"player_row\": (\n",
        "                    # The default policy is LTFTTorchPolicy (as defined in our Trainable class: LTFTTrainer) \n",
        "                    None,\n",
        "                    IteratedPrisonersDilemma.OBSERVATION_SPACE,\n",
        "                    IteratedPrisonersDilemma.ACTION_SPACE,\n",
        "                    # We can provide an additionnal configuration dict to this policy. \n",
        "                    #   It will be merged with a copy of the rllib_config that we are currenlty creating.\n",
        "                    {}),\n",
        "                \"player_col\": (\n",
        "                    None,\n",
        "                    IteratedPrisonersDilemma.OBSERVATION_SPACE,\n",
        "                    IteratedPrisonersDilemma.ACTION_SPACE,\n",
        "                    {}),\n",
        "            },\n",
        "            # We need to define how the agent_id used in the environment (dict keys) will be associated \n",
        "            #  to the policy_id of the policies above (dict keys). Here they are simply identical.\n",
        "            \"policy_mapping_fn\": lambda agent_id: agent_id,\n",
        "        },\n",
        "\n",
        "        # We add some callbacks needed by the LTFT policy and ask for additionnal logs.\n",
        "        \"callbacks\": miscellaneous.merge_callbacks(LTFTCallbacks,\n",
        "                                                   log.get_logging_callbacks_class(\n",
        "                                                       log_env_step=True,\n",
        "                                                       log_from_policy=True)),\n",
        "    })\n",
        "    return policies_config\n",
        "\n",
        "\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-zRmL2YAPGI"
      },
      "source": [
        "Configure the environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:01.059449Z",
          "start_time": "2021-03-03T16:22:00.967357Z"
        },
        "id": "4-VPfgGP9-XH"
      },
      "source": [
        "def get_rllib_config_WIP(hyperparameters: dict)-> dict:\n",
        "\n",
        "    rllib_config = {}\n",
        "    rllib_config.update(get_policies_config(hyperparameters))   \n",
        "    rllib_config.update(get_env_config(hyperparameters))\n",
        "    # ...\n",
        "    # ...\n",
        "    # ...\n",
        "    \n",
        "    return rllib_config\n",
        "\n",
        "def get_env_config(hp):\n",
        "    env_config = {\n",
        "        # We provide the environment class\n",
        "        \"env\": get_env_class(),\n",
        "        # And the dictionnary that will be sent to initialize the environment\n",
        "        \"env_config\": {\n",
        "            \"players_ids\": [\"player_row\", \"player_col\"],\n",
        "            \"max_steps\": hp[\"n_steps_per_epi\"],  # Length of an episode\n",
        "        },\n",
        "    }\n",
        "    return env_config\n",
        "\n",
        "def get_env_class():\n",
        "    # We add a wrapper around the environment to add some variance to the rewards returned\n",
        "    MyUncertainIPD = add_RewardUncertaintyEnvClassWrapper(\n",
        "        IteratedPrisonersDilemma,\n",
        "        reward_uncertainty_std=0.1)\n",
        "    return MyUncertainIPD\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zSAw-PjAXMu"
      },
      "source": [
        "Configure the default `DQN` policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:01.160306Z",
          "start_time": "2021-03-03T16:22:01.061174Z"
        },
        "id": "gej3suhHAo0S"
      },
      "source": [
        "def get_rllib_config_WIP(hyperparameters: dict)-> dict:\n",
        "\n",
        "    rllib_config = {}\n",
        "    rllib_config.update(get_policies_config(hyperparameters))   \n",
        "    rllib_config.update(get_env_config(hyperparameters))\n",
        "    rllib_config.update(get_default_DQN_config(hyperparameters))\n",
        "    # ...\n",
        "    # ...\n",
        "    \n",
        "    return rllib_config\n",
        "\n",
        "##### NEW ######\n",
        "def get_default_DQN_config(hp):\n",
        "    # The LTFT policy uses three DQN policies and a supervised learning policy.\n",
        "    # We provide here some additional configuration for the DQN policies \n",
        "    # (this additional configuration will also be sent to the supervised learning policy \n",
        "    # which will ignore it)\n",
        "\n",
        "    default_DQN_config = {\n",
        "        # === DQN Models ===\n",
        "        # Minimum env steps to optimize for per train call. This value does\n",
        "        # not affect learning, only the length of iterations.\n",
        "        \"timesteps_per_iteration\": hp[\"n_steps_per_epi\"],\n",
        "        # Update the target network every `target_network_update_freq` steps.\n",
        "        \"target_network_update_freq\": hp[\"n_steps_per_epi\"],\n",
        "        # === Replay buffer ===\n",
        "        # Size of the replay buffer. Note that if async_updates is set, then\n",
        "        # each worker will have a replay buffer of this size.\n",
        "        \"buffer_size\": int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"]),\n",
        "        # Whether to use dueling dqn\n",
        "        \"dueling\": False,\n",
        "        # Dense-layer setup for each the advantage branch and the value branch\n",
        "        # in a dueling architecture.\n",
        "        \"hiddens\": [32],\n",
        "        # Whether to use double dqn\n",
        "        \"double_q\": False,\n",
        "        # If True prioritized replay buffer will be used.\n",
        "        \"prioritized_replay\": False,\n",
        "        \"model\": {\n",
        "            # Number of hidden layers for fully connected net\n",
        "            \"fcnet_hiddens\": [32, 2],\n",
        "            # Nonlinearity for fully connected net (tanh, relu)\n",
        "            \"fcnet_activation\": \"relu\",\n",
        "        },\n",
        "    }\n",
        "    return default_DQN_config\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UvhCvbtAuPO"
      },
      "source": [
        "Configure the exploration policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:01.253341Z",
          "start_time": "2021-03-03T16:22:01.162680Z"
        },
        "id": "aJIHVPLjbfiC"
      },
      "source": [
        "def get_rllib_config_WIP(hyperparameters: dict)-> dict:\n",
        "\n",
        "    rllib_config = {}\n",
        "    rllib_config.update(get_policies_config(hyperparameters))   \n",
        "    rllib_config.update(get_env_config(hyperparameters))\n",
        "    rllib_config.update(get_default_DQN_config(hyperparameters))\n",
        "    rllib_config.update(get_exploration_config(hyperparameters))\n",
        "    # ...\n",
        "    \n",
        "    return rllib_config\n",
        "\n",
        "##### NEW ######\n",
        "def get_exploration_config(hp):\n",
        "    exploration_config = {\n",
        "        # === Exploration Settings ===\n",
        "        # Set to False for no exploration behavior (e.g., for evaluation).\n",
        "        \"explore\": True,\n",
        "        # Provide a dict specifying the Exploration object's config.\n",
        "        \"exploration_config\": {\n",
        "            # The Exploration class to use. In the simplest case, this is the name\n",
        "            # (str) of any class present in the `rllib.utils.exploration` package.\n",
        "            # You can also provide the python class directly or the full location\n",
        "            # of your class (e.g. \"ray.rllib.utils.exploration.epsilon_greedy.\n",
        "            # EpsilonGreedy\").\n",
        "            \"type\": exploration.SoftQSchedule,\n",
        "            # Add constructor kwargs here (if any).\n",
        "            \"temperature_schedule\": PiecewiseSchedule(\n",
        "                endpoints=[\n",
        "                    (0, 1.0), (int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"] * 0.75), 0.1)],\n",
        "                outside_value=0.1,\n",
        "                framework=\"torch\")\n",
        "        },\n",
        "\n",
        "    }\n",
        "\n",
        "    return exploration_config\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLHzzUrXfCoP"
      },
      "source": [
        "Configure the optimization and others:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:22:01.345715Z",
          "start_time": "2021-03-03T16:22:01.255659Z"
        },
        "id": "qqlYU7ZiALxl"
      },
      "source": [
        "def get_rllib_config(hyperparameters: dict)-> dict:\n",
        "\n",
        "    rllib_config = {}\n",
        "    rllib_config.update(get_policies_config(hyperparameters))   \n",
        "    rllib_config.update(get_env_config(hyperparameters))\n",
        "    rllib_config.update(get_default_DQN_config(hyperparameters))\n",
        "    rllib_config.update(get_exploration_config(hyperparameters))\n",
        "    rllib_config.update(get_optimization_and_general_config(hyperparameters))\n",
        "\n",
        "    return rllib_config\n",
        "\n",
        "##### NEW ######\n",
        "def get_optimization_and_general_config(hp: dict):\n",
        "\n",
        "    optim_and_general_config = {\n",
        "        \n",
        "        # === Optimization ===\n",
        "        # Learning rate for adam optimizer\n",
        "        \"lr\": hp[\"base_lr\"],\n",
        "        # Learning rate schedule\n",
        "        \"lr_schedule\": [(0, hp[\"base_lr\"]),\n",
        "                        (int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"]), hp[\"base_lr\"] / 1e9)],\n",
        "        # How many steps of the model to sample before learning starts.\n",
        "        \"learning_starts\": int(hp[\"n_steps_per_epi\"] * hp[\"bs_epi_mul\"]),\n",
        "        # Update the replay buffer with this many samples at once. Note that\n",
        "        # this setting applies per-worker if num_workers > 1.\n",
        "        \"rollout_fragment_length\": hp[\"n_steps_per_epi\"],\n",
        "        # Size of a batch sampled from replay buffer for training. Note that\n",
        "        # if async_updates is set, then each worker returns gradients for a\n",
        "        # batch of this size.\n",
        "        \"train_batch_size\": int(hp[\"n_steps_per_epi\"] * hp[\"bs_epi_mul\"]),\n",
        "        \"gamma\": 0.5,\n",
        "\n",
        "        # === General config ===\n",
        "        \"framework\": \"torch\",\n",
        "        \"batch_mode\": \"complete_episodes\",\n",
        "        # LTFT supports only 1 worker only otherwise it would be mixing several opponents trajectories\n",
        "        \"num_workers\": 0,\n",
        "        # LTFT supports only 1 env per worker only otherwise several episodes would be played at the same time\n",
        "        \"num_envs_per_worker\": 1,\n",
        "        \"seed\": tune.grid_search(hp[\"seeds\"]),\n",
        "\n",
        "    }\n",
        "\n",
        "    return optim_and_general_config\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQC1SE5tfasX"
      },
      "source": [
        "Start the training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:23:26.095903Z",
          "start_time": "2021-03-03T16:22:01.347987Z"
        },
        "id": "bfEaO43v3UBB"
      },
      "source": [
        "def get_stop_config(hp):\n",
        "    stop_config = {\n",
        "        \"episodes_total\": hp[\"n_epi\"], \n",
        "    }\n",
        "    return stop_config\n",
        "\n",
        "ltft_hparameters = {\n",
        "    \"n_epi\": 400,\n",
        "    \"n_steps_per_epi\": 20,\n",
        "    \"bs_epi_mul\": 4,\n",
        "    \"base_lr\": 0.04,\n",
        "    \"spl_lr_mul\": 10.0,\n",
        "    \"seeds\": miscellaneous.get_random_seeds(1),\n",
        "    \"debug\": False,\n",
        "}\n",
        "\n",
        "\n",
        "rllib_config = get_rllib_config(ltft_hparameters)\n",
        "stop_config = get_stop_config(ltft_hparameters)\n",
        "ray.shutdown()\n",
        "ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
        "tune_analysis_self_play = ray.tune.run(LTFTTrainer, config=rllib_config,\n",
        "                        checkpoint_freq=0, stop=stop_config, \n",
        "                        checkpoint_at_end=False, name=\"LTFT_exp\")\n",
        "ray.shutdown()\n",
        "\n",
        "check_learning_achieved(tune_results=tune_analysis_self_play, \n",
        "                        min_=-42, trial_idx=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adTH0zX-QtBY"
      },
      "source": [
        "`LTFT` agents should learn to cooperate and we should reach a \"reward\" close to -40. This is the sum for both players's rewards during an entire episode.  \n",
        "Averaged by 20 steps, this gives use -2 per step which is the best possible welfare in the `IteratedPrisonersDilemma` environment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1NySahgaPQx"
      },
      "source": [
        "##  b. Using some `RLLib` functionnalities  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w57wPt7Ydu4n"
      },
      "source": [
        "We can easily change the model and dataflow used by `RLLib` policies by changing the configuration dict. Let try to reduce the training time.  \n",
        "\n",
        "We are going to make the following changes:  \n",
        "- using a smaller network\n",
        "- using Dueling Double DQN (D3QN) instead of DQN.\n",
        "- training for less episodes\n",
        "- training with less steps per episodes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:23:26.104599Z",
          "start_time": "2021-03-03T16:23:26.098036Z"
        },
        "id": "yCZzbx3Ld0vz"
      },
      "source": [
        "def get_default_DQN_config(hp):\n",
        "    default_DQN_config = {\n",
        "        \"timesteps_per_iteration\": hp[\"n_steps_per_epi\"],\n",
        "        \"target_network_update_freq\": hp[\"n_steps_per_epi\"],\n",
        "        \"prioritized_replay\": False,\n",
        "\n",
        "        ##### MODIFIED ######\n",
        "        \"buffer_size\": int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"]),\n",
        "        \"dueling\": True, # instead of False\n",
        "        \"hiddens\": [4], # instead of 32\n",
        "        \"double_q\": True, # instead of False\n",
        "        \"model\": {\n",
        "            \"fcnet_hiddens\": [4, 2], # instead of [32, 2]\n",
        "            \"fcnet_activation\": \"relu\",\n",
        "        },\n",
        "        #####\n",
        "    }\n",
        "    return default_DQN_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:23:55.096156Z",
          "start_time": "2021-03-03T16:23:26.107174Z"
        },
        "id": "zETjK-Y5ZKLA"
      },
      "source": [
        "def get_stop_config(hp):\n",
        "    stop_config = {\n",
        "        \"episodes_total\": hp[\"n_epi\"],\n",
        "    }\n",
        "    return stop_config\n",
        "\n",
        "ltft_hparameters = {\n",
        "    ##### MODIFIED ######\n",
        "    \"n_epi\": 200,  # instead of 400\n",
        "    \"n_steps_per_epi\": 10, # instead of 20\n",
        "    #####\n",
        "    \"bs_epi_mul\": 4,\n",
        "    \"base_lr\": 0.04,\n",
        "    \"spl_lr_mul\": 10.0,\n",
        "    \"seeds\": miscellaneous.get_random_seeds(1),\n",
        "    \"debug\": False,\n",
        "}\n",
        "\n",
        "\n",
        "rllib_config = get_rllib_config(ltft_hparameters)\n",
        "stop_config = get_stop_config(ltft_hparameters)\n",
        "ray.shutdown()\n",
        "ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
        "tune_analysis_self_play = ray.tune.run(LTFTTrainer, config=rllib_config,\n",
        "                        checkpoint_freq=0, stop=stop_config, \n",
        "                        checkpoint_at_end=False, name=\"LTFT_exp\")\n",
        "ray.shutdown()\n",
        "\n",
        "check_learning_achieved(tune_results=tune_analysis_self_play,\n",
        "                        min_=-22, trial_idx=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kn_Ev7fcPNk"
      },
      "source": [
        "Our training time is now around 15 seconds long while previously it was around 35 seconds (these values depend on which machine Google Colab is running).   \n",
        "\n",
        "And we still achieve cooperation since the welfare (total reward) per step is still -2.\n",
        "\n",
        "If you want, cou can try to determine which modification was the most important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Cg-m_n4Itx"
      },
      "source": [
        "##  c. Use TensorBoard to visualize the trainings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BisqDo64Itx"
      },
      "source": [
        "You can uncomment and use TensorBoard to view trial performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:23:55.100316Z",
          "start_time": "2021-03-03T16:23:55.097863Z"
        },
        "id": "u00VE8oB4Itx"
      },
      "source": [
        "# %load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-03T16:23:55.191184Z",
          "start_time": "2021-03-03T16:23:55.101876Z"
        },
        "id": "NpeJySSk4Itx",
        "scrolled": true
      },
      "source": [
        "# %tensorboard --logdir /root/ray_results/ # On Google Colab\n",
        "# %tensorboard --logdir ~/ray_results/ # On your machine\n",
        "\n",
        "# You can filter the graphs with \"reward|mean_welfare|defection_metric|entropy|CC\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}