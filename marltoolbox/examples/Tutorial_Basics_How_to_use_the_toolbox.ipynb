{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKYet7sZ4Itg"
   },
   "source": [
    "# Tutorial: How to use the `marltoolbox`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wGpnJei4Itm"
   },
   "source": [
    "**Overview of the toolbox**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Major features of this toolbox:**  \n",
    "This toolbox contains algorithms, environments, evaluation tools, and \n",
    "helper functions to conduct research on bargaining in MARL.\n",
    "\n",
    "**Additional features of using the `Ray/Tune/RLLib` research framework:**  \n",
    "This toolbox relies on the [`Ray/Tune/RLLib` framework](https://docs.ray.io/en/master/rllib.html) \n",
    "to provide the basic RL components and research functionalities.   \n",
    "- using components from `RLLib` with extensive configuration available\n",
    "  (e.g. using a PPO policy or a priority replay buffer)\n",
    "- track your experiments, log easily in TensorBoard, run hyperparameter search\n",
    "- be agnostic to the deep learning framework\n",
    "- create new algorithms using the very simple `Tune` API or the `RLLib` API\n",
    "- use the `RLLib` API to take advantage of a fully customizable training pipeline\n",
    "- create distributed algorithms (e.g. by using the policy factory of `RLLib`)  \n",
    "\n",
    "\n",
    "**Philosophy**: Implement when needed.\n",
    "Improve at each new use. Keep it simple. Keep it flexible. \n",
    "Keep the maintenance cost low.  \n",
    "\n",
    "**Support**: We <ins>actively support</ins> researchers by adding tools that they see relevant for research on \n",
    "bargaining \n",
    "in MARL.  \n",
    "\n",
    "  \n",
    "**Code**: https://github.com/longtermrisk/marltoolbox/tree/master/marltoolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJnqi3Tt5PTN"
   },
   "source": [
    "## Install the toolbox (and Ray, Tune, RLLib, PyTorch, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppl4AnD_FLvj"
   },
   "source": [
    "If you are running on Google Colab (which you should), uncomment the cell below to install the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:30.973911Z",
     "start_time": "2021-03-19T16:34:30.971119Z"
    },
    "id": "DsrzADRv4Itm"
   },
   "outputs": [],
   "source": [
    "# print(\"Setting up colab environment\")\n",
    "\n",
    "# !pip uninstall -y pyarrow\n",
    "# !pip install bs4\n",
    "# !git clone https://github.com/longtermrisk/marltoolbox.git\n",
    "# !pip install -e marltoolbox/.\n",
    "# !pip uninstall -y dataclasses\n",
    "\n",
    "# # Needed for TensorBoard\n",
    "# !pip install tensorflow\n",
    "\n",
    "# # # A hack to force the runtime to restart, needed to include the above dependencies.\n",
    "# print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n",
    "# import os\n",
    "# os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jlW2Qb3BNqs"
   },
   "source": [
    "**After you run the cell above, comment all its lines.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX-suOml4Itn"
   },
   "source": [
    "## Plan\n",
    "\n",
    "1. Running experiments using the `Tune` class API  \n",
    "  a. Using the `IteratedPrisonersDilemma` environment from `marltoolbox` and components from `RLLib`   \n",
    "  b. Using `Tune`'s hyperparameter search functionnality \n",
    "\n",
    "2. Running experiments using the `RLLib` API  \n",
    "  a. Using the `IteratedPrisonersDilemma` environment and the `LTFT` algorithm from `marltoolbox`    \n",
    "  b. Using some `RLLib` functionnalities    \n",
    "  c. Use TensorBoard to visualize the trainings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVHvW6PB_EBN"
   },
   "source": [
    "##Â Requirements\n",
    "\n",
    "Be sure to have read, at least, the following quick introductions to `Tune` and `RLLib`:  \n",
    "- Quick introduction to \n",
    "[`Tune`'s key concepts](https://docs.ray.io/en/master/tune/key-concepts.html) (< 5 min).  \n",
    "- Quick introduction \n",
    "[`RLlib` in 60 seconds](https://docs.ray.io/en/master/rllib.html#rllib-in-60-seconds) (< 5 min).  \n",
    "- The README of the `Ray`/`Tune`/`RLLib` project:\n",
    "[`Ray` github](https://github.com/ray-project/ray) (<5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGZWm3Op4Ito"
   },
   "source": [
    "# 1. Running experiments using the `Tune` class API  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:33.511730Z",
     "start_time": "2021-03-19T16:34:30.977094Z"
    },
    "id": "BdtPEwKt-b47"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.pg import PGTorchPolicy, DEFAULT_CONFIG\n",
    "from ray.rllib.evaluation.sample_batch_builder import MultiAgentSampleBatchBuilder\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "from marltoolbox.envs.matrix_sequential_social_dilemma import IteratedPrisonersDilemma\n",
    "from marltoolbox.utils.miscellaneous import check_learning_achieved\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfu_VKVU4Ito"
   },
   "source": [
    " ## a. Using the `IteratedPrisonersDilemma` environment from `marltoolbox` and components from `RLLib`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3f0f36U-nwE"
   },
   "source": [
    "We use the `Tune` class API, which requires a `Trainable` class with at minimum a setup and a step method.\n",
    "We are going to implement this class step by step, adding a few elements at a time.  \n",
    "\n",
    "As explained in the `Tune` introduction (see Requirements above), the `Trainable` will be used by `Tune` to spawn experiments. `Tune` will handle the following functionalities: running experiments, running hyperparameter search, logging to TensorBoard, tracking the hyperparameters, checkpointing, etc...  \n",
    "\n",
    "We name our `Trainable` class `TrainableWIP` (WIP for Work In Progress). We will name the complete class `Trainable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:33.516593Z",
     "start_time": "2021-03-19T16:34:33.513567Z"
    },
    "id": "MHNIE5wp-nV8"
   },
   "outputs": [],
   "source": [
    "class TrainableWIP(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        # config (dict): A dict of hyperparameters\n",
    "        pass \n",
    "\n",
    "    def step(self):  # This is called iteratively to train the agents.\n",
    "        pass\n",
    "        return {\"fake_score\": self.fake_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0y9EBOZm9qz7"
   },
   "source": [
    "We use the `IteratedPrisonersDilemma` environment (IPD) from the toolbox. This is a two player game.  \n",
    "Let's look at its payoffs (rewards) given the joint actions of the players:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:33.610403Z",
     "start_time": "2021-03-19T16:34:33.518701Z"
    },
    "id": "nEC3Fbp9trEZ"
   },
   "outputs": [],
   "source": [
    "ipd_env_payoffs = IteratedPrisonersDilemma({}).PAYOUT_MATRIX\n",
    "for a_1, action_player_1 in enumerate([\"Coop\",\"Defect\"]):\n",
    "    for a_2, action_player_2 in enumerate([\"Coop\",\"Defect\"]):\n",
    "        print(f\"Payoffs for action pair ({action_player_1},{action_player_2}): \" \n",
    "              f\"({ipd_env_payoffs[a_1][a_2][0]},{ipd_env_payoffs[a_1][a_2][1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjvxM5aYu-14"
   },
   "source": [
    "Create the environment in the Trainable class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:33.692992Z",
     "start_time": "2021-03-19T16:34:33.612074Z"
    },
    "id": "6ZyWFTfy-KSd"
   },
   "outputs": [],
   "source": [
    "class TrainableWIP(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        self.env = self._init_env(config)\n",
    "\n",
    "##### NEW ######\n",
    "    def _init_env(self, config):\n",
    "        return IteratedPrisonersDilemma(config[\"env_config\"])\n",
    "#####\n",
    "\n",
    "    def step(self): \n",
    "        pass\n",
    "        return {\"fake_score\": self.fake_score}\n",
    "\n",
    "##### NEW ######\n",
    "# This dict will be sent to the setup method by Tune when we will run the training.\n",
    "tune_config = {\n",
    "    \"env_config\": {\n",
    "        \"max_steps\": 10,  # Length of an episode\n",
    "    }\n",
    "}\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsnOVlVwAPFS"
   },
   "source": [
    "We create two simple PolicyGradient(PG) players using the `PGTorchPolicy` policy class from `RLLib`.  \n",
    "And we create a `MultiAgentSampleBatchBuilder` (from `RLLib`) to aggregate our data into batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:33.771253Z",
     "start_time": "2021-03-19T16:34:33.694659Z"
    },
    "id": "DrBiGbnSBFns"
   },
   "outputs": [],
   "source": [
    "class TrainableWIP(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        self.env = self._init_env(config)\n",
    "        self.players = self._init_players(config)\n",
    "        self.multi_agent_batch_builder = self._init_batch_builder()\n",
    "        \n",
    "    def _init_env(self, config):\n",
    "        return IteratedPrisonersDilemma(config[\"env_config\"])\n",
    "\n",
    "##### NEW ######\n",
    "    def _init_players(self, config):\n",
    "        # We will use the default config provided for the PG policy by RLLib, \n",
    "        #   with a few modfications.\n",
    "        my_pg_config = DEFAULT_CONFIG\n",
    "        my_pg_config[\"gamma\"] = 0.5\n",
    "        my_pg_config[\"train_batch_size\"] = config[\"env_config\"][\"max_steps\"]\n",
    "\n",
    "        players = {}\n",
    "        for player_id in self.env.players_ids:\n",
    "            players[player_id] = PGTorchPolicy(self.env.OBSERVATION_SPACE, \n",
    "                                              self.env.ACTION_SPACE,\n",
    "                                              my_pg_config)\n",
    "        return players\n",
    "          \n",
    "    def _init_batch_builder(self):\n",
    "        return MultiAgentSampleBatchBuilder(\n",
    "            policy_map={player_id: player for player_id, player in self.players.items()},\n",
    "            clip_rewards=False,\n",
    "            callbacks=DefaultCallbacks()\n",
    "        )\n",
    "#####\n",
    "\n",
    "    def step(self):  # This is called iteratively.\n",
    "        pass\n",
    "        return {\"fake_score\": self.fake_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bec0882xClRv"
   },
   "source": [
    "We play one episode per call to `TrainableWIP.step`.  \n",
    "And then we report the total reward (of both players) averaged per environment step.  \n",
    "This information will be saved as a metric and displayed in TensorBoard as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:33.860323Z",
     "start_time": "2021-03-19T16:34:33.772886Z"
    },
    "id": "tB0ig1cuCkl7"
   },
   "outputs": [],
   "source": [
    "class TrainableWIP(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        self.env = self._init_env(config)\n",
    "        self.players = self._init_players(config)\n",
    "        self.multi_agent_batch_builder = self._init_batch_builder()\n",
    "        self.total_welfare = None\n",
    "\n",
    "    def _init_env(self, config):\n",
    "        return IteratedPrisonersDilemma(config[\"env_config\"])\n",
    "\n",
    "    def _init_players(self, config):\n",
    "        my_pg_config = DEFAULT_CONFIG\n",
    "        my_pg_config[\"gamma\"] = 0.5\n",
    "        my_pg_config[\"train_batch_size\"] = config[\"env_config\"][\"max_steps\"]\n",
    "\n",
    "        players = {}\n",
    "        for player_id in self.env.players_ids:\n",
    "            players[player_id] = PGTorchPolicy(self.env.OBSERVATION_SPACE, \n",
    "                                              self.env.ACTION_SPACE,\n",
    "                                              my_pg_config)\n",
    "        return players\n",
    "          \n",
    "    def _init_batch_builder(self):\n",
    "        return MultiAgentSampleBatchBuilder(\n",
    "            policy_map={player_id: player for player_id, player in self.players.items()},\n",
    "            clip_rewards=False,\n",
    "            callbacks=DefaultCallbacks()\n",
    "        )\n",
    "\n",
    "##### NEW ######\n",
    "    def step(self):\n",
    "        self.to_report = {}\n",
    "        self._play_one_episode() \n",
    "        self.to_report[\"mean_welfare\"] = self.total_welfare / self.config[\"env_config\"][\"max_steps\"]\n",
    "        return self.to_report\n",
    "\n",
    "    def _play_one_episode(self):\n",
    "        obs_before_act = self.env.reset()\n",
    "        done = {\"__all__\": False}\n",
    "        self.total_welfare = 0.0\n",
    "        while not done[\"__all__\"]:\n",
    "            obs_after_act, actions, rewards, done = self._play_one_step(obs_before_act)\n",
    "            self._add_step_in_batch_builder_buffer(obs_before_act, actions, rewards, done)\n",
    "            obs_before_act = obs_after_act\n",
    "          \n",
    "    def _play_one_step(self, obs_before_act):\n",
    "        actions = {player_id: player_policy.compute_actions([obs_before_act[player_id]])[0][0] \n",
    "                                  for player_id, player_policy in self.players.items()}\n",
    "\n",
    "        obs_after_act, rewards, done, info = self.env.step(actions)\n",
    "        self.to_report.update(info)\n",
    "        \n",
    "        return obs_after_act, actions, rewards, done\n",
    "\n",
    "    def _add_step_in_batch_builder_buffer(self, obs_before_act, actions, rewards, done):\n",
    "        for player_id in self.players.keys():\n",
    "            self.total_welfare += rewards[player_id]\n",
    "\n",
    "            step_player_values = {\n",
    "                \"eps_id\": self.training_iteration,\n",
    "                \"obs\": obs_before_act[player_id],\n",
    "                \"actions\": actions[player_id],\n",
    "                \"rewards\": rewards[player_id],\n",
    "                \"dones\": done[player_id],\n",
    "            }\n",
    "            # The policy_id and agent_id used in the RLLib API are the same in our case (equal to player_id)\n",
    "            self.multi_agent_batch_builder.add_values(agent_id=player_id, policy_id=player_id, **step_player_values) \n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uEOXrUJCfre"
   },
   "source": [
    "Finally, after each epsisode we train the policies of both our agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:33.964357Z",
     "start_time": "2021-03-19T16:34:33.863720Z"
    },
    "id": "s4G0pHVJCpGU"
   },
   "outputs": [],
   "source": [
    "class Trainable(tune.Trainable):\n",
    "\n",
    "    def setup(self, config):\n",
    "        self.env = self._init_env(config)\n",
    "        self.players = self._init_players(config)\n",
    "        self.multi_agent_batch_builder = self._init_batch_builder()\n",
    "        \n",
    "    def _init_env(self, config):\n",
    "        return IteratedPrisonersDilemma(config[\"env_config\"])\n",
    "\n",
    "    def _init_players(self, config):\n",
    "        my_pg_config = DEFAULT_CONFIG\n",
    "        my_pg_config[\"gamma\"] = 0.5\n",
    "        my_pg_config[\"train_batch_size\"] = config[\"env_config\"][\"max_steps\"]\n",
    "\n",
    "        players = {}\n",
    "        for player_id in self.env.players_ids:\n",
    "            players[player_id] = PGTorchPolicy(self.env.OBSERVATION_SPACE, \n",
    "                                              self.env.ACTION_SPACE,\n",
    "                                              my_pg_config)\n",
    "        return players\n",
    "          \n",
    "    def _init_batch_builder(self):\n",
    "        return MultiAgentSampleBatchBuilder(\n",
    "            policy_map={player_id: player for player_id, player in self.players.items()},\n",
    "            clip_rewards=False,\n",
    "            callbacks=DefaultCallbacks()\n",
    "        )\n",
    "\n",
    "    def step(self):\n",
    "        self.to_report = {}\n",
    "\n",
    "        self._play_one_episode()\n",
    "        self._optimize_weights()\n",
    "\n",
    "        self.to_report[\"mean_welfare\"] = self.total_welfare / self.config[\"env_config\"][\"max_steps\"]\n",
    "        self.to_report[\"training_iteration\"] = self.training_iteration # This is an attribute from tune.Trainable \n",
    "        return self.to_report \n",
    "\n",
    "    def _play_one_episode(self):\n",
    "        obs_before_act = self.env.reset()\n",
    "        done = {\"__all__\": False}\n",
    "        self.total_welfare = 0.0\n",
    "        while not done[\"__all__\"]:\n",
    "            obs_after_act, actions, rewards, done = self._play_one_step(obs_before_act)\n",
    "            self._add_step_in_batch_builder_buffer(obs_before_act, actions, rewards, done)\n",
    "            obs_before_act = obs_after_act\n",
    "      \n",
    "    def _play_one_step(self, obs_before_act):\n",
    "        actions = {player_id: player_policy.compute_actions([obs_before_act[player_id]])[0][0] \n",
    "            for player_id, player_policy in self.players.items()}\n",
    "\n",
    "        obs_after_act, rewards, done, info = self.env.step(actions)\n",
    "        self.to_report.update(info)\n",
    "\n",
    "        return obs_after_act, actions, rewards, done\n",
    "        \n",
    "    def _add_step_in_batch_builder_buffer(self, obs_before_act, actions, rewards, done):\n",
    "        for player_id in self.players.keys():\n",
    "            self.total_welfare += rewards[player_id]\n",
    "\n",
    "            step_player_values = {\n",
    "                \"eps_id\": self.training_iteration,\n",
    "                \"obs\": obs_before_act[player_id],\n",
    "                \"actions\": actions[player_id],\n",
    "                \"rewards\": rewards[player_id],\n",
    "                \"dones\": done[player_id],\n",
    "            }\n",
    "            self.multi_agent_batch_builder.add_values(agent_id=player_id, policy_id=player_id, **step_player_values) \n",
    "\n",
    "##### NEW ######\n",
    "    def _optimize_weights(self):\n",
    "        \n",
    "        multiagent_batch = self.multi_agent_batch_builder.build_and_reset()\n",
    "        for player_id, player in self.players.items():\n",
    "            multiagent_batch = self._center_reward(multiagent_batch, player_id)\n",
    "            stats = player.learn_on_batch(multiagent_batch.policy_batches[player_id])\n",
    "\n",
    "    def _center_reward(self, multiagent_batch, player_id):\n",
    "        multiagent_batch.policy_batches[player_id][\"rewards\"] = (multiagent_batch.policy_batches[player_id][\"rewards\"] - \n",
    "                                                        multiagent_batch.policy_batches[player_id][\"rewards\"].mean())\n",
    "        return multiagent_batch\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kvb080G9CM0x"
   },
   "source": [
    "We can now run this experiment with `Tune`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:45.227612Z",
     "start_time": "2021-03-19T16:34:33.969541Z"
    },
    "id": "ST5qb68DCMI7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tune_config = {\n",
    "    \"env_config\": {\n",
    "        \"max_steps\": 10, # Length of an episode\n",
    "    }\n",
    "}\n",
    "\n",
    "# stop the training after N Trainable.step (here this is equivalent to N episodes and N updates)\n",
    "stop_config = {\"training_iteration\": 200} \n",
    "\n",
    "# Restart Ray defensively in case the ray connection is lost.\n",
    "ray.shutdown() \n",
    "ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
    "# Run the experiment\n",
    "tune_analysis = tune.run(\n",
    "    Trainable,\n",
    "    stop=stop_config,\n",
    "    config=tune_config,\n",
    "    name=\"PG_IPD\",\n",
    "    )\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "check_learning_achieved(tune_results=tune_analysis, metric=\"mean_welfare\", max_=-3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcsUOT-HD2Kf"
   },
   "source": [
    "You should get a mean_welfare close to -4, which means that both players are defecting and they both get a reward of -2 per step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgH0UUQ1RAcU"
   },
   "source": [
    "##  b. Using `Tune`'s hyperparameter search functionnality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbAZZ_S_RZeF"
   },
   "source": [
    "We are going to do a simple hyperparameter grid search using `Tune`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:56.851230Z",
     "start_time": "2021-03-19T16:34:45.229917Z"
    },
    "id": "vMbfhunq_ben"
   },
   "outputs": [],
   "source": [
    "tune_config = {\n",
    "    \"env_config\": {\n",
    "        \"max_steps\": 10,\n",
    "    }\n",
    "}\n",
    "\n",
    "##### NEW ######\n",
    "# Usually hyperparameter searches are done in the tune_config dictionary \n",
    "# but here varying \"training_iteration\" is interesting.\n",
    "stop_config = {\"training_iteration\": tune.grid_search([2, 4, 8, 16, 32, 64, 128])} \n",
    "#####\n",
    "\n",
    "ray.shutdown() \n",
    "ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
    "tune_analysis = tune.run(\n",
    "    Trainable,\n",
    "    stop=stop_config,\n",
    "    config=tune_config,\n",
    "    name=\"PG_IPD\",\n",
    "    )\n",
    "ray.shutdown()\n",
    "\n",
    "check_learning_achieved(tune_results=tune_analysis, metric=\"mean_welfare\", max_=-3.5, trial_idx=6)\n",
    "check_learning_achieved(tune_results=tune_analysis, metric=\"mean_welfare\", max_=-2.5, trial_idx=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZRENNdiSLV4"
   },
   "source": [
    "As expected, the more we train, the worst we are!   \n",
    "This is expected since we are playing on the `IteratedPrisonersDilemma` environment with selfish agents.  \n",
    "Both agents are slowly learning to defect.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTPki_b7t8iC"
   },
   "source": [
    "All hyperparameter search spaces available in `Tune` are listed at [`Tune`'s search-spaces](https://docs.ray.io/en/master/tune/key-concepts.html#search-spaces)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVLRgZrAZLvf"
   },
   "source": [
    "# 2. Running experiments using the `RLLib` API  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:56.872064Z",
     "start_time": "2021-03-19T16:34:56.852894Z"
    },
    "id": "9uHr3nuI-Lap"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.utils import merge_dicts\n",
    "from ray.rllib.utils.schedules import PiecewiseSchedule\n",
    "from ray.rllib.utils.typing import TrainerConfigDict\n",
    "\n",
    "from marltoolbox.envs.matrix_sequential_social_dilemma import IteratedPrisonersDilemma\n",
    "from marltoolbox.algos import ltft\n",
    "from marltoolbox.utils import log, miscellaneous, exploration\n",
    "from marltoolbox.envs.utils.wrappers import add_RewardUncertaintyEnvClassWrapper\n",
    "from marltoolbox.utils.miscellaneous import check_learning_achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_eyBAAGaOB8"
   },
   "source": [
    "##  a. Using the `IteratedPrisonersDilemma` environment and the `LTFT` algorithm from `marltoolbox`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jefGxUbqb0TC"
   },
   "source": [
    "We are going to train two `LTFT` agents in the `IteratedPrisonersDilemma` environment (both from the toolbox). [LTFT(learning tit-for-tat) link](https://longtermrisk.org/files/toward_cooperation_learning_games_oct_2020.pdf) \n",
    "\n",
    "When using the `RLLib` API we provide an `RLLib` Trainable class and customize it extensively (instead of creating our own Trainable class from scratch like we did with the `Tune` API). \n",
    "Inside a configuration dictionary, we provide everything like the environment, the exploration policy, etc..., and we can customize the agent's policies.  \n",
    "\n",
    "Let's configure that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-g2yQI2AT7u"
   },
   "source": [
    "Configure the policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:56.943772Z",
     "start_time": "2021-03-19T16:34:56.873419Z"
    },
    "id": "mbxWsThD-vEt"
   },
   "outputs": [],
   "source": [
    "def get_rllib_config_WIP(hyperparameters: dict)-> dict:\n",
    "\n",
    "    rllib_config = {}\n",
    "    rllib_config.update(get_policies_config(hyperparameters))   \n",
    "    # ...\n",
    "    # ...\n",
    "    # ...\n",
    "    # ...\n",
    "\n",
    "\n",
    "    return rllib_config\n",
    "\n",
    "##### NEW ######\n",
    "def get_policies_config(hp):\n",
    "\n",
    "    # We will need to use the LTFTTrainer to manage the dataflow\n",
    "    # and we use the DEFAULT_CONFIG from this Trainer\n",
    "    policies_config = copy.deepcopy(ltft.DEFAULT_CONFIG)\n",
    "    policies_config.update({\n",
    "        # Inside the \"multiagent\" key of the RLLib config dict, we define all the policies that are going to be used\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "                \"player_row\": (\n",
    "                    # The default policy is LTFTTorchPolicy (as defined in our Trainable class: LTFTTrainer) \n",
    "                    None,\n",
    "                    IteratedPrisonersDilemma.OBSERVATION_SPACE,\n",
    "                    IteratedPrisonersDilemma.ACTION_SPACE,\n",
    "                    # We can provide an additionnal configuration dict to this policy. \n",
    "                    #   It will be merged with a copy of the rllib_config that we are currenlty creating.\n",
    "                    {}),\n",
    "                \"player_col\": (\n",
    "                    None,\n",
    "                    IteratedPrisonersDilemma.OBSERVATION_SPACE,\n",
    "                    IteratedPrisonersDilemma.ACTION_SPACE,\n",
    "                    {}),\n",
    "            },\n",
    "            # We need to define how the agent_id used in the environment (dict keys) will be associated \n",
    "            #  to the policy_id of the policies above (dict keys). Here they are simply identical.\n",
    "            \"policy_mapping_fn\": lambda agent_id: agent_id,\n",
    "            \"replay_mode\": \"lockstep\",\n",
    "        },\n",
    "\n",
    "        # We add some callbacks needed by the LTFT policy and ask for additionnal logs.\n",
    "        \"callbacks\": miscellaneous.merge_callbacks(ltft.LTFTCallbacks,\n",
    "                                                   log.get_logging_callbacks_class(\n",
    "                                                       log_env_step=True,\n",
    "                                                       log_from_policy=True)),\n",
    "    })\n",
    "    return policies_config\n",
    "\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-zRmL2YAPGI"
   },
   "source": [
    "Configure the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:57.037223Z",
     "start_time": "2021-03-19T16:34:56.945964Z"
    },
    "id": "4-VPfgGP9-XH"
   },
   "outputs": [],
   "source": [
    "def get_rllib_config_WIP(hyperparameters: dict)-> dict:\n",
    "\n",
    "    rllib_config = {}\n",
    "    rllib_config.update(get_policies_config(hyperparameters))   \n",
    "##### NEW ######\n",
    "    rllib_config.update(get_env_config(hyperparameters))\n",
    "#####\n",
    "    # ...\n",
    "    # ...\n",
    "    # ...\n",
    "    \n",
    "    return rllib_config\n",
    "\n",
    "##### NEW ######\n",
    "def get_env_config(hp):\n",
    "    env_config = {\n",
    "        # We provide the environment class\n",
    "        \"env\": get_env_class(),\n",
    "        # And the dictionnary that will be sent to initialize the environment\n",
    "        \"env_config\": {\n",
    "            \"players_ids\": [\"player_row\", \"player_col\"],\n",
    "            \"max_steps\": hp[\"n_steps_per_epi\"],  # Length of an episode\n",
    "        },\n",
    "    }\n",
    "    return env_config\n",
    "\n",
    "def get_env_class():\n",
    "    # We add a wrapper around the environment to add some variance to the rewards returned\n",
    "    MyUncertainIPD = add_RewardUncertaintyEnvClassWrapper(\n",
    "        IteratedPrisonersDilemma,\n",
    "        reward_uncertainty_std=0.1)\n",
    "    return MyUncertainIPD\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zSAw-PjAXMu"
   },
   "source": [
    "Configure the default `DQN` policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:57.126800Z",
     "start_time": "2021-03-19T16:34:57.039972Z"
    },
    "id": "gej3suhHAo0S"
   },
   "outputs": [],
   "source": [
    "def get_rllib_config_WIP(hyperparameters: dict)-> dict:\n",
    "\n",
    "    rllib_config = {}\n",
    "    rllib_config.update(get_policies_config(hyperparameters))   \n",
    "    rllib_config.update(get_env_config(hyperparameters))\n",
    "##### NEW ######\n",
    "    rllib_config.update(get_default_DQN_config(hyperparameters))\n",
    "#####\n",
    "    # ...\n",
    "    # ...\n",
    "    \n",
    "    return rllib_config\n",
    "\n",
    "##### NEW ######\n",
    "def get_default_DQN_config(hp):\n",
    "    # The LTFT policy uses three DQN policies and a supervised learning policy.\n",
    "    # We provide here some additional configuration for the DQN policies \n",
    "    # (this additional configuration will also be sent to the supervised learning policy \n",
    "    # which will ignore it)\n",
    "\n",
    "    default_DQN_config = {\n",
    "        # === DQN Models ===\n",
    "        # Minimum env steps to optimize for per train call. This value does\n",
    "        # not affect learning, only the length of iterations.\n",
    "        \"timesteps_per_iteration\": hp[\"n_steps_per_epi\"],\n",
    "        \"min_iter_time_s\": 2.0,\n",
    "        # Update the target network every `target_network_update_freq` steps.\n",
    "        \"target_network_update_freq\": 3 * hp[\"n_steps_per_epi\"],\n",
    "        # === Replay buffer ===\n",
    "        # Size of the replay buffer. Note that if async_updates is set, then\n",
    "        # each worker will have a replay buffer of this size.\n",
    "        \"buffer_size\": int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"]),\n",
    "        # Whether to use dueling dqn\n",
    "        \"dueling\": False,\n",
    "        # Dense-layer setup for each the advantage branch and the value branch\n",
    "        # in a dueling architecture.\n",
    "        \"hiddens\": [32],\n",
    "        # Whether to use double dqn\n",
    "        \"double_q\": False,\n",
    "        # If True prioritized replay buffer will be used.\n",
    "        \"prioritized_replay\": False,\n",
    "        \"model\": {\n",
    "            # Number of hidden layers for fully connected net\n",
    "            \"fcnet_hiddens\": [32, 2],\n",
    "            # Nonlinearity for fully connected net (tanh, relu)\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        },\n",
    "    }\n",
    "    return default_DQN_config\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UvhCvbtAuPO"
   },
   "source": [
    "Configure the exploration policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:57.215466Z",
     "start_time": "2021-03-19T16:34:57.128267Z"
    },
    "id": "aJIHVPLjbfiC"
   },
   "outputs": [],
   "source": [
    "def get_rllib_config_WIP(hyperparameters: dict)-> dict:\n",
    "\n",
    "    rllib_config = {}\n",
    "    rllib_config.update(get_policies_config(hyperparameters))   \n",
    "    rllib_config.update(get_env_config(hyperparameters))\n",
    "    rllib_config.update(get_default_DQN_config(hyperparameters))\n",
    "##### NEW ######\n",
    "    rllib_config.update(get_exploration_config(hyperparameters))\n",
    "#####\n",
    "    # ...\n",
    "    \n",
    "    return rllib_config\n",
    "\n",
    "##### NEW ######\n",
    "def get_exploration_config(hp):\n",
    "    exploration_config = {\n",
    "        # === Exploration Settings ===\n",
    "        # Set to False for no exploration behavior (e.g., for evaluation).\n",
    "        \"explore\": True,\n",
    "        # Provide a dict specifying the Exploration object's config.\n",
    "        \"exploration_config\": {\n",
    "            # The Exploration class to use. In the simplest case, this is the name\n",
    "            # (str) of any class present in the `rllib.utils.exploration` package.\n",
    "            # You can also provide the python class directly or the full location\n",
    "            # of your class (e.g. \"ray.rllib.utils.exploration.epsilon_greedy.\n",
    "            # EpsilonGreedy\").\n",
    "            \"type\": exploration.SoftQScheduleWtClustering,\n",
    "            # Add constructor kwargs here (if any).\n",
    "            \"clustering_distance\": 0.2,\n",
    "            \"temperature_schedule\": PiecewiseSchedule(\n",
    "                endpoints=[\n",
    "                    (0, 1.0), (int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"] * 0.75), 0.1)],\n",
    "                outside_value=0.1,\n",
    "                framework=\"torch\")\n",
    "        },\n",
    "\n",
    "    }\n",
    "\n",
    "    return exploration_config\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLHzzUrXfCoP"
   },
   "source": [
    "Configure the optimization and others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:34:57.358296Z",
     "start_time": "2021-03-19T16:34:57.220349Z"
    },
    "id": "qqlYU7ZiALxl"
   },
   "outputs": [],
   "source": [
    "def get_rllib_config(hyperparameters: dict)-> dict:\n",
    "\n",
    "    rllib_config = {}\n",
    "    rllib_config.update(get_policies_config(hyperparameters))   \n",
    "    rllib_config.update(get_env_config(hyperparameters))\n",
    "    rllib_config.update(get_default_DQN_config(hyperparameters))\n",
    "    rllib_config.update(get_exploration_config(hyperparameters))\n",
    "    rllib_config.update(get_optimization_and_general_config(hyperparameters))\n",
    "\n",
    "    return rllib_config\n",
    "\n",
    "##### NEW ######\n",
    "def get_optimization_and_general_config(hp: dict):\n",
    "\n",
    "    optim_and_general_config = {\n",
    "        \n",
    "        # === Optimization ===\n",
    "        # Learning rate\n",
    "        \"lr\": hp[\"base_lr\"],\n",
    "        # Learning rate schedule\n",
    "        \"lr_schedule\": [(0, hp[\"base_lr\"]),\n",
    "                        (int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"]), hp[\"base_lr\"] / 1e9)],\n",
    "        # How many steps of the model to sample before learning starts.\n",
    "        \"learning_starts\": int(hp[\"n_steps_per_epi\"] * hp[\"bs_epi_mul\"]),\n",
    "        # Update the replay buffer with this many samples at once. Note that\n",
    "        # this setting applies per-worker if num_workers > 1.\n",
    "        \"rollout_fragment_length\": hp[\"n_steps_per_epi\"],\n",
    "        # Size of a batch sampled from replay buffer for training. Note that\n",
    "        # if async_updates is set, then each worker returns gradients for a\n",
    "        # batch of this size.\n",
    "        \"train_batch_size\": int(hp[\"n_steps_per_epi\"] * hp[\"bs_epi_mul\"]),\n",
    "        \"gamma\": 0.5,\n",
    "\n",
    "        # === General config ===\n",
    "        \"framework\": \"torch\",\n",
    "        \"batch_mode\": \"complete_episodes\",\n",
    "        # LTFT supports only 1 worker only otherwise it would be mixing several opponents trajectories\n",
    "        \"num_workers\": 0,\n",
    "        # LTFT supports only 1 env per worker only otherwise several episodes would be played at the same time\n",
    "        \"num_envs_per_worker\": 1,\n",
    "        \"seed\": tune.grid_search(hp[\"seeds\"]),\n",
    "\n",
    "    }\n",
    "\n",
    "    return optim_and_general_config\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQC1SE5tfasX"
   },
   "source": [
    "Start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:37:33.838518Z",
     "start_time": "2021-03-19T16:34:57.363147Z"
    },
    "id": "bfEaO43v3UBB"
   },
   "outputs": [],
   "source": [
    "def get_stop_config(hp):\n",
    "    stop_config = {\n",
    "        \"episodes_total\": hp[\"n_epi\"], \n",
    "    }\n",
    "    return stop_config\n",
    "\n",
    "ltft_hparameters = {\n",
    "    \"n_epi\": 400,\n",
    "    \"n_steps_per_epi\": 20,\n",
    "    \"bs_epi_mul\": 4,\n",
    "    \"base_lr\": 0.04,\n",
    "    \"spl_lr_mul\": 10.0,\n",
    "    \"seeds\": miscellaneous.get_random_seeds(1),\n",
    "    \"debug\": False,\n",
    "}\n",
    "\n",
    "\n",
    "rllib_config = get_rllib_config(ltft_hparameters)\n",
    "stop_config = get_stop_config(ltft_hparameters)\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
    "tune_analysis_self_play = ray.tune.run(ltft.LTFTTrainer, config=rllib_config,\n",
    "                        checkpoint_freq=0, stop=stop_config, \n",
    "                        checkpoint_at_end=False, name=\"LTFT_exp\")\n",
    "ray.shutdown()\n",
    "\n",
    "check_learning_achieved(tune_results=tune_analysis_self_play, \n",
    "                        min_=-42, trial_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adTH0zX-QtBY"
   },
   "source": [
    "`LTFT` agents should learn to cooperate and we should reach a \"reward\" close to -40. This is the sum for both players' rewards during an entire episode.  \n",
    "Averaged by 20 steps, this gives use -2 per step which is the best possible welfare in the `IteratedPrisonersDilemma` environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1NySahgaPQx"
   },
   "source": [
    "##  b. Using some `RLLib` functionnalities  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w57wPt7Ydu4n"
   },
   "source": [
    "We can easily change the model and dataflow used by `RLLib` policies by changing the configuration dict. Let try to reduce the training time.  \n",
    "\n",
    "We are going to make the following changes:  \n",
    "- using a smaller network\n",
    "- using Dueling Double DQN (D3QN) instead of DQN.\n",
    "- training for less episodes\n",
    "- training with less steps per episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:37:33.844194Z",
     "start_time": "2021-03-19T16:37:33.840095Z"
    },
    "id": "yCZzbx3Ld0vz"
   },
   "outputs": [],
   "source": [
    "def get_default_DQN_config(hp):\n",
    "    default_DQN_config = {\n",
    "        \"timesteps_per_iteration\": hp[\"n_steps_per_epi\"],\n",
    "        \"target_network_update_freq\": hp[\"n_steps_per_epi\"],\n",
    "        \"prioritized_replay\": False,\n",
    "\n",
    "        ##### MODIFIED ######\n",
    "        \"buffer_size\": int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"]),\n",
    "        \"dueling\": True, # instead of False\n",
    "        \"hiddens\": [4], # instead of 32\n",
    "        \"double_q\": True, # instead of False\n",
    "        \"model\": {\n",
    "            \"fcnet_hiddens\": [4, 2], # instead of [32, 2]\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        },\n",
    "        #####\n",
    "    }\n",
    "    return default_DQN_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:38:32.233178Z",
     "start_time": "2021-03-19T16:37:33.846591Z"
    },
    "id": "zETjK-Y5ZKLA"
   },
   "outputs": [],
   "source": [
    "def get_stop_config(hp):\n",
    "    stop_config = {\n",
    "        \"episodes_total\": hp[\"n_epi\"],\n",
    "    }\n",
    "    return stop_config\n",
    "\n",
    "ltft_hparameters = {\n",
    "    ##### MODIFIED ######\n",
    "    \"n_epi\": 200,  # instead of 400\n",
    "    \"n_steps_per_epi\": 10, # instead of 20\n",
    "    #####\n",
    "    \"bs_epi_mul\": 4,\n",
    "    \"base_lr\": 0.04,\n",
    "    \"spl_lr_mul\": 10.0,\n",
    "    \"seeds\": miscellaneous.get_random_seeds(1),\n",
    "    \"debug\": False,\n",
    "}\n",
    "\n",
    "\n",
    "rllib_config = get_rllib_config(ltft_hparameters)\n",
    "stop_config = get_stop_config(ltft_hparameters)\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
    "tune_analysis_self_play = ray.tune.run(ltft.LTFTTrainer, config=rllib_config,\n",
    "                        checkpoint_freq=0, stop=stop_config, \n",
    "                        checkpoint_at_end=False, name=\"LTFT_exp\")\n",
    "ray.shutdown()\n",
    "\n",
    "check_learning_achieved(tune_results=tune_analysis_self_play,\n",
    "                        min_=-22, trial_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Kn_Ev7fcPNk"
   },
   "source": [
    "Our training time is now significantly shorter than previously!   \n",
    "\n",
    "And we still achieve cooperation since the welfare (total reward) per step is still -2.\n",
    "\n",
    "If you want, you can try to determine which modification was the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2Cg-m_n4Itx"
   },
   "source": [
    "##  c. Use TensorBoard to visualize the trainings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BisqDo64Itx"
   },
   "source": [
    "You can uncomment and use TensorBoard to view trial performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:38:32.237350Z",
     "start_time": "2021-03-19T16:38:32.234952Z"
    },
    "id": "u00VE8oB4Itx"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:38:32.343372Z",
     "start_time": "2021-03-19T16:38:32.238939Z"
    },
    "id": "NpeJySSk4Itx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir /root/ray_results/ # On Google Colab\n",
    "# %tensorboard --logdir ~/ray_results/ # On your machine\n",
    "\n",
    "# You can filter the graphs with \"reward|mean_welfare|defection_metric|entropy|CC\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial - Basics - How to use the toolbox.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
