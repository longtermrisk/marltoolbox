{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Tutorial - L1BR and same and cross-play performance evaluation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKYet7sZ4Itg"
      },
      "source": [
        "# Tutorial - L1BR and same and cross-play performance evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJnqi3Tt5PTN"
      },
      "source": [
        "## Install the toolbox (and Ray, Tune, RLLib, PyTorch, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsrzADRv4Itm"
      },
      "source": [
        "!pip uninstall -y pyarrow\n",
        "!pip install bs4\n",
        "## If you are NOT running on Google Colab (which you should), comment below to remove installing the necessary dependencies \n",
        "print(\"Setting up colab environment\")\n",
        "!git clone https://github.com/longtermrisk/marltoolbox.git\n",
        "\n",
        "## Here are different installation instructions to support different algorithms\n",
        "### Default install\n",
        "!pip install -e marltoolbox/.\n",
        "\n",
        "# Needed for TensorBoard\n",
        "!pip install tensorflow\n",
        "\n",
        "# # A hack to force the runtime to restart, needed to include the above dependencies.\n",
        "print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n",
        "import os\n",
        "os._exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jlW2Qb3BNqs"
      },
      "source": [
        "After you run this cell, comment all its lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX-suOml4Itn"
      },
      "source": [
        "## Plan  \n",
        "\n",
        "3. Using the experimentation features in the toolbox  \n",
        "  a. Evaluate the same and cross-play performances of MA algorithms  \n",
        "  b. Evaluate the exploitability of an algorithm using Level 1 Best-Response (L1BR)  \n",
        "  \n",
        "(Section 1 and 2 are in the tutorial: Basics - How to use the toolbox)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVHvW6PB_EBN"
      },
      "source": [
        "## Requirements\n",
        "\n",
        "You have done the first tutorial (Basics - How to use the toolbox)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGZWm3Op4Ito"
      },
      "source": [
        " # 3. Evaluating the same and cross-play performances of MA algorithms "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfu_VKVU4Ito"
      },
      "source": [
        " ## a. Evaluate the same and cross-play performances of MA algorithms  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdtPEwKt-b47"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.evaluation.sample_batch_builder import MultiAgentSampleBatchBuilder\n",
        "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
        "from ray.rllib.agents.ppo import PPOTrainer, PPOTorchPolicy\n",
        "from ray.rllib.agents.ppo.ppo_torch_policy import setup_mixins\n",
        "\n",
        "from marltoolbox.utils import log, miscellaneous\n",
        "from marltoolbox.envs.matrix_sequential_social_dilemma import IteratedBoS\n",
        "from marltoolbox.utils import same_and_cross_perf, restore\n",
        "from marltoolbox.utils.plot import PlotConfig\n",
        "from marltoolbox.envs.utils.wrappers import add_RewardUncertaintyEnvClassWrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3f0f36U-nwE"
      },
      "source": [
        "We need to train some agents with different seeds to then compute their same and cross-play performances after deployment.\n",
        "\n",
        "We are going to train PPO agents on the BachOrStravinsky(BoS) game using the RLLib API.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fZtrKHovNvV"
      },
      "source": [
        "bos_env_payoffs = IteratedBoS({}).PAYOUT_MATRIX\n",
        "for a_1, action_player_1 in enumerate(['Bach','Stravinsky']):\n",
        "    for a_2, action_player_2 in enumerate(['Bach','Stravinsky']):\n",
        "        print(f\"payoffs for action pair ({action_player_1},{action_player_2}): \" \n",
        "              f\"({bos_env_payoffs[a_1][a_2][0]},{bos_env_payoffs[a_1][a_2][1]})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akV87_3evRdb"
      },
      "source": [
        " Here is the configuration for such training, I will not detail it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHNIE5wp-nV8"
      },
      "source": [
        "def get_trainer_config(hp):\n",
        "    train_n_replicates = hp[\"train_n_replicates\"]\n",
        "    seeds = miscellaneous.get_random_seeds(train_n_replicates)\n",
        "    exp_name, _ = log.log_in_current_day_dir(\"PPO_BoS\")\n",
        "\n",
        "    # This modification to the policy will allow us to load each policies from different checkpoints \n",
        "    # This will be used during evaluation.\n",
        "    def merged_after_init(*args, **kwargs):\n",
        "      setup_mixins(*args, **kwargs)\n",
        "      restore.after_init_load_policy_checkpoint(*args, **kwargs)\n",
        "    MyPPOPolicy = PPOTorchPolicy.with_updates(after_init=merged_after_init)\n",
        "\n",
        "    stop_config = {\n",
        "        \"episodes_total\": hp[\"episodes_total\"],\n",
        "    }\n",
        "\n",
        "    env_config = {\n",
        "        \"players_ids\": [\"player_row\", \"player_col\"],\n",
        "        \"max_steps\": hp[\"steps_per_epi\"],\n",
        "        \"get_additional_info\": True,\n",
        "    }\n",
        "\n",
        "    trainer_config = {\n",
        "        \"env\": add_RewardUncertaintyEnvClassWrapper(\n",
        "                  IteratedBoS,\n",
        "                  reward_uncertainty_std=0.1),\n",
        "        \"env_config\": env_config,\n",
        "\n",
        "        \"multiagent\": {\n",
        "            \"policies\": {\n",
        "                env_config[\"players_ids\"][0]: (MyPPOPolicy,\n",
        "                                               IteratedBoS.OBSERVATION_SPACE,\n",
        "                                               IteratedBoS.ACTION_SPACE,\n",
        "                                               {}),\n",
        "                env_config[\"players_ids\"][1]: (MyPPOPolicy,\n",
        "                                               IteratedBoS.OBSERVATION_SPACE,\n",
        "                                               IteratedBoS.ACTION_SPACE,\n",
        "                                               {}),\n",
        "            },\n",
        "            \"policy_mapping_fn\": lambda agent_id: agent_id,\n",
        "        },\n",
        "\n",
        "        #### PPO config ####\n",
        "        # Size of batches collected from each worker.\n",
        "        \"rollout_fragment_length\": hp[\"steps_per_epi\"], \n",
        "        # Number of timesteps collected for each SGD round. This defines the size\n",
        "        # of each SGD epoch.\n",
        "        \"train_batch_size\": hp[\"steps_per_epi\"]*3, \n",
        "        # Total SGD batch size across all devices for SGD. This defines the\n",
        "        # minibatch size within each epoch.\n",
        "        \"sgd_minibatch_size\": hp[\"steps_per_epi\"],\n",
        "        # Number of SGD iterations in each outer loop (i.e., number of epochs to\n",
        "        # execute per train batch).\n",
        "        \"num_sgd_iter\": 3,\n",
        "        \"model\": {\n",
        "            # Number of hidden layers for fully connected net\n",
        "            \"fcnet_hiddens\": [4, 2],\n",
        "            # Nonlinearity for fully connected net (tanh, relu)\n",
        "            \"fcnet_activation\": \"relu\",\n",
        "        },\n",
        "\n",
        "\n",
        "        \"lr\": hp[\"base_lr\"],\n",
        "        \"lr_schedule\": [(0, hp[\"base_lr\"]),\n",
        "                (int(hp[\"steps_per_epi\"] * hp[\"episodes_total\"]), hp[\"base_lr\"] / 1e9)],\n",
        "    \n",
        "        \"seed\": tune.grid_search(seeds),\n",
        "        \"callbacks\": log.get_logging_callbacks_class(),\n",
        "        \"framework\": \"torch\",\n",
        "        \"num_workers\":0,\n",
        "    }\n",
        "\n",
        "    return trainer_config, env_config, stop_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmeYqMa00Gns"
      },
      "source": [
        "Let's train 16 PPO agents (8 seeds x 2 players): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XijF3Q7O0FxF"
      },
      "source": [
        "hyperparameters = {\n",
        "    \"steps_per_epi\": 20,\n",
        "    \"train_n_replicates\": 8,\n",
        "    \"episodes_total\": 200,\n",
        "    \"exp_name\": \"PPO_BoS\",\n",
        "    \"base_lr\": 5e-1,\n",
        "}\n",
        "\n",
        "trainer_config, _, stop_config = get_trainer_config(hyperparameters)\n",
        "ray.shutdown()\n",
        "ray.init(num_cpus=os.cpu_count(), num_gpus=0, local_mode=False)\n",
        "tune_analysis = tune.run(PPOTrainer, config=trainer_config, stop=stop_config,\n",
        "                    checkpoint_freq=0, checkpoint_at_end=True, name=hyperparameters[\"exp_name\"],\n",
        "                    metric=\"episode_reward_mean\", mode=\"max\")\n",
        "ray.shutdown()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6reFekyz6En"
      },
      "source": [
        "We now have 16 PPO agents trained with 8 differents random seeds, which perform well on BoS.  \n",
        "We will be able to load these agents using the checkpoints created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4qziRvL0o_E"
      },
      "source": [
        "print(\"location of the best checkpoint\",tune_analysis.best_checkpoint)\n",
        "tune_analysis_per_exp = {\"\": tune_analysis}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y9EBOZm9qz7"
      },
      "source": [
        "We will use the SameAndCrossPlayEvaluator (from our toolbox) to evaluate the same and cross-play performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZyWFTfy-KSd"
      },
      "source": [
        "def evaluate_same_and_cross_perf(tune_analysis_per_exp, hp):\n",
        "    config_eval, env_config, stop_config, hp_eval = generate_eval_config(hp)\n",
        "\n",
        "    evaluator = same_and_cross_perf.SameAndCrossPlayEvaluator(exp_name=hp_eval[\"exp_name\"])\n",
        "    analysis_metrics_per_mode = evaluator.perform_evaluation_or_load_data(\n",
        "        evaluation_config=config_eval, \n",
        "        stop_config=stop_config,\n",
        "        policies_to_load_from_checkpoint=copy.deepcopy(env_config[\"players_ids\"]),\n",
        "        tune_analysis_per_exp=tune_analysis_per_exp,\n",
        "        TrainerClass=PPOTrainer,\n",
        "        n_cross_play_per_checkpoint=2,\n",
        "        to_load_path=None)\n",
        "\n",
        "    plot_config = PlotConfig(xlim=hp_eval[\"x_limits\"], ylim=hp_eval[\"y_limits\"],\n",
        "                             markersize=5, alpha=1.0, jitter=hp_eval[\"jitter\"],\n",
        "                             xlabel=\"player 1 payoffs\", ylabel=\"player 2 payoffs\",\n",
        "                             title=\"cross and same-play performances: BoS\",\n",
        "                             x_scale_multiplier=hp_eval[\"scale_multipliers\"][0],\n",
        "                             y_scale_multiplier=hp_eval[\"scale_multipliers\"][1])\n",
        "    evaluator.plot_results(analysis_metrics_per_mode, plot_config=plot_config,\n",
        "                           x_axis_metric=f\"policy_reward_mean/{env_config['players_ids'][0]}\",\n",
        "                           y_axis_metric=f\"policy_reward_mean/{env_config['players_ids'][1]}\")\n",
        "\n",
        "def generate_eval_config(hp):\n",
        "    \n",
        "    hp_eval = copy.deepcopy(hp)\n",
        "    hp_eval[\"steps_per_epi\"]= 20\n",
        "    hp_eval[\"episodes_total\"]= 1\n",
        "    hp_eval[\"scale_multipliers\"] = (1/hp_eval[\"steps_per_epi\"], 1/hp_eval[\"steps_per_epi\"])\n",
        "    hp_eval[\"base_lr\"]= 0.0\n",
        "    hp_eval[\"jitter\"]= 0.05\n",
        "    hp_eval[\"x_limits\"]= (-0.5,3.5)\n",
        "    hp_eval[\"y_limits\"]= (-0.5,3.5)\n",
        "\n",
        "    trainer_config, env_config, stop_config = get_trainer_config(hp_eval)\n",
        "\n",
        "    trainer_config[\"explore\"] = False\n",
        "    trainer_config[\"seed\"] = 1111\n",
        "    trainer_config[\"train_batch_size\"] = hp_eval[\"steps_per_epi\"]\n",
        "\n",
        "    return trainer_config, env_config, stop_config, hp_eval\n",
        "\n",
        "ray.shutdown()\n",
        "evaluate_same_and_cross_perf(tune_analysis_per_exp, hyperparameters)\n",
        "ray.shutdown()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsnOVlVwAPFS"
      },
      "source": [
        "We can see the cross-play and same-play performances in the plot. We should see some failures in cross-play (close to (0,0)). If there is no failure, you can restart the notebook and run again all the cells above to produce new evaluations with new agents. Theses failures are explained by the fact that the PPO agents only learned to coordinate on playing either Bach or Stravinsky. They have not learned to adapt to a change of behavior in the other player."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgH0UUQ1RAcU"
      },
      "source": [
        "##  b. Evaluate the exploitability of an algorithm using Level 1 Best-Response (L1BR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmvRIbPqUAwy"
      },
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.agents.dqn import DQNTrainer\n",
        "from ray.rllib.agents.dqn.dqn_torch_policy import DQNTorchPolicy, build_q_stats\n",
        "from ray.rllib.policy.policy import Policy\n",
        "from ray.rllib.utils import merge_dicts\n",
        "from ray.rllib.utils.schedules import PiecewiseSchedule\n",
        "from ray.rllib.utils.typing import TrainerConfigDict\n",
        "\n",
        "import torch\n",
        "\n",
        "from marltoolbox.envs.matrix_sequential_social_dilemma import IteratedPrisonersDilemma\n",
        "from marltoolbox.algos.learning_tit_for_tat.ltft import LTFT_DEFAULT_CONFIG_UPDATE, LTFT, LTFTCallbacks\n",
        "from marltoolbox.algos.supervised_learning import SPLTorchPolicy\n",
        "from marltoolbox.utils import log, miscellaneous, exploration, lvl1_best_response\n",
        "from marltoolbox.envs.utils.wrappers import add_RewardUncertaintyEnvClassWrapper\n",
        "from marltoolbox.algos import population"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbAZZ_S_RZeF"
      },
      "source": [
        "We are going to see if `LTFT` is exploitable after deployement in IPD. We will train two populations of agents. First the Level 0 agents that will be `LTFT` agents trained in self-play. Then we will freeze their weights like if they were deployed in production. We will then train level 1 PG agents against this population of level 0 agents.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xyAY7QxTvwY"
      },
      "source": [
        "To train the Lvl0 `LTFT` agents, we will use the code from the first tutorial, section 2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYq4qi2dF5aI"
      },
      "source": [
        "def get_env_config(hp):\n",
        "    env_config = {\n",
        "        # We provide the environment class\n",
        "        \"env\": get_env_class(),\n",
        "        \"env_config\": {\n",
        "            \"players_ids\": [\"player_row\", \"player_col\"],\n",
        "            \"max_steps\": hp[\"n_steps_per_epi\"],\n",
        "        },\n",
        "    }\n",
        "    return env_config\n",
        "\n",
        "def get_env_class():\n",
        "    # We add a wrapper around the environment to add some randomness to the rewards returned at each step\n",
        "    MyUncertainIPD = add_RewardUncertaintyEnvClassWrapper(\n",
        "        IteratedPrisonersDilemma,\n",
        "        reward_uncertainty_std=0.1)\n",
        "    return MyUncertainIPD\n",
        "\n",
        "def get_policies_config(hp):\n",
        "\n",
        "    # We will need to use the DQNTrainer (from RLLib) to manage the dataflow and \n",
        "    #   to provide the required default config for the base policy we are going to use: DQNTorchPolicy\n",
        "\n",
        "    policies_config = {\n",
        "        # Inside the \"multiagent\" key of the RLLib config dict, we define all the policies that are going to be used\n",
        "        \"multiagent\": {\n",
        "            \"policies\": {\n",
        "                \"player_row\": (\n",
        "                    # The default policy is DQNTorchPolicy (as defined in our Trainable class: DQNTrainer) \n",
        "                    #   but we overwrite it to use the LTFT policy from the toolbox\n",
        "                    LTFT,\n",
        "                    IteratedPrisonersDilemma.OBSERVATION_SPACE,\n",
        "                    IteratedPrisonersDilemma.ACTION_SPACE,\n",
        "                    # We provide an additionnal configuration dict to this policy. \n",
        "                    #   It will be merged with a copy of the rllib_config that we will provide to the RLLib Trainable (DQNTrainer).\n",
        "                    get_ltft_policy_config(hp)),\n",
        "                \"player_col\": (\n",
        "                    # To use the the default policy (DQNTorchPolicy), we would have set the first arg of this tuple to None\n",
        "                    LTFT,\n",
        "                    IteratedPrisonersDilemma.OBSERVATION_SPACE,\n",
        "                    IteratedPrisonersDilemma.ACTION_SPACE,\n",
        "                    get_ltft_policy_config(hp)),\n",
        "            },\n",
        "            # We need to define how the agent_id (dict keys) used in the environment will be associated \n",
        "            #  to the policy_id (dict keys) of the policies above. Here they are identical.\n",
        "            \"policy_mapping_fn\": lambda agent_id: agent_id,\n",
        "        },\n",
        "\n",
        "        # Callbacks that will be run during various phases of training. \n",
        "        # We add some callbacks needed by the LTFT policy and ask for additionnal logs.\n",
        "        \"callbacks\": miscellaneous.merge_callbacks(LTFTCallbacks,\n",
        "                                                   log.get_logging_callbacks_class(\n",
        "                                                       log_env_step=True,\n",
        "                                                       log_from_policy=True)),\n",
        "    }\n",
        "    return policies_config\n",
        "\n",
        "\n",
        "def sgd_optimizer_dqn(policy: Policy, config: TrainerConfigDict) -> \"torch.optim.Optimizer\":\n",
        "    return torch.optim.SGD(policy.q_func_vars, lr=policy.cur_lr, momentum=config[\"sgd_momentum\"])\n",
        "\n",
        "\n",
        "def sgd_optimizer_spl(policy: Policy, config: TrainerConfigDict) -> \"torch.optim.Optimizer\":\n",
        "    return torch.optim.SGD(policy.model.parameters(), lr=policy.cur_lr, momentum=config[\"sgd_momentum\"])\n",
        "\n",
        "\n",
        "def get_rllib_config(hyperparameters: dict)-> dict:\n",
        "\n",
        "    rllib_config = {}\n",
        "    rllib_config.update(get_env_config(hyperparameters))\n",
        "    rllib_config.update(get_policies_config(hyperparameters))   \n",
        "    rllib_config.update(get_default_DQN_config(hyperparameters))\n",
        "    rllib_config.update(get_exploration_config(hyperparameters))\n",
        "    rllib_config.update(get_optimization_and_general_config(hyperparameters))\n",
        "\n",
        "    return rllib_config\n",
        "\n",
        "def get_ltft_policy_config(hp):\n",
        "\n",
        "    MyDQNTorchPolicy = DQNTorchPolicy.with_updates(optimizer_fn=sgd_optimizer_dqn)\n",
        "\n",
        "    ltft_config = merge_dicts(\n",
        "      LTFT_DEFAULT_CONFIG_UPDATE,\n",
        "      {\n",
        "        \"sgd_momentum\": 0.9,\n",
        "        'nested_policies': [\n",
        "            {\"Policy_class\": MyDQNTorchPolicy, \"config_update\": {}},\n",
        "            {\"Policy_class\": MyDQNTorchPolicy, \"config_update\": {}},\n",
        "            {\"Policy_class\": MyDQNTorchPolicy, \"config_update\": {}},\n",
        "            {\"Policy_class\": SPLTorchPolicy.with_updates(optimizer_fn=sgd_optimizer_spl), \n",
        "             \"config_update\": {\n",
        "                \"learn_action\": True,\n",
        "                \"learn_reward\": False,\n",
        "                \"sgd_momentum\": 0.75,\n",
        "                \"explore\": False,\n",
        "                \"timesteps_per_iteration\": hp[\"n_steps_per_epi\"],\n",
        "                # === Optimization ===\n",
        "                \"lr\": hp[\"base_lr\"] * hp[\"spl_lr_mul\"],\n",
        "                \"lr_schedule\": [(0, hp[\"base_lr\"] * hp[\"spl_lr_mul\"]),\n",
        "                                (int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"]), hp[\"base_lr\"] / 1e9)],\n",
        "                \"loss_fn\": torch.nn.CrossEntropyLoss(\n",
        "                    weight=None,\n",
        "                    size_average=None,\n",
        "                    ignore_index=-100,\n",
        "                    reduce=None,\n",
        "                    reduction='mean')\n",
        "              }\n",
        "             },\n",
        "          ],\n",
        "      }\n",
        "    )\n",
        "    return ltft_config\n",
        "\n",
        "def get_default_DQN_config(hp):\n",
        "    default_DQN_config = {\n",
        "        # === DQN Models ===\n",
        "        # Minimum env steps to optimize for per train call. This value does\n",
        "        # not affect learning, only the length of iterations.\n",
        "        \"timesteps_per_iteration\": hp[\"n_steps_per_epi\"],\n",
        "        # Update the target network every `target_network_update_freq` steps.\n",
        "        \"target_network_update_freq\": hp[\"n_steps_per_epi\"],\n",
        "        # === Replay buffer ===\n",
        "        # Size of the replay buffer. Note that if async_updates is set, then\n",
        "        # each worker will have a replay buffer of this size.\n",
        "        \"buffer_size\": int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"]),\n",
        "        # Whether to use dueling dqn\n",
        "        \"dueling\": False,\n",
        "        # Dense-layer setup for each the advantage branch and the value branch\n",
        "        # in a dueling architecture.\n",
        "        \"hiddens\": [32],\n",
        "        # Whether to use double dqn\n",
        "        \"double_q\": False,\n",
        "        # If True prioritized replay buffer will be used.\n",
        "        \"prioritized_replay\": False,\n",
        "        \"model\": {\n",
        "            # Number of hidden layers for fully connected net\n",
        "            \"fcnet_hiddens\": [32, 2],\n",
        "            # Nonlinearity for fully connected net (tanh, relu)\n",
        "            \"fcnet_activation\": \"relu\",\n",
        "        },\n",
        "    }\n",
        "    return default_DQN_config\n",
        "\n",
        "def get_exploration_config(hp):\n",
        "    exploration_config = {\n",
        "        # === Exploration Settings ===\n",
        "        # Set to False for no exploration behavior (e.g., for evaluation).\n",
        "        \"explore\": True,\n",
        "        # Provide a dict specifying the Exploration object's config.\n",
        "        \"exploration_config\": {\n",
        "            # The Exploration class to use. In the simplest case, this is the name\n",
        "            # (str) of any class present in the `rllib.utils.exploration` package.\n",
        "            # You can also provide the python class directly or the full location\n",
        "            # of your class (e.g. \"ray.rllib.utils.exploration.epsilon_greedy.\n",
        "            # EpsilonGreedy\").\n",
        "            \"type\": exploration.SoftQSchedule,\n",
        "            # Add constructor kwargs here (if any).\n",
        "            \"temperature_schedule\": PiecewiseSchedule(\n",
        "                endpoints=[\n",
        "                    (0, 1.0), (int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"] * 0.75), 0.1)],\n",
        "                outside_value=0.1,\n",
        "                framework=\"torch\")\n",
        "        },\n",
        "\n",
        "    }\n",
        "\n",
        "    return exploration_config\n",
        "\n",
        "def get_optimization_and_general_config(hp: dict):\n",
        "\n",
        "    optim_and_general_config = {\n",
        "        \n",
        "\n",
        "        # === Optimization ===\n",
        "        # Learning rate for adam optimizer\n",
        "        \"lr\": hp[\"base_lr\"],\n",
        "        # Learning rate schedule\n",
        "        \"lr_schedule\": [(0, hp[\"base_lr\"]),\n",
        "                        (int(hp[\"n_steps_per_epi\"] * hp[\"n_epi\"]), hp[\"base_lr\"] / 1e9)],\n",
        "        # How many steps of the model to sample before learning starts.\n",
        "        \"learning_starts\": int(hp[\"n_steps_per_epi\"] * hp[\"bs_epi_mul\"]),\n",
        "        # Update the replay buffer with this many samples at once. Note that\n",
        "        # this setting applies per-worker if num_workers > 1.\n",
        "        \"rollout_fragment_length\": hp[\"n_steps_per_epi\"],\n",
        "        # Size of a batch sampled from replay buffer for training. Note that\n",
        "        # if async_updates is set, then each worker returns gradients for a\n",
        "        # batch of this size.\n",
        "        \"train_batch_size\": int(hp[\"n_steps_per_epi\"] * hp[\"bs_epi_mul\"]),\n",
        "        \"gamma\": 0.5,\n",
        "\n",
        "        # === General config ===\n",
        "        \"framework\": \"torch\",\n",
        "        \"batch_mode\": \"complete_episodes\",\n",
        "        # LTFT supports only 1 worker only otherwise it would be mixing several opponents trajectories\n",
        "        \"num_workers\": 0,\n",
        "        # LTFT supports only 1 env per worker only otherwise several episodes would be played at the same time\n",
        "        \"num_envs_per_worker\": 1,\n",
        "        \"seed\": tune.grid_search(hp[\"seeds\"]),\n",
        "\n",
        "    }\n",
        "\n",
        "    return optim_and_general_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMbfhunq_ben"
      },
      "source": [
        "def get_stop_config(hp):\n",
        "    stop_config = {\n",
        "        \"episodes_total\": hp[\"n_epi\"],\n",
        "    }\n",
        "    return stop_config\n",
        "\n",
        "def train_lvl0_agents(lvl0_hparameters):\n",
        "\n",
        "    rllib_config = get_rllib_config(lvl0_hparameters)\n",
        "    stop_config = get_stop_config(lvl0_hparameters)\n",
        "    ray.shutdown()\n",
        "    ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
        "    tune_analysis_lvl0 = ray.tune.run(DQNTrainer, config=rllib_config,\n",
        "                            checkpoint_freq=0, stop=stop_config, \n",
        "                            checkpoint_at_end=True,\n",
        "                            metric=\"episode_reward_mean\", mode=\"max\",\n",
        "                            name=\"Lvl0_LTFT\")\n",
        "    ray.shutdown()\n",
        "    return tune_analysis_lvl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVcItfIxUIKO"
      },
      "source": [
        "We train 4 lvl0 LTFT agents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im5nFVs7YUtU"
      },
      "source": [
        "lvl0_hparameters = {\n",
        "    \"n_epi\": 400,\n",
        "    \"n_steps_per_epi\": 20,\n",
        "    \"bs_epi_mul\": 4,\n",
        "    \"base_lr\": 0.04,\n",
        "    \"spl_lr_mul\": 10.0,\n",
        "    \"train_n_replicates\": 2,\n",
        "    \"debug\": False,\n",
        "}\n",
        "lvl0_hparameters[\"seeds\"] = miscellaneous.get_random_seeds(lvl0_hparameters[\"train_n_replicates\"])\n",
        "\n",
        "tune_analysis_lvl0 = train_lvl0_agents(lvl0_hparameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZRENNdiSLV4"
      },
      "source": [
        "We now have several pairs of `LTFT` agents trained in self-play. We are playing in `IteratedPrisonersDilemma` and thus we may fear that an opponent could exploit our agents after they have been deployed.  \n",
        "We are going to look at that precisely. We will train lvl1 DQN agents that will learn while the `LTFT` agents are frozen (not learning any more). The DQN agents will learn by playing against a population of `LTFT` agents. This is used to simulate the fact that when training the exploiters, we may not know which `LTFT` agent will be in practice deployed and thus we want to produce an agent that would exploit any `LTFT` agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2loJhuzVho6"
      },
      "source": [
        "def train_lvl1_agents(hp_lvl1, tune_analysis_lvl0):\n",
        "\n",
        "    rllib_config = get_rllib_config(hp_lvl1)\n",
        "    stop_config = get_stop_config(hp_lvl1)\n",
        "\n",
        "    # We use an helper to extract all the checkpoints saved in the tune_analysis_lvl0\n",
        "    checkpoints_lvl0 = miscellaneous.extract_checkpoints(tune_analysis_lvl0)\n",
        "    \n",
        "    rllib_config = modify_conf_for_lvl1_training(hp_lvl1, get_env_config(hp_lvl1)[\"env_config\"], rllib_config, checkpoints_lvl0)\n",
        "\n",
        "    ray.shutdown()\n",
        "    ray.init(num_cpus=os.cpu_count(), num_gpus=0) \n",
        "    tune_analysis_lvl1 = ray.tune.run(DQNTrainer, config=rllib_config,\n",
        "                                      stop=stop_config,\n",
        "                                      checkpoint_at_end=True,\n",
        "                                      metric=\"episode_reward_mean\", mode=\"max\",\n",
        "                                      name=\"Lvl1_DQN\")\n",
        "    ray.shutdown()\n",
        "    return tune_analysis_lvl1\n",
        "\n",
        "def modify_conf_for_lvl1_training(hp_lvl1, env_config, rllib_config_lvl1, lvl0_checkpoints):\n",
        "    # The lvl0 agents will be player 2, The lvl1 agents will be player 1\n",
        "    lvl0_policy_idx = 1\n",
        "    lvl1_policy_idx = 0\n",
        "    lvl0_policy_id = env_config[\"players_ids\"][lvl0_policy_idx]\n",
        "    lvl1_policy_id = env_config[\"players_ids\"][lvl1_policy_idx]\n",
        "\n",
        "    # Use a DQN Policy (with SGD + Momentum optimizer) as the lvl1 agent (instead of LTFT with nested DQN)\n",
        "    MyDQNTorchPolicy = DQNTorchPolicy.with_updates(optimizer_fn=sgd_optimizer_dqn)\n",
        "    rllib_config_lvl1[\"multiagent\"][\"policies\"][lvl1_policy_id] = (\n",
        "        MyDQNTorchPolicy,\n",
        "        IteratedPrisonersDilemma.OBSERVATION_SPACE,\n",
        "        IteratedPrisonersDilemma.ACTION_SPACE,\n",
        "        {\"sgd_momentum\": 0.9}\n",
        "    )\n",
        "\n",
        "    # We add a callack needed by the PopulationOfIdenticalAlgo\n",
        "    rllib_config_lvl1[\"callbacks\"] = miscellaneous.merge_callbacks(\n",
        "        LTFTCallbacks,\n",
        "        population.PopulationOfIdenticalAlgoCallBacks,\n",
        "        log.get_logging_callbacks_class(log_env_step=True, log_from_policy=True))\n",
        "\n",
        "    # Finally, we replace player_2's policy (LTFT) by a PopulationOfIdenticalAlgo policy that use nested LTFT policies\n",
        "    lvl1_best_response.prepare_config_for_lvl1_training(\n",
        "        config=rllib_config_lvl1,\n",
        "        lvl0_policy_id=lvl0_policy_id, \n",
        "        lvl1_policy_id=lvl1_policy_id,\n",
        "        select_n_lvl0_from_population=hp_lvl1[\"n_seeds_lvl0\"] // hp_lvl1[\"n_seeds_lvl1\"], # Each lvl1 agent will be trained against a population of select_n_lvl0_from_population lvl0 agents\n",
        "        n_lvl1_to_train=hp_lvl1[\"n_seeds_lvl1\"], # We will train n_lvl1 agents\n",
        "        overlapping_population=False, # We only use once the lvl0 agents\n",
        "        lvl0_checkpoints=lvl0_checkpoints)\n",
        "    # l1br_configuration_helper = lvl1_best_response.L1BRConfigurationHelper(rllib_config_lvl1, lvl0_policy_id, lvl1_policy_id)\n",
        "    # l1br_configuration_helper.define_exp(\n",
        "    #     use_n_lvl0_agents_in_each_population=hp_lvl1[\"n_seeds_lvl0\"] // hp_lvl1[\"n_seeds_lvl1\"],\n",
        "    #     train_n_lvl1_agents=hp_lvl1[\"n_seeds_lvl1\"],\n",
        "    #     lvl0_checkpoints=lvl0_checkpoints)\n",
        "    # rllib_config_lvl1 = l1br_configuration_helper.prepare_config_for_lvl1_training()\n",
        "\n",
        "    return rllib_config_lvl1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2kmotZMYZBX"
      },
      "source": [
        "lvl1_hparameters = copy.deepcopy(lvl0_hparameters)\n",
        "lvl1_hparameters[\"n_seeds_lvl0\"] = lvl0_hparameters[\"train_n_replicates\"]\n",
        "lvl1_hparameters[\"n_seeds_lvl1\"] = min(lvl0_hparameters[\"train_n_replicates\"]//2, 2)\n",
        "\n",
        "tune_analysis_lvl1 = train_lvl1_agents(lvl1_hparameters, tune_analysis_lvl0)\n",
        "print(tune_analysis_lvl1.results_df.columns)\n",
        "print(tune_analysis_lvl1.results_df.head())\n",
        "print(tune_analysis_lvl1.results_df[\"episode_reward_mean\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Cg-m_n4Itx"
      },
      "source": [
        "##  c. Use TensorBoard to visualize the trainings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BisqDo64Itx"
      },
      "source": [
        "You can use TensorBoard to view trial performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u00VE8oB4Itx"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NpeJySSk4Itx"
      },
      "source": [
        "%tensorboard --logdir /root/ray_results/ # On Google Colab\n",
        "# %tensorboard --logdir ~/ray_results/ # On your machine\n",
        "\n",
        "# You can filter the graphs with \"mean_welfare|defection_metric|entropy|CC\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}