import logging

from ray.rllib import SampleBatch
from ray.rllib.utils import merge_dicts
from ray.rllib.utils.annotations import override

from marltoolbox.algos import hierarchical, amTFT
from marltoolbox.algos.amTFT import base
from marltoolbox.algos.exploiters import level_1, dual_behavior
from marltoolbox.utils import restore

logger = logging.getLogger(__name__)

DEFAULT_CONFIG = merge_dicts(
    level_1.DEFAULT_CONFIG,
    {
        "lookahead_n_times": 5,
        "exploiter_activated": True,
    }
)


class _AmtftExploiterWeightsManager:
    DUAL_WEIGHTS_KEY = "dual_policies"
    COMMENT_MOVING_WEIGHTS_AROUND = """
        Complicated moving around the weights to allow to use the
        vanilla WeightsExchanger & setting the lvl1 agent to observe our
        dual policies (not observing the opponent).
        """

    @override(hierarchical.HierarchicalTorchPolicy)
    def get_weights(self):
        logger.debug(self.COMMENT_MOVING_WEIGHTS_AROUND)

        all_weights = super().get_weights()
        lvl1_weights, dual_weights = \
            self._invert_dual_pi_and_lvl1_pi(all_weights)

        partially_inversed_weights = lvl1_weights
        partially_inversed_weights[self.DUAL_WEIGHTS_KEY] = dual_weights
        return partially_inversed_weights

    def _invert_dual_pi_and_lvl1_pi(self, all_weights):
        (coop_pi_key, selfish_pi_key, lvl1_pi_key,
         own_coop_pi_key, own_selfish_pi_key,
         opp_coop_pi_key, opp_selfish_pi_key) = \
            self._get_policies_weights_keys()

        lvl1_weights = all_weights.pop(lvl1_pi_key)
        dual_weights = all_weights

        tmp_own_coop_pi = lvl1_weights[own_coop_pi_key]
        tmp_own_selfish_pi = lvl1_weights[own_selfish_pi_key]

        lvl1_weights[own_coop_pi_key] = dual_weights[coop_pi_key]
        lvl1_weights[own_selfish_pi_key] = dual_weights[selfish_pi_key]

        dual_weights[coop_pi_key] = tmp_own_coop_pi
        dual_weights[selfish_pi_key] = tmp_own_selfish_pi

        return lvl1_weights, dual_weights

    def _get_policies_weights_keys(self):
        coop_pi_key = self.nested_key(self.COOP_POLICY_IDX)
        selfish_pi_key = self.nested_key(self.SELFISH_POLICY_IDX)
        lvl1_pi_key = self.nested_key(self.LEVEL_1_POLICY_IDX)
        own_coop_pi_key = self.simulated_opponent.nested_key(
            base.OWN_COOP_POLICY_IDX)
        own_selfish_pi_key = self.simulated_opponent.nested_key(
            base.OWN_SELFISH_POLICY_IDX)
        opp_coop_pi_key = self.simulated_opponent.nested_key(
            base.OPP_COOP_POLICY_IDX)
        opp_selfish_pi_key = self.simulated_opponent.nested_key(
            base.OPP_SELFISH_POLICY_IDX)
        return coop_pi_key, selfish_pi_key, lvl1_pi_key, \
               own_coop_pi_key, own_selfish_pi_key, \
               opp_coop_pi_key, opp_selfish_pi_key

    @override(hierarchical.HierarchicalTorchPolicy)
    def set_weights(self, weights):

        partially_inversed_weights = weights
        weights = \
            self._reverse_dual_pi_and_lvl1_pi(partially_inversed_weights)
        super().set_weights(weights)

    def _reverse_dual_pi_and_lvl1_pi(self, partially_inversed_weights):
        (coop_pi_key, selfish_pi_key, lvl1_pi_key,
         own_coop_pi_key, own_selfish_pi_key,
         opp_coop_pi_key, opp_selfish_pi_key) = \
            self._get_policies_weights_keys()

        dual_weights = partially_inversed_weights.pop(self.DUAL_WEIGHTS_KEY)
        lvl1_weights = partially_inversed_weights

        tmp_own_coop_pi = lvl1_weights[own_coop_pi_key]
        tmp_own_selfish_pi = lvl1_weights[own_selfish_pi_key]

        lvl1_weights[own_coop_pi_key] = dual_weights[coop_pi_key]
        lvl1_weights[own_selfish_pi_key] = dual_weights[selfish_pi_key]

        dual_weights[coop_pi_key] = tmp_own_coop_pi
        dual_weights[selfish_pi_key] = tmp_own_selfish_pi

        weights = dual_weights
        weights[lvl1_pi_key] = lvl1_weights
        return weights

    def _set_lvl1_as_opponent(self, weights):

        (_, _, _,
         own_coop_pi_key, own_selfish_pi_key,
         opp_coop_pi_key, opp_selfish_pi_key) = \
            self._get_policies_weights_keys()

        tmp_own_coop_pi_key = weights[own_coop_pi_key]
        tmp_own_selfish_pi_key = weights[own_selfish_pi_key]

        weights[own_coop_pi_key] = weights[opp_coop_pi_key]
        weights[own_selfish_pi_key] = weights[opp_selfish_pi_key]

        weights[opp_coop_pi_key] = tmp_own_coop_pi_key
        weights[opp_selfish_pi_key] = tmp_own_selfish_pi_key

        return weights


class Level1amTFTExploiterTorchPolicy(_AmtftExploiterWeightsManager,
                                      level_1.Level1ExploiterTorchPolicy,
                                      amTFT.AmTFTReferenceClass):

    def __init__(self, observation_space, action_space, config, **kwargs):
        super().__init__(observation_space, action_space, config, **kwargs)
        self.active_algo_idx = self.COOP_POLICY_IDX
        self.working_state = config["working_state"]
        self.exploiter_activated = config.get("exploiter_activated")

        if restore.LOAD_FROM_CONFIG_KEY in config.keys():
            restore.after_init_load_policy_checkpoint(self)

        if not self.exploiter_activated:
            self._set_simulated_opponent_as_vanilla_amTFT()

    def _set_simulated_opponent_as_vanilla_amTFT(self):
        opp_coop = \
            self.simulated_opponent.algorithms[base.OWN_COOP_POLICY_IDX]
        opp_selfish = \
            self.simulated_opponent.algorithms[base.OWN_SELFISH_POLICY_IDX]
        own_coop = \
            self.simulated_opponent.algorithms[base.OPP_COOP_POLICY_IDX]
        own_seflish = \
            self.simulated_opponent.algorithms[base.OPP_SELFISH_POLICY_IDX]

        self.simulated_opponent.algorithms[base.OWN_COOP_POLICY_IDX] = \
            own_coop
        self.simulated_opponent.algorithms[base.OWN_SELFISH_POLICY_IDX] = \
            own_seflish
        self.simulated_opponent.algorithms[base.OPP_COOP_POLICY_IDX] = \
            opp_coop
        self.simulated_opponent.algorithms[base.OWN_SELFISH_POLICY_IDX] = \
            opp_selfish

    @property
    def working_state(self):
        return self.simulated_opponent.working_state

    @working_state.setter
    def working_state(self, value):
        self.simulated_opponent.working_state = value

    def on_episode_start(self, worker, *args, **kwargs):
        self.worker = worker

    def on_episode_end(self, *args, **kwargs):
        self.simulated_opponent.on_episode_end(*args, **kwargs)

    def on_observation_fn(self, *args, **kwargs):
        self.simulated_opponent.on_observation_fn(*args, **kwargs)

    def on_episode_step(self, *args, **kwargs):
        self.simulated_opponent.on_episode_step(*args, **kwargs)

    def on_train_result(self, *args, **kwargs):
        self.simulated_opponent.on_train_result(*args, **kwargs)

    @override(level_1.Level1ExploiterTorchPolicy)
    def _train_level_1_policy(self, learner_stats, samples):
        # Don't train the nested amTFT policy (the weights will be copied
        # into it)
        return learner_stats

    @override(dual_behavior.DualBehaviorTorchPolicy)
    def _train_dual_policies(self, samples: SampleBatch):
        learner_stats = {"learner_stats": {}}
        for policy_n, policy in enumerate(self.algorithms):
            if policy_n == self.active_algo_idx:
                logger.debug(f"train policy {policy}")
                samples_copy = samples.copy()
                samples_copy = self._modify_batch_for_policy(policy_n,
                                                             samples_copy)
                learner_stats_one_policy = policy.learn_on_batch(samples_copy)
                learner_stats["learner_stats"][f"algo{policy_n}"] = \
                    learner_stats_one_policy
        return learner_stats

    @override(level_1.Level1ExploiterTorchPolicy)
    def compute_actions(self, *args, **kwargs):

        self._to_log["use_simulated_opponent"] = 0.0
        self._to_log["use_exploiter_cooperating"] = 0.0
        self._to_log["use_exploiter_selfish"] = 0.0

        # use the simulated amTFT opponent when performing rollouts
        if self.simulated_opponent.performing_rollouts:
            self._to_log["use_simulated_opponent"] = 1.0
            return self.simulated_opponent.compute_actions(*args, **kwargs)

        if not self.exploiter_activated:
            self._to_log["use_simulated_opponent"] = 1.0
            return self.simulated_opponent.compute_actions(*args, **kwargs)

        # use the coop policy when training the coop policy of amTFT
        is_training_coop = \
            self.simulated_opponent.working_state == amTFT.WORKING_STATES[0]
        if is_training_coop:
            self.active_algo_idx = self.COOP_POLICY_IDX
            self._to_log["use_exploiter_cooperating"] = 1.0
            return dual_behavior.DualBehaviorTorchPolicy.compute_actions(
                self, *args, **kwargs)

        # use the selfish policy when training the selfish policy of amTFT
        is_training_selfish = \
            self.simulated_opponent.working_state == amTFT.WORKING_STATES[1]
        if is_training_selfish:
            self.active_algo_idx = self.SELFISH_POLICY_IDX
            self._to_log["use_exploiter_selfish"] = 1.0
            return dual_behavior.DualBehaviorTorchPolicy.compute_actions(
                self, *args, **kwargs)

        # use the selfish policy if the opponent will not punish
        return super().compute_actions(*args, **kwargs)

    def _is_cooperation_needed_to_prevent_punishement(
            self, selfish_action_tuple, *args, **kwargs):
        self._to_log["expl_min_anticipated_gain"] = None

        super()._is_cooperation_needed_to_prevent_punishement(
            selfish_action_tuple, *args, **kwargs)

    def _lookahead_for_opponent_punishing(self,
                                          selfish_action,
                                          *args,
                                          **kwargs):
        selfish_action = selfish_action[0]
        last_obs = kwargs["episodes"][0]._agent_to_last_raw_obs
        selfish_step_debit = self.simulated_opponent._compute_debit(
            last_obs, selfish_action, self.worker, None, None, None, None)

        if "expl_min_anticipated_gain" not in self._to_log.keys() or \
                self._to_log["expl_min_anticipated_gain"] > selfish_step_debit:
            self._to_log["expl_min_anticipated_gain"] = selfish_step_debit

        # being selfish has a negative impact against a cooperative opponent
        selfish_action_is_not_useful = selfish_step_debit <= 0

        opponent_will_start_to_punish = \
            self.simulated_opponent._is_starting_new_punishment_required(
                manual_threshold=
                self.simulated_opponent.debit_threshold -
                selfish_step_debit)

        return selfish_action_is_not_useful or opponent_will_start_to_punish

