from ray.rllib.utils import merge_dicts
from ray.rllib.utils.annotations import override

from marltoolbox.algos import hierarchical, amTFT
from marltoolbox.algos.exploiters import level_1

DEFAULT_CONFIG = merge_dicts(
    level_1.DEFAULT_CONFIG,
    {
        "bias_in_punishment_decision": 0.0,
        "multiplier_in_punishment_decision": 1.0,
    }
)


class Level1amTFTExploiterTorchPolicy(level_1.Level1ExploiterTorchPolicy,
                                      amTFT.AmTFTReferenceClass):
    AMTFT_POLICY_IDX = level_1.Level1ExploiterTorchPolicy.LEVEL_1_POLICY_IDX
    DUAL_WEIGHTS_KEY = "dual_policies"

    def __init__(self, observation_space, action_space, config, **kwargs):
        super().__init__(observation_space, action_space, config, **kwargs)

        self.lvl1_amTFT_algo = self.algorithms[self.AMTFT_POLICY_IDX]

    def on_episode_end(self, *args, **kwargs):
        self.lvl1_amTFT_algo.on_episode_end(*args, **kwargs)

    def on_observation_fn(self, *args, **kwargs):
        self.lvl1_amTFT_algo.on_observation_fn(*args, **kwargs)

    def on_episode_step(self, *args, **kwargs):
        self.lvl1_amTFT_algo.on_episode_step(*args, **kwargs)

    def on_train_result(self, *args, **kwargs):
        self.lvl1_amTFT_algo.on_train_result(*args, **kwargs)

    def _train_level_1_policy(self, learner_stats, samples):
        return learner_stats

    def _is_cooperation_needed_to_prevent_punishement(self):
        biaised_threshold = \
            (self.multiplier_in_punishment_decision * \
             self.lvl1_amTFT_algo.debit_threshold) + \
            self.bias_in_punishment_decision

        is_close_to_start_punishement = \
            self.lvl1_amTFT_algo._is_starting_new_punishment_required(
                manual_threshold=biaised_threshold
            )
        is_punishing = self.lvl1_amTFT_algo._is_punishment_planned()

        return is_close_to_start_punishement or is_punishing

    MOVING_WEIGHTS_AROUND_CLUE = """
        Complicated moving aroud the weights to allow to use the
        vanilla WeightsExchanger & setting the lvl1 agent to observe our
        dual policies not the opponent.
        For the complete logic look at:
        get_weights then set_weights then _set_lvl1_as_opponent 
        """

    @override(hierarchical.HierarchicalTorchPolicy)
    def get_weights(self):
        self.MOVING_WEIGHTS_AROUND_CLUE

        all_weights = super().get_weights()
        lvl1_weights, dual_weights = \
            self._invert_dual_pi_and_lvl1_pi(all_weights)

        partially_inversed_weights = lvl1_weights
        partially_inversed_weights[self.DUAL_WEIGHTS_KEY] = dual_weights
        return partially_inversed_weights

    def _invert_dual_pi_and_lvl1_pi(self, all_weights):
        (coop_pi_key, selfish_pi_key, lvl1_pi_key,
         own_coop_pi_key, own_selfish_pi_key,
         opp_coop_pi_key, opp_selfish_pi_key) = \
            self._get_policies_weights_keys()

        lvl1_weights = all_weights.pop(lvl1_pi_key)
        dual_weights = all_weights

        tmp_own_coop_pi = lvl1_weights[own_coop_pi_key]
        tmp_own_selfish_pi = lvl1_weights[own_selfish_pi_key]

        lvl1_weights[own_coop_pi_key] = dual_weights[coop_pi_key]
        lvl1_weights[own_selfish_pi_key] = dual_weights[selfish_pi_key]

        dual_weights[coop_pi_key] = tmp_own_coop_pi
        dual_weights[selfish_pi_key] = tmp_own_selfish_pi

        return lvl1_weights, dual_weights

    def _get_policies_weights_keys(self):
        coop_pi_key = self.nested_key(self.COOP_POLICY_IDX)
        selfish_pi_key = self.nested_key(self.SELFISH_POLICY_IDX)
        lvl1_pi_key = self.nested_key(self.LEVEL_1_POLICY_IDX)
        own_coop_pi_key = self.lvl1_amTFT_algo.nested_key(
            self.lvl1_amTFT_algo.OWN_COOP_POLICY_IDX)
        own_selfish_pi_key = self.lvl1_amTFT_algo.nested_key(
            self.lvl1_amTFT_algo.OWN_SELFISH_POLICY_IDX)
        opp_coop_pi_key = self.lvl1_amTFT_algo.nested_key(
            self.lvl1_amTFT_algo.OPP_COOP_POLICY_IDX)
        opp_selfish_pi_key = self.lvl1_amTFT_algo.nested_key(
            self.lvl1_amTFT_algo.OPP_SELFISH_POLICY_IDX)
        return coop_pi_key, selfish_pi_key, lvl1_pi_key, \
               own_coop_pi_key, own_selfish_pi_key, \
               opp_coop_pi_key, opp_selfish_pi_key

    @override(hierarchical.HierarchicalTorchPolicy)
    def set_weights(self, weights):
        self.MOVING_WEIGHTS_AROUND_CLUE

        partially_inversed_weights = weights
        weights = \
            self._reverse_dual_pi_and_lvl1_pi(partially_inversed_weights)
        super().set_weights(weights)

    def _reverse_dual_pi_and_lvl1_pi(self, partially_inversed_weights):
        (coop_pi_key, selfish_pi_key, lvl1_pi_key,
         own_coop_pi_key, own_selfish_pi_key,
         opp_coop_pi_key, opp_selfish_pi_key) = \
            self._get_policies_weights_keys()

        dual_weights = partially_inversed_weights.pop(self.DUAL_WEIGHTS_KEY)
        lvl1_weights = partially_inversed_weights

        tmp_own_coop_pi = lvl1_weights[own_coop_pi_key]
        tmp_own_selfish_pi = lvl1_weights[own_selfish_pi_key]

        lvl1_weights[own_coop_pi_key] = dual_weights[coop_pi_key]
        lvl1_weights[own_selfish_pi_key] = dual_weights[selfish_pi_key]

        dual_weights[coop_pi_key] = tmp_own_coop_pi
        dual_weights[selfish_pi_key] = tmp_own_selfish_pi

        weights = dual_weights
        weights[lvl1_pi_key] = lvl1_weights
        return weights

    @property
    def working_state(self):
        return self.lvl1_amTFT_algo.working_state

    @working_state.setter
    def working_state(self, value):
        self.lvl1_amTFT_algo.working_state = value

    def _set_lvl1_as_opponent(self, weights):
        self.MOVING_WEIGHTS_AROUND_CLUE

        (_, _, _,
         own_coop_pi_key, own_selfish_pi_key,
         opp_coop_pi_key, opp_selfish_pi_key) = \
            self._get_policies_weights_keys()

        tmp_own_coop_pi_key = weights[own_coop_pi_key]
        tmp_own_selfish_pi_key = weights[own_selfish_pi_key]

        weights[own_coop_pi_key] = weights[opp_coop_pi_key]
        weights[own_selfish_pi_key] = weights[opp_selfish_pi_key]

        weights[opp_coop_pi_key] = tmp_own_coop_pi_key
        weights[opp_selfish_pi_key] = tmp_own_selfish_pi_key

        return weights