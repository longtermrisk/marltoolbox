import logging

from ray.rllib import SampleBatch
from ray.rllib.utils import merge_dicts
from ray.rllib.utils.annotations import override

from marltoolbox.algos import hierarchical, amTFT
from marltoolbox.algos.amTFT import base
from marltoolbox.algos.exploiters import level_1, dual_behavior
from marltoolbox.utils import restore

logger = logging.getLogger(__name__)

DEFAULT_CONFIG = merge_dicts(
    level_1.DEFAULT_CONFIG,
    {
        "lookahead_n_times": 5,
        "exploiter_activated": True,
    },
)


class Level1amTFTExploiterTorchPolicy(
    level_1.Level1ExploiterTorchPolicy, amTFT.AmTFTReferenceClass
):
    DUAL_WEIGHTS_KEY = "dual_policies"

    def __init__(self, observation_space, action_space, config, **kwargs):
        super().__init__(observation_space, action_space, config, **kwargs)
        self.active_algo_idx = self.COOP_POLICY_IDX
        self.working_state = config["working_state"]
        self.exploiter_activated = config.get("exploiter_activated")

        if restore.LOAD_FROM_CONFIG_KEY in config.keys():
            restore.after_init_load_policy_checkpoint(self)

        if self.working_state in base.WORKING_STATES_IN_EVALUATION:
            if self.exploiter_activated:
                self._set_lvl1_to_behave_like_opponent()

    @property
    def working_state(self):
        return self.simulated_opponent.working_state

    @working_state.setter
    def working_state(self, value):
        self.simulated_opponent.working_state = value

    def on_episode_start(self, worker, *args, **kwargs):
        self.worker = worker

    def on_episode_end(self, *args, **kwargs):
        self.simulated_opponent.on_episode_end(*args, **kwargs)

    def on_observation_fn(self, *args, **kwargs):
        self.simulated_opponent.on_observation_fn(*args, **kwargs)

    def on_episode_step(self, *args, **kwargs):
        self.simulated_opponent.on_episode_step(*args, **kwargs)

    def on_train_result(self, *args, **kwargs):
        self.simulated_opponent.on_train_result(*args, **kwargs)

    @override(level_1.Level1ExploiterTorchPolicy)
    def _train_level_1_policy(self, learner_stats, samples):
        learner_stats_simu_opp = self.simulated_opponent.learn_on_batch(
            samples
        )
        learner_stats["simu_opp"] = learner_stats_simu_opp
        return learner_stats

    @override(dual_behavior.DualBehaviorTorchPolicy)
    def _train_dual_policies(self, samples: SampleBatch):
        learner_stats = {"learner_stats": {}}
        if self.working_state == base.WORKING_STATES[0]:
            learner_stats["learner_stats"] = super()._train_dual_policies(
                samples
            )
        return learner_stats

    @override(level_1.Level1ExploiterTorchPolicy)
    def compute_actions(self, *args, **kwargs):

        # use the simulated amTFT opponent when performing rollouts
        if self.simulated_opponent.performing_rollouts:
            return self.simulated_opponent.compute_actions(*args, **kwargs)

        self._to_log["use_simulated_opponent"] = 0.0
        self._to_log["use_exploiter_cooperating"] = 0.0
        self._to_log["use_exploiter_selfish"] = 0.0

        # Use the simulated amTFT opponent when training
        if self.working_state not in base.WORKING_STATES_IN_EVALUATION:
            self._to_log["use_simulated_opponent"] = 1.0
            self.active_algo_idx = self.LEVEL_1_POLICY_IDX
            return self.simulated_opponent.compute_actions(*args, **kwargs)

        # Use the simulated amTFT opponent when not using the exploiter
        if not self.exploiter_activated:
            self._to_log["use_simulated_opponent"] = 1.0
            self.active_algo_idx = self.LEVEL_1_POLICY_IDX
            return self.simulated_opponent.compute_actions(*args, **kwargs)

        outputs = super().compute_actions(*args, **kwargs)
        return outputs

    def _is_cooperation_needed_to_prevent_punishement(
        self, selfish_action_tuple, *args, **kwargs
    ):
        self._to_log["expl_min_anticipated_gain"] = None

        return super()._is_cooperation_needed_to_prevent_punishement(
            selfish_action_tuple, *args, **kwargs
        )

    def _lookahead_for_opponent_punishing(
        self, selfish_action, *args, **kwargs
    ) -> bool:
        selfish_action = selfish_action[0]
        last_obs = kwargs["episodes"][0]._agent_to_last_raw_obs
        selfish_step_debit = self.simulated_opponent._compute_debit(
            last_obs, selfish_action, self.worker, None, None, None, None
        )
        if (
            "expl_min_anticipated_gain" not in self._to_log.keys()
            or self._to_log["expl_min_anticipated_gain"] is None
            or self._to_log["expl_min_anticipated_gain"] > selfish_step_debit
        ):
            self._to_log["expl_min_anticipated_gain"] = selfish_step_debit

        # being selfish has a negative impact against a cooperative opponent
        selfish_action_is_not_useful = selfish_step_debit <= 0

        opponent_will_start_to_punish = (
            self.simulated_opponent._is_starting_new_punishment_required(
                manual_threshold=self.simulated_opponent.debit_threshold
                - selfish_step_debit
            )
        )
        return selfish_action_is_not_useful or opponent_will_start_to_punish

    @override(hierarchical.HierarchicalTorchPolicy)
    def get_weights(self):
        all_weights = super().get_weights()

        lvl1_weights, dual_weights = self._split_dual_pi_and_lv1_pi(
            all_weights
        )
        formatted_weights = lvl1_weights
        formatted_weights[self.DUAL_WEIGHTS_KEY] = dual_weights
        return formatted_weights

    def _split_dual_pi_and_lv1_pi(self, all_weights):

        lvl1_pi_key = self.nested_key(self.LEVEL_1_POLICY_IDX)
        lvl1_weights = all_weights.pop(lvl1_pi_key)
        dual_weights = all_weights

        return lvl1_weights, dual_weights

    @override(hierarchical.HierarchicalTorchPolicy)
    def set_weights(self, formatted_weights):
        all_weights = self._reverse_weights_split(formatted_weights)
        super().set_weights(all_weights)

    def _reverse_weights_split(self, formatted_weights):

        dual_weights = formatted_weights.pop(self.DUAL_WEIGHTS_KEY)
        lvl1_weights = formatted_weights

        lvl1_pi_key = self.nested_key(self.LEVEL_1_POLICY_IDX)
        all_weights = dual_weights
        all_weights[lvl1_pi_key] = lvl1_weights

        return all_weights

    def _set_lvl1_to_behave_like_opponent(self):
        weights = self.get_weights()

        (
            own_coop_pi_key,
            own_selfish_pi_key,
            opp_coop_pi_key,
            opp_selfish_pi_key,
        ) = self._get_policies_weights_keys()

        tmp_own_coop_pi_key = weights[own_coop_pi_key]
        tmp_own_selfish_pi_key = weights[own_selfish_pi_key]

        weights[own_coop_pi_key] = weights[opp_coop_pi_key]
        weights[own_selfish_pi_key] = weights[opp_selfish_pi_key]

        weights[opp_coop_pi_key] = tmp_own_coop_pi_key
        weights[opp_selfish_pi_key] = tmp_own_selfish_pi_key

        self.set_weights(weights)

    def _get_policies_weights_keys(self):
        own_coop_pi_key = self.simulated_opponent.nested_key(
            base.OWN_COOP_POLICY_IDX
        )
        own_selfish_pi_key = self.simulated_opponent.nested_key(
            base.OWN_SELFISH_POLICY_IDX
        )
        opp_coop_pi_key = self.simulated_opponent.nested_key(
            base.OPP_COOP_POLICY_IDX
        )
        opp_selfish_pi_key = self.simulated_opponent.nested_key(
            base.OPP_SELFISH_POLICY_IDX
        )

        return (
            own_coop_pi_key,
            own_selfish_pi_key,
            opp_coop_pi_key,
            opp_selfish_pi_key,
        )
