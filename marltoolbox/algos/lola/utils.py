"""
Various utility functions.
"""
import numpy as np
import tensorflow as tf


def batch_to_seq(h, nbatch, nsteps, flat=False):
    if flat:
        h = tf.reshape(h, [nbatch, nsteps])
    else:
        h = tf.reshape(h, [nbatch, nsteps, -1])
    return [
        tf.squeeze(v, [1])
        for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)
    ]


def seq_to_batch(h, flat=False):
    shape = h[0].get_shape().as_list()
    if not flat:
        assert len(shape) > 1
        nh = h[0].get_shape()[-1].value
        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
    else:
        return tf.reshape(tf.stack(values=h, axis=1), [-1])


def lstm(xs, s, scope, nh, init_scale=1.0):
    nbatch, nin = [v.value for v in xs[0].get_shape()]
    nsteps = len(xs)
    with tf.variable_scope(scope):
        wx = tf.get_variable(
            "wx", [nin, nh * 4], initializer=ortho_init(init_scale)
        )
        wh = tf.get_variable(
            "wh", [nh, nh * 4], initializer=ortho_init(init_scale)
        )
        b = tf.get_variable(
            "b", [nh * 4], initializer=tf.constant_initializer(0.0)
        )

    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
    for idx, x in enumerate(xs):
        c = c
        h = h
        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b
        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
        i = tf.nn.sigmoid(i)
        f = tf.nn.sigmoid(f)
        o = tf.nn.sigmoid(o)
        u = tf.tanh(u)
        c = f * c + i * u
        h = o * tf.tanh(c)
        xs[idx] = h
    s = tf.concat(axis=1, values=[c, h])
    return xs, s


def ortho_init(scale=1.0):
    def _ortho_init(shape, dtype, partition_info=None):
        # lasagne ortho init for tf
        shape = tuple(shape)
        if len(shape) == 2:
            flat_shape = shape
        elif len(shape) == 4:  # assumes NHWC
            flat_shape = (np.prod(shape[:-1]), shape[-1])
        else:
            raise NotImplementedError
        a = np.random.normal(0.0, 1.0, flat_shape)
        u, _, v = np.linalg.svd(a, full_matrices=False)
        q = (
            u if u.shape == flat_shape else v
        )  # pick the one with the correct shape
        q = q.reshape(shape)
        return (scale * q[: shape[0], : shape[1]]).astype(np.float32)

    return _ortho_init


def get_session():
    return tf.get_default_session()


def var_shape(x):
    out = x.get_shape().as_list()
    return out


def intprod(x):
    return int(np.prod(x))


def numel(x):
    return intprod(var_shape(x))


def flatgrad(loss, var_list, clip_norm=None):
    grads = tf.gradients(loss, var_list)
    if clip_norm is not None:
        grads = [tf.clip_by_norm(grad, clip_norm=clip_norm) for grad in grads]
    return tf.concat(
        axis=0,
        values=[
            tf.reshape(
                grad if grad is not None else tf.zeros_like(v), [numel(v)]
            )
            for (v, grad) in zip(var_list, grads)
        ],
    )


class SetFromFlat(object):
    def __init__(self, var_list, dtype=tf.float32):
        assigns = []
        shapes = list(map(var_shape, var_list))
        total_size = np.sum([intprod(shape) for shape in shapes])

        self.theta = theta = tf.placeholder(dtype, [total_size])
        start = 0
        assigns = []
        for (shape, v) in zip(shapes, var_list):
            size = intprod(shape)
            assigns.append(
                tf.assign(v, tf.reshape(theta[start : start + size], shape))
            )
            start += size
        self.op = tf.group(*assigns)

    def __call__(self, theta):
        get_session().run(self.op, feed_dict={self.theta: theta})


class SetFromFlatWtSess(object):
    def __init__(self, var_list, sess, dtype=tf.float32):
        self.sess = sess

        assigns = []
        shapes = list(map(var_shape, var_list))
        total_size = np.sum([intprod(shape) for shape in shapes])

        self.theta = theta = tf.placeholder(dtype, [total_size])
        start = 0
        assigns = []
        for (shape, v) in zip(shapes, var_list):
            size = intprod(shape)
            assigns.append(
                tf.assign(v, tf.reshape(theta[start : start + size], shape))
            )
            start += size
        self.op = tf.group(*assigns)

    def __call__(self, theta):
        self.sess.run(self.op, feed_dict={self.theta: theta})


class GetFlat(object):
    def __init__(self, var_list):
        self.op = tf.concat(
            axis=0, values=[tf.reshape(v, [numel(v)]) for v in var_list]
        )

    def __call__(self):
        return get_session().run(self.op)


class GetFlatWtSess(object):
    def __init__(self, var_list, sess):
        self.sess = sess
        self.op = tf.concat(
            axis=0, values=[tf.reshape(v, [numel(v)]) for v in var_list]
        )

    def __call__(self):
        return self.sess.run(self.op)


def get_monte_carlo(reward, y, trace_length, batch_size):
    reward = np.reshape(reward, ((batch_size, trace_length)))
    reward_buffer = np.zeros(((batch_size, trace_length + 1)))
    reward_buffer[:, :trace_length] = reward
    discounted_reward = np.zeros(((batch_size, trace_length)))

    for t in range(trace_length - 1, -1, -1):
        reward_buffer[:, t + 1 :] *= y
        discounted_reward[:, t] = np.sum(reward_buffer[:, t:], 1)

    return np.reshape(discounted_reward, (batch_size * trace_length))


def make_cube(trace_length):
    cube = tf.Variable(tf.zeros([trace_length, trace_length, trace_length]))

    cube_ops = []
    for i in range(trace_length):
        cube_ops.append(
            cube[i, : (i + 1), : (i + 1)].assign(tf.ones([i + 1, i + 1]))
        )
    return cube, cube_ops
