import logging

import numpy as np
from ray.rllib import SampleBatch
from ray.rllib.utils import merge_dicts
from ray.rllib.utils.annotations import override

from marltoolbox.algos import hierarchical
from marltoolbox.utils import postprocessing

logger = logging.getLogger(__name__)

DEFAULT_CONFIG = merge_dicts(
    hierarchical.DEFAULT_CONFIG,
    {
        "welfare_key": postprocessing.WELFARE_UTILITARIAN,
    }
)


class DualBehaviorTorchPolicy(hierarchical.HierarchicalTorchPolicy):
    """
    This policy trains 2 replicates of the same policy with different
    initialization. The first policy is cooperative while the second is more
    selfish. (You need to provide the configuration to step these behavior)

    """
    COOP_POLICY_IDX = 0
    SELFISH_POLICY_IDX = 1
    DUAL_POLICIES = (COOP_POLICY_IDX, SELFISH_POLICY_IDX)

    def __init__(self, observation_space, action_space, config, **kwargs):
        super().__init__(observation_space, action_space, config, **kwargs)
        self.active_algo_idx = self.SELFISH_POLICY_IDX

    @override(hierarchical.HierarchicalTorchPolicy)
    def _learn_on_batch(self, samples: SampleBatch):
        return self._train_dual_policies(samples)

    def _train_dual_policies(self, samples: SampleBatch):
        learner_stats = {"learner_stats": {}}
        for policy_n, policy in enumerate(self.algorithms):
            if policy_n in self.DUAL_POLICIES:
                logger.debug(f"train policy {policy}")
                samples_copy = samples.copy()
                samples_copy = self._modify_batch_for_policy(policy_n,
                                                             samples_copy)
                learner_stats_one_policy = policy.learn_on_batch(samples_copy)
                learner_stats["learner_stats"][f"algo{policy_n}"] = \
                    learner_stats_one_policy
        return learner_stats

    def _modify_batch_for_policy(self, policy_n, samples_copy):
        if policy_n == self.COOP_POLICY_IDX:
            samples_copy = \
                self._overwrite_reward_for_policy_in_use(samples_copy)
        return samples_copy

    def _overwrite_reward_for_policy_in_use(self, samples_copy):
        samples_copy[samples_copy.REWARDS] = \
            np.array(samples_copy.data[self.config["welfare_key"]])
        logger.debug(f"overwrite reward with {self.welfare_key}")
        return samples_copy