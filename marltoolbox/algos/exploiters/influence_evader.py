import logging

# import numpy as np
from ray.rllib import SampleBatch
from ray.rllib.models.modelv2 import ModelV2
from ray.rllib.utils import merge_dicts
from ray.rllib.utils.annotations import override
from ray.rllib.utils.torch_ops import convert_to_torch_tensor

from marltoolbox.algos import hierarchical
from marltoolbox.algos.exploiters import dual_behavior
from marltoolbox.utils import postprocessing

logger = logging.getLogger(__name__)

DEFAULT_CONFIG = merge_dicts(
    dual_behavior.DEFAULT_CONFIG,
    {
        "start_exploit_at_step_n": 1000,
        "copy_weights_every_n_steps": 1000,
    }
)


# class InfluenceEvaderTorchPolicy(hierarchical.HierarchicalTorchPolicy):
class InfluenceEvaderTorchPolicy(dual_behavior.DualBehaviorTorchPolicy):
    """
    This policy trains 2 replicates of the same policy with different
    initialization. The first policy is cooperative while the second is more
    selfish. (You need to provide the configuration to setup these behaviors)

    After a specified number of env steps
    (start_exploit_at_step_n) this policy starts to exploit by copying at
    regular interval (copy_weights_every_n_steps) the weights of its second
    policy into its first policy which is the one used to act.
    """
    IN_USE_POLICY_IDX = \
        dual_behavior.DualBehaviorTorchPolicy.COOP_POLICY_IDX
    EVADING_POLICY_IDX = \
        dual_behavior.DualBehaviorTorchPolicy.SELFISH_POLICY_IDX

    def __init__(self, observation_space, action_space, config, **kwargs):
        super().__init__(observation_space, action_space, config, **kwargs)

        assert len(self.algorithms) == 2
        self.active_algo_idx = self.IN_USE_POLICY_IDX

        self.start_exploit_at_step_n = \
            config.get("start_exploit_at_step_n")
        self.copy_weights_every_n_steps = \
            config.get("copy_weights_every_n_steps")

        self.exploitation_started = False
        self.last_weights_copy_at_step_n = -1

    @override(hierarchical.HierarchicalTorchPolicy)
    def learn_on_batch(self, samples: SampleBatch):
        learner_stats = super().learn_on_batch(samples)
        # learner_stats = self._train_all_policies(samples)

        self._copy_weights_from_evade_model_to_in_use_model()
        return learner_stats

    # def _train_all_policies(self, samples: SampleBatch):
    #     learner_stats = {"learner_stats": {}}
    #     for policy_n, policy in enumerate(self.algorithms):
    #         logger.debug(f"train policy {policy}")
    #         samples_copy = samples.copy()
    #         samples_copy = self._modify_batch_for_policy(policy_n,
    #                                                      samples_copy)
    #         learner_stats_one_policy = policy.learn_on_batch(samples_copy)
    #         learner_stats["learner_stats"][f"learner_stats_algo{policy_n}"] = \
    #             learner_stats_one_policy
    #     return learner_stats
    #
    # def _modify_batch_for_policy(self, policy_n, samples_copy):
    #     if policy_n == self.IN_USE_POLICY_IDX:
    #         samples_copy = \
    #             self._overwrite_reward_for_policy_in_use(samples_copy)
    #     return samples_copy
    #
    # def _overwrite_reward_for_policy_in_use(self, samples_copy):
    #     samples_copy[samples_copy.REWARDS] = \
    #         np.array(samples_copy.data[self.config["welfare_key"]])
    #     logger.debug("overwrite reward for IN_USE_POLICY_IDX")
    #     return samples_copy

    def get_evading_weights(self):
        all_weights = {}
        for attribute_str in dir(self.algorithms[self.EVADING_POLICY_IDX]):
            attribute = getattr(self.algorithms[self.EVADING_POLICY_IDX],
                                attribute_str)
            if isinstance(attribute, ModelV2):
                all_weights[attribute_str] = attribute.state_dict()
        return all_weights

    def get_in_use_weights(self):
        all_weights = {}
        for attribute_str in dir(self.algorithms[self.IN_USE_POLICY_IDX]):
            attribute = getattr(self.algorithms[self.IN_USE_POLICY_IDX],
                                attribute_str)
            if isinstance(attribute, ModelV2):
                all_weights[attribute_str] = attribute.state_dict()
        return all_weights

    def _copy_weights_from_evade_model_to_in_use_model(self):
        self._to_log["evader_copied_weights"] = 0.0
        self._copy_weights_at_start_of_exploitation()
        self._copy_weights_after_start_every_n_steps()

    def _copy_weights_at_start_of_exploitation(self):
        if not self.exploitation_started and \
                self.global_timestep >= self.start_exploit_at_step_n:
            self.exploitation_started = True
            self._copy_weights()

    def _copy_weights(self):
        self._copy_weights_between_all_models()
        self.last_weights_copy_at_step_n = self.global_timestep
        logger.debug("copy weights from evade policy to policy in use")
        self._to_log["evader_copied_weights"] = 1.0

    def _copy_weights_between_all_models(self):
        for attribute_str in dir(self.algorithms[self.IN_USE_POLICY_IDX]):
            attr_in_use = getattr(self.algorithms[self.IN_USE_POLICY_IDX],
                                  attribute_str)
            if isinstance(attr_in_use, ModelV2):
                attr_evading = getattr(
                    self.algorithms[self.EVADING_POLICY_IDX],
                    attribute_str)
                assert isinstance(attr_evading, ModelV2)
                self._copy_weights_of_one_model(attr_in_use, attr_evading)
                logger.debug(f"copy weights for attribute {attribute_str}")

    def _copy_weights_of_one_model(self, model_in_use, model_evading):
        weights = model_evading.state_dict()
        weights = convert_to_torch_tensor(weights, device=self.device)
        model_in_use.load_state_dict(weights)

    def _copy_weights_after_start_every_n_steps(self):
        n_steps_since_last_copy = \
            self.global_timestep - self.last_weights_copy_at_step_n
        if self.exploitation_started and \
                n_steps_since_last_copy >= self.copy_weights_every_n_steps:
            self._copy_weights()
