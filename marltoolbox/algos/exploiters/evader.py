import logging

from ray.rllib import SampleBatch
from ray.rllib.utils import merge_dicts

from marltoolbox.algos import hierarchical

logger = logging.getLogger(__name__)

DEFAULT_CONFIG = merge_dicts(
    hierarchical.DEFAULT_CONFIG,
    {
        "start_exploit_at_step_n": 1000,
        "copy_weights_every_n_steps": 1000,
    }
)


class EvaderTorchPolicy(hierarchical.HierarchicalTorchPolicy):
    IN_USE_POLICY_IDX = 0
    EVADING_POLICY_IDX = 1

    def __init__(self, observation_space, action_space, config, **kwargs):
        super().__init__(observation_space, action_space, config, **kwargs)

        assert len(self.algorithms) == 2

        self.start_exploit_at_step_n = \
            config.get("start_exploit_at_step_n")
        self.copy_weights_every_n_steps = \
            config.get("copy_weights_every_n_steps")

        self.exploitation_started = False
        self.last_weights_copy_at_step_n = -1

    def learn_on_batch(self, samples: SampleBatch):
        learner_stats = self._train_all_policies(samples)

        self._copy_weights_from_evade_model_to_in_use_model()
        return learner_stats

    def _train_all_policies(self, samples: SampleBatch):
        learner_stats = {"learner_stats": {}}
        for policy_n, policy in enumerate(self.algorithms):
            samples_copy = samples.copy()
            learner_stats_one_policy = policy.learn_on_batch(samples_copy)
            learner_stats["learner_stats"][f"learner_stats_algo{policy_n}"] = \
                learner_stats_one_policy
        return learner_stats

    def get_evading_weights(self):
        return self.algorithms[self.EVADING_POLICY_IDX].model.variables()

    def get_in_use_weights(self):
        return self.algorithms[self.IN_USE_POLICY_IDX].model.variables()

    def _copy_weights_from_evade_model_to_in_use_model(self):
        self._copy_weights_at_start_of_exploitation()
        self._copy_weights_after_start_every_n_steps()

    def _copy_weights_at_start_of_exploitation(self):
        if not self.exploitation_started and \
                self.global_timestep >= self.start_exploit_at_step_n:
            self.exploitation_started = True
            self._copy_weights()

    def _copy_weights(self):
        self.algorithms[self.IN_USE_POLICY_IDX].model.load_state_dict(
            self.algorithms[self.EVADING_POLICY_IDX].model.state_dict())
        self.last_weights_copy_at_step_n = self.global_timestep
        logger.info("copy weights from evade policy to policy in use")

    def _copy_weights_after_start_every_n_steps(self):
        n_steps_since_last_copy = \
            self.global_timestep - self.last_weights_copy_at_step_n

        if self.exploitation_started and \
                n_steps_since_last_copy >= self.copy_weights_every_n_steps:
            self._copy_weights()
