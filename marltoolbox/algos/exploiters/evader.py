import copy
import logging
from typing import Dict

from ray.rllib import SampleBatch
from ray.rllib.policy import Policy
from ray.rllib.utils import merge_dicts
from ray.rllib.utils.annotations import override

from marltoolbox.algos import hierarchical
from marltoolbox.utils.postprocessing import WELFARES

logger = logging.getLogger(__name__)

DEFAULT_CONFIG = merge_dicts(
    hierarchical.DEFAULT_CONFIG,
    {
        "start_exploit_at_step_n": 1000,
        "copy_weights_every_n_steps": 1000,
    }
)


class EvaderTorchPolicy(hierarchical.HierarchicalTorchPolicy):
    """
    This policy train 2 replicates of the same policy with different
    initialization. The first policy is cooperative while the second is more
    selfish. (You need to provide the configuration to step these behavior)

    After a specified number of env steps
    (start_exploit_at_step_n) this policy starts to exploit by copying at
    regular interval (copy_weights_every_n_steps) the weights of its second
    policy into its first policy which is the one used to act.
    """
    IN_USE_POLICY_IDX = 0
    EVADING_POLICY_IDX = 1

    def __init__(self, observation_space, action_space, config, **kwargs):
        super().__init__(observation_space, action_space, config, **kwargs)

        assert len(self.algorithms) == 2

        self.start_exploit_at_step_n = \
            config.get("start_exploit_at_step_n")
        self.copy_weights_every_n_steps = \
            config.get("copy_weights_every_n_steps")

        self.exploitation_started = False
        self.last_weights_copy_at_step_n = -1

    @override(hierarchical.HierarchicalTorchPolicy)
    def learn_on_batch(self, samples: SampleBatch):
        learner_stats = self._train_all_policies(samples)

        self._copy_weights_from_evade_model_to_in_use_model()
        return learner_stats

    def _train_all_policies(self, samples: SampleBatch):
        learner_stats = {"learner_stats": {}}
        for policy_n, policy in enumerate(self.algorithms):
            samples_copy = samples.copy()
            learner_stats_one_policy = policy.learn_on_batch(samples_copy)
            learner_stats["learner_stats"][f"learner_stats_algo{policy_n}"] = \
                learner_stats_one_policy
        return learner_stats

    def get_evading_weights(self):
        return self.algorithms[self.EVADING_POLICY_IDX].model.variables()

    def get_in_use_weights(self):
        return self.algorithms[self.IN_USE_POLICY_IDX].model.variables()

    def _copy_weights_from_evade_model_to_in_use_model(self):
        self._to_log["evader_copied_weights"] = False
        self._copy_weights_at_start_of_exploitation()
        self._copy_weights_after_start_every_n_steps()

    def _copy_weights_at_start_of_exploitation(self):
        if not self.exploitation_started and \
                self.global_timestep >= self.start_exploit_at_step_n:
            self.exploitation_started = True
            self._copy_weights()

    def _copy_weights(self):
        self.algorithms[self.IN_USE_POLICY_IDX].model.load_state_dict(
            self.algorithms[self.EVADING_POLICY_IDX].model.state_dict())
        self.last_weights_copy_at_step_n = self.global_timestep
        logger.info("copy weights from evade policy to policy in use")
        self._to_log["evader_copied_weights"] = True

    def _copy_weights_after_start_every_n_steps(self):
        n_steps_since_last_copy = \
            self.global_timestep - self.last_weights_copy_at_step_n

        if self.exploitation_started and \
                n_steps_since_last_copy >= self.copy_weights_every_n_steps:
            self._copy_weights()


def create_evader_policy_config(
        welfare_function=None,
        cooperative_policy_config_update: Dict = None,
        start_exploit_at_step_n: int = None,
        copy_weights_every_n_steps: int = None,
        exploiter_policy: Policy = None,
):
    evader_policy_config = _prepare_nested_policies_configs(
        exploiter_policy, welfare_function,
        cooperative_policy_config_update)

    evader_policy_config = _update_hyperparameter_if_provided(
        evader_policy_config,
        start_exploit_at_step_n,
        copy_weights_every_n_steps)

    return evader_policy_config


def _prepare_nested_policies_configs(exploiter_policy, welfare_function,
                                     cooperative_policy_config_update):
    config_update, cooperative_config_update = {}, {}

    cooperative_config_update = \
        _change_first_nested_policy_to_cooperativeness(
            cooperative_config_update,
            welfare_function,
            cooperative_policy_config_update)

    evader_policy_config = _format_as_hierarchical_policy_config(
        cooperative_config_update, config_update, exploiter_policy)

    return evader_policy_config


def _change_first_nested_policy_to_cooperativeness(
        cooperative_config_update, welfare_function,
        cooperative_policy_config_update):
    if welfare_function is not None:
        assert welfare_function in WELFARES
        cooperative_config_update.update(
            {"welfare_function": welfare_function})
    elif cooperative_policy_config_update is not None:
        cooperative_config_update.update(cooperative_policy_config_update)
    return cooperative_config_update


def _format_as_hierarchical_policy_config(
        cooperative_config_update, config_update, exploiter_policy):
    evader_policy_config = copy.deepcopy(DEFAULT_CONFIG)
    evader_policy_config["nested_policies"] = [
        {"Policy_class": exploiter_policy,
         "config_update": cooperative_config_update},
        {"Policy_class": exploiter_policy,
         "config_update": config_update}
    ]
    return evader_policy_config


def _update_hyperparameter_if_provided(
        evader_policy_config,
        start_exploit_at_step_n,
        copy_weights_every_n_steps):
    if start_exploit_at_step_n is not None:
        evader_policy_config["start_exploit_at_step_n"] = \
            start_exploit_at_step_n
    if copy_weights_every_n_steps is not None:
        evader_policy_config["copy_weights_every_n_steps"] = \
            copy_weights_every_n_steps

    return evader_policy_config
