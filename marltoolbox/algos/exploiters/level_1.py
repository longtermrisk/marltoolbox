import logging
from typing import Dict, TYPE_CHECKING

from ray.rllib import SampleBatch
from ray.rllib.agents.callbacks import DefaultCallbacks
from ray.rllib.evaluation import MultiAgentEpisode
from ray.rllib.policy import Policy
from ray.rllib.utils import merge_dicts
from ray.rllib.utils.annotations import override
from ray.rllib.utils.typing import AgentID, PolicyID

from marltoolbox.algos.exploiters import dual_behavior

if TYPE_CHECKING:
    from ray.rllib.evaluation import RolloutWorker

logger = logging.getLogger(__name__)

DEFAULT_CONFIG = merge_dicts(
    dual_behavior.DEFAULT_CONFIG,
    {
        "lookahead_n_times": int(1),
    }
)


class Level1ExploiterTorchPolicy(dual_behavior.DualBehaviorTorchPolicy):
    """
    This exploiter policy trains a 3rd policy which is a simulation of the
    opponent. This exploiter policy then uses the simulated behavior of the
    opponent to predict when the opponent is going to punish it. The
    exploiter will switch from selfishness to cooperation at that times.

    This policy trains 2 replicates of the same policy with different
    initialization. The first policy is cooperative while the second is more
    selfish. (You need to provide the configuration to setup these behaviors)


    """
    LEVEL_1_POLICY_IDX = 2

    def __init__(self, observation_space, action_space, config, **kwargs):
        super().__init__(observation_space, action_space, config, **kwargs)
        self.simulated_opponent = self.algorithms[self.LEVEL_1_POLICY_IDX]
        self.lookahead_n_times = config.get("lookahead_n_times")

    @override(dual_behavior.DualBehaviorTorchPolicy)
    def _learn_on_batch(self, samples: SampleBatch):
        learner_stats = super()._learn_on_batch(samples)
        learner_stats = self._train_level_1_policy(learner_stats, samples)
        return learner_stats

    def _train_level_1_policy(self, learner_stats, samples):
        raise NotImplementedError()

    @override(Policy)
    def compute_actions(self, *args, **kwargs):

        selfish_action_tuple = self._compute_selfish_action(*args, **kwargs)

        coop_needed = self._is_cooperation_needed_to_prevent_punishement(
            selfish_action_tuple, *args, **kwargs)

        if coop_needed:
            self.active_algo_idx = self.COOP_POLICY_IDX
            self._to_log["use_exploiter_cooperating"] = 1.0
            self._to_log["use_exploiter_selfish"] = 0.0
            return super().compute_actions(*args, **kwargs)
        else:
            self._to_log["use_exploiter_cooperating"] = 0.0
            self._to_log["use_exploiter_selfish"] = 1.0
            self.active_algo_idx = self.SELFISH_POLICY_IDX
            return selfish_action_tuple

    def _compute_selfish_action(self, *args, **kwargs):
        tmp_active_algo_idx = self.active_algo_idx
        self.active_algo_idx = self.SELFISH_POLICY_IDX
        selfish_action_tuple = super().compute_actions(*args, **kwargs)
        self.active_algo_idx = tmp_active_algo_idx
        return selfish_action_tuple

    def _is_cooperation_needed_to_prevent_punishement(
            self, selfish_action_tuple, *args, **kwargs):

        selfish_action = selfish_action_tuple[0]

        is_punishing = self.simulated_opponent._is_punishment_planned()

        is_going_to_punish_at_next_step = None
        for _ in range(self.lookahead_n_times):
            is_going_to_punish_at_next_step = \
                self._lookahead_for_opponent_punishing(
                    selfish_action,
                    *args,
                    **kwargs)
            if is_going_to_punish_at_next_step:
                break

        self._to_log["simu_opp_is_punishing"] = is_punishing
        self._to_log["simu_opp_is_going_to_punish_at_next_step"] = \
            is_going_to_punish_at_next_step

        return is_punishing or is_going_to_punish_at_next_step

    def _lookahead_for_opponent_punishing(
            self, selfish_action, *args, **kwargs):
        raise NotImplementedError()


class ObserveOpponentCallbacks(DefaultCallbacks):

    def on_postprocess_trajectory(
            self, *, worker: "RolloutWorker", episode: MultiAgentEpisode,
            agent_id: AgentID, policy_id: PolicyID,
            policies: Dict[PolicyID, Policy], postprocessed_batch: SampleBatch,
            original_batches: Dict[AgentID, SampleBatch], **kwargs):
        policy_ids = list(policies.keys())
        opp_policy_id = policy_ids.remove(policy_id)
        assert len(opp_policy_id) == 1
        opp_policy_id = opp_policy_id[0]

        opponent_batch = original_batches[opp_policy_id]
        postprocessed_batch.__setattr__("data_opponent", opponent_batch.copy())
