import logging
from typing import List, Union, Optional, Dict, Tuple, TYPE_CHECKING

from ray.rllib import SampleBatch
from ray.rllib.evaluation import MultiAgentEpisode
from ray.rllib.policy import Policy
from ray.rllib.utils import merge_dicts
from ray.rllib.utils.annotations import override
from ray.rllib.utils.typing import AgentID, PolicyID
from ray.rllib.utils.typing import TensorType
from ray.rllib.agents.callbacks import DefaultCallbacks

from marltoolbox.algos import hierarchical
from marltoolbox.algos.exploiters import dual_behavior

if TYPE_CHECKING:
    from ray.rllib.evaluation import RolloutWorker

logger = logging.getLogger(__name__)

DEFAULT_CONFIG = merge_dicts(
    dual_behavior.DEFAULT_CONFIG,
    {
        "bias_in_punishment_decision": 0.0,
        "multiplier_in_punishment_decision": 1.0,
    }
)


class Level1ExploiterTorchPolicy(dual_behavior.DualBehaviorTorchPolicy):
    """
    This exploiter policy trains a 3rd policy which is a simulation of the
    opponent. This exploiter policy then uses the simulated behavior of the
    opponent to predict when the opponent is going to punish it. The
    exploiter will switch from selfishness to cooperation at that times.

    This policy trains 2 replicates of the same policy with different
    initialization. The first policy is cooperative while the second is more
    selfish. (You need to provide the configuration to setup these behaviors)


    """
    LEVEL_1_POLICY_IDX = 2

    def __init__(self, observation_space, action_space, config, **kwargs):
        super().__init__(observation_space, action_space, config, **kwargs)

        self.bias_in_punishment_decision = \
            config.get("bias_in_punishment_decision")
        self.multiplier_in_punishment_decision = \
            config.get("multiplier_in_punishment_decision")

    @override(hierarchical.HierarchicalTorchPolicy)
    def learn_on_batch(self, samples: SampleBatch):
        learner_stats = super().learn_on_batch(samples)
        learner_stats = self._train_level_1_policy(learner_stats, samples)
        return learner_stats

    def _train_level_1_policy(self, learner_stats, samples):
        raise NotImplementedError()

    def compute_actions(
            self,
            obs_batch: Union[List[TensorType], TensorType],
            state_batches: Optional[List[TensorType]] = None,
            prev_action_batch: Union[List[TensorType], TensorType] = None,
            prev_reward_batch: Union[List[TensorType], TensorType] = None,
            info_batch: Optional[Dict[str, list]] = None,
            episodes: Optional[List["MultiAgentEpisode"]] = None,
            explore: Optional[bool] = None,
            timestep: Optional[int] = None,
            **kwargs) -> \
            Tuple[TensorType, List[TensorType], Dict[str, TensorType]]:

        if self._is_cooperation_needed_to_prevent_punishement():
            self.active_algo_idx = self.COOP_POLICY_IDX
        else:
            self.active_algo_idx = self.SELFISH_POLICY_IDX

        super().compute_actions(obs_batch, state_batches, prev_action_batch,
                                prev_reward_batch, info_batch, episodes,
                                explore, timestep, **kwargs)

    def _is_cooperation_needed_to_prevent_punishement(self):
        raise NotImplementedError()


class ObserveOpponentCallbacks(DefaultCallbacks):

    def on_postprocess_trajectory(
            self, *, worker: "RolloutWorker", episode: MultiAgentEpisode,
            agent_id: AgentID, policy_id: PolicyID,
            policies: Dict[PolicyID, Policy], postprocessed_batch: SampleBatch,
            original_batches: Dict[AgentID, SampleBatch], **kwargs):

        policy_ids = list(policies.keys())
        opp_policy_id = policy_ids.remove(policy_id)
        assert len(opp_policy_id) == 1
        opp_policy_id = opp_policy_id[0]

        opponent_batch = original_batches[opp_policy_id]
        postprocessed_batch.__setattr__("data_opponent", opponent_batch.copy())
