from ray.rllib.utils import merge_dicts

from marltoolbox.algos.exploiters import level_1

DEFAULT_CONFIG = merge_dicts(
    level_1.DEFAULT_CONFIG,
    {
        "bias_in_punishment_decision": 0.0,
        "multiplier_in_punishment_decision": 1.0,
    },
)


class Level1LTFTExploiterTorchPolicy(
    level_1.Level1ExploiterTorchPolicy,
):
    LTFT_POLICY_IDX = level_1.Level1ExploiterTorchPolicy.LEVEL_1_POLICY_IDX

    def __init__(self, observation_space, action_space, config, **kwargs):
        super().__init__(observation_space, action_space, config, **kwargs)

        self.lvl1_LTFT_algo = self.algorithms[self.AMTFT_POLICY_IDX]

    def on_episode_end(self, *args, **kwargs):
        self.lvl1_LTFT_algo.on_episode_end(*args, **kwargs)

    def on_observation_fn(self, *args, **kwargs):
        self.lvl1_LTFT_algo.on_observation_fn(*args, **kwargs)

    def on_episode_step(self, *args, **kwargs):
        self.lvl1_LTFT_algo.on_episode_step(*args, **kwargs)

    def on_postprocess_trajectory(self, *args, **kwargs):
        self.lvl1_LTFT_algo.on_postprocess_trajectory(*args, **kwargs)

    def postprocess_trajectory(self, *args, **kwargs):
        self.lvl1_LTFT_algo.postprocess_trajectory(*args, **kwargs)

    def _train_level_1_policy(self, learner_stats, samples):
        new_samples = samples.copy()
        new_samples.data = new_samples.data_opponent
        stats = self.lvl1_LTFT_algo.learn_on_batch(new_samples)
        learner_stats["learner_stats"][f"algo{self.LTFT_POLICY_IDX}"] = stats
        return learner_stats

    def _is_cooperation_needed_to_prevent_punishement(self):
        biaised_threshold = (
            self.multiplier_in_punishment_decision
            * self.lvl1_LTFT_algo.debit_threshold
        ) + self.bias_in_punishment_decision

        return self.lvl1_LTFT_algo._is_starting_new_punishment_required(
            manual_threshold=biaised_threshold
        )
